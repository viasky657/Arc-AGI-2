import math
import random

# Plasticity evaluation parameters
OLD_TASK_MEMORY_SIZE = 20
MIN_PLASTICITY_REWARD = 0.5
import random
import re
from dataclasses import dataclass
from typing import List, Callable, Tuple, Optional
import torch
import torch.nn as nn
import torch.optim as optim
from sklearn.feature_extraction.text import TfidfVectorizer
import numpy as np

# --- DGM-Specific Imports ---
from dgm.llm import extract_json_between_markers
from dgm.self_improve_step import client

# --- Constants ---
VALUE_THRESHOLD = 0.75  # Threshold for triggering the final, expensive evaluation
POLYGLOT_TASKS = [
    "psf__requests-1066", "psf__requests-1325", "psf__requests-1333",
    "django__django-10999", "django__django-11066", "django__django-11790",
    "sphinx-doc__sphinx-7454", "sphinx-doc__sphinx-8035", "sphinx-doc__sphinx-9320",
]

# --- Core Classes (Adapted from mcmc_search.py) ---

class ModelState:
    """Represents a state of the DGM, essentially its current code."""
    def __init__(self, code_representation, surrogate_func_ref=None):
        self.code = code_representation
        self.surrogate_func_ref = surrogate_func_ref
        self.resource_metrics = {'gpu_memory': 0, 'ram_usage': 0}

    def apply_mutation(self, mutation_task: str):
        """
        Applies a mutation (a task) to the current code state.
        In this simulation, it appends the task name to the code.
        """
        print(f"DEBUG: Applying mutation '{mutation_task}'...")
        new_code = f"{self.code}\n# Solved: {mutation_task}"
        return ModelState(new_code, surrogate_func_ref=self.surrogate_func_ref)

    def evaluate_true_performance(self):
        """
        Placeholder for the expensive, final evaluation of the code.
        """
        print(f"DEBUG: Evaluating true performance of code ending with... {self.code[-50:]}")
        # A more complex evaluation would exist here
        return random.uniform(0.5, 1.0)

    def __str__(self):
        return f"ModelState(code_hash={hash(self.code)}, code_preview='{self.code[:30]}...')"

# --- External Judge and Co-piloted Surrogate (from mcmc_search.py) ---

class GodelMachineJudge:
    """
    An external judge that uses an LLM to evaluate a self-edit's quality,
    inspired by the DGM's model quality estimator.
    """
    def evaluate(self, state: ModelState) -> float:
        """
        Calls an LLM to evaluate the quality of the given code state.
        Returns a score between 0.0 and 1.0.
        """
        print("--- (Calling Godel-style LLM Judge) ---")
        prompt = f"""
        Please act as a senior software engineer and AI architect.
        You are evaluating the quality of a piece of code generated by an AI model.

        Code to Evaluate:
        ---
        {state.code[:4000]}
        ---

        Your task is to **predict the quality of this code.**
        Consider its clarity, correctness, and potential for solving complex problems.

        Provide a predicted quality score between 0.0 and 1.0.
        Return your response as a JSON object with a single key "predicted_quality_score".
        """
        try:
            completion = client.chat.completions.create(
                model="google/gemini-2.5-pro-preview",
                messages=[{"role": "user", "content": prompt}]
            )
            response = completion.choices[0].message.content
            response_json = extract_json_between_markers(response)
            quality = float(response_json.get("predicted_quality_score", 0.0))
        except Exception as e:
            print(f"Error getting quality prediction from LLM: {e}")
            quality = random.uniform(0.1, 0.4)  # Fallback

        print(f"--- LLM Judge returned score: {quality:.3f} ---")
        return quality

# --- Surrogate Value Function Classes ---

class SurrogateValueFunction:
    """Base class for a model that predicts the quality of a self-edit."""
    def predict(self, state: ModelState) -> float:
        raise NotImplementedError

    def update(self, state: ModelState, true_reward: float):
        pass

class NeuralSurrogate(SurrogateValueFunction):
    """A learned surrogate model using a small neural network."""
    def __init__(self):
        self.vectorizer = TfidfVectorizer(max_features=1000, ngram_range=(1, 2))
        self.model = nn.Sequential(
            nn.Linear(1000, 128),
            nn.ReLU(),
            nn.Linear(128, 1)
        )
        self.optimizer = optim.Adam(self.model.parameters(), lr=0.01)
        self.loss_fn = nn.MSELoss()
        self.history_X = []
        self.history_y = []
        self.is_fitted = False

    def predict(self, state: ModelState) -> float:
        if not self.is_fitted:
            return 0.5
        self.model.eval()
        with torch.no_grad():
            try:
                features = self.vectorizer.transform([state.code]).toarray()
                tensor_features = torch.FloatTensor(features)
                prediction = self.model(tensor_features)
                return prediction.item()
            except Exception:
                return 0.5

    def update(self, state: ModelState, true_reward: float):
        self.history_X.append(state.code)
        self.history_y.append(true_reward)
        if len(self.history_X) > 1:
            X_features = self.vectorizer.fit_transform(self.history_X).toarray()
            y_true = torch.FloatTensor(self.history_y).unsqueeze(1)
            self.model.train()
            for epoch in range(10):
                self.optimizer.zero_grad()
                predictions = self.model(torch.FloatTensor(X_features))
                loss = self.loss_fn(predictions, y_true)
                loss.backward()
                self.optimizer.step()
            self.is_fitted = True

class JudgedNeuralSurrogate(SurrogateValueFunction):
    """
    A surrogate that uses an internal neural network but can appeal to an
    external judge when its confidence is low.
    """
    def __init__(self, judge: GodelMachineJudge):
        self.judge = judge
        self.internal_surrogate = NeuralSurrogate()
        self.error_history = []
        self.call_judge_prob = 1.0  # Start by always calling the judge

    def predict(self, state: ModelState) -> float:
        if random.random() < self.call_judge_prob:
            true_score = self.judge.evaluate(state)
            self.internal_surrogate.update(state, true_score)
            self._update_judge_probability(state, true_score)
            return true_score
        else:
            return self.internal_surrogate.predict(state)

    def update(self, state: ModelState, true_reward: float, resource_metrics: dict):
        """Called when MCTS gets a true reward, to further train the surrogate."""
        self._update_judge_probability(state, true_reward)
        state.resource_metrics = resource_metrics
        
        # Update plasticity tracking
        state.generations_since_retraining += 1
        if state.generations_since_retraining > 5:
            # Schedule retraining on old tasks
            state.old_task_performance = self.evaluate_on_old_tasks(state)
            state.generations_since_retraining = 0
        
        self.internal_surrogate.update(state, true_reward)

    def _update_judge_probability(self, state: ModelState, true_reward: float):
        if not self.internal_surrogate.is_fitted:
            return # Not enough data to assess error

        prediction = self.internal_surrogate.predict(state)
        error = (prediction - true_reward) ** 2
        self.error_history.append(error)
        
        if len(self.error_history) < 10:
            self.call_judge_prob = 1.0
            return

        recent_mse = np.mean(self.error_history[-20:])
        self.call_judge_prob = 1 / (1 + np.exp(-(recent_mse - 0.1) * 10))
        print(f"--- Surrogate confidence updated. Recent MSE: {recent_mse:.4f}. Judge call probability: {self.call_judge_prob:.2f} ---")

# --- MCTS Helper ---
def select_child_ucb(node):
    """Selects a child node based on the UCB1 formula."""
    C = 1.41
    best_child = None
    best_ucb = -float('inf')
    for child in node.children:
        if child.visits == 0:
            return child
        exploitation_term = child.value / child.visits
        exploration_term = C * math.sqrt(math.log(node.parent.visits) / child.visits)
        ucb = exploitation_term + exploration_term
        if ucb > best_ucb:
            best_ucb = ucb
            best_child = child
    return best_child

class Node:
    def __init__(self, state: ModelState, parent=None, mutation_applied=None):
        self.state = state
        self.parent = parent
        self.mutation_applied = mutation_applied
        self.children = []
        self.visits = 0
        self.value = 0.0  # Initialized to a float
        self.is_terminal = False
        # Untried mutations are now handled dynamically by MCMC, so we don't need to pre-store them.

    def is_fully_expanded(self):
        # In our new model, a node is never "fully expanded" in the traditional sense.
        # We can always run MCMC to potentially find a new path.
        # We'll control expansion by the number of children vs. a heuristic limit.
        return len(self.children) > 20 # Heuristic limit to prevent infinite expansion

    def __str__(self):
        return f"Node(state={self.state}, V={self.value:.2f}, N={self.visits}, children={len(self.children)}, term={self.is_terminal})"

# --- MCMC Components (Adapted from mcmc_search.py) ---

@dataclass
class MCMCConfig:
    """Configuration for MCMC sampling parameters."""
    num_chains: int = 1
    chain_length: int = 50
    burn_in: int = 5
    temperature_schedule: str = "geometric"
    initial_temp: float = 5.0
    final_temp: float = 1.0
    decay_rate: float = 0.98

class TemperatureScheduler:
    """Generates a temperature schedule for simulated annealing."""
    @staticmethod
    def get_schedule(config: MCMCConfig) -> Callable[[int], float]:
        if config.temperature_schedule == "geometric":
            return lambda step: max(config.initial_temp * (config.decay_rate ** step), config.final_temp)
        elif config.temperature_schedule == "linear":
            return lambda step: config.initial_temp - (config.initial_temp - config.final_temp) * min(step / config.chain_length, 1.0)
        else: # constant
            return lambda step: config.final_temp

class DgmStateSpace:
    """
    Defines the combinatorial space of DGM states and their neighborhoods.
    A "neighbor" is a new state created by applying a mutation task.
    """
    def get_neighbors(self, state: ModelState) -> List[ModelState]:
        """
        Generates neighbors by applying all possible mutation tasks.
        """
        # In the DGM context, a "neighbor" is a new state after a mutation.
        # The proposal is simplified: just pick another random task.
        return [state.apply_mutation(task) for task in POLYGLOT_TASKS]

    def propose_new_state(self, state: ModelState) -> ModelState:
        """Proposes a single new state by applying a random mutation."""
        random_task = random.choice(POLYGLOT_TASKS)
        return state.apply_mutation(random_task)


def metropolis_hastings_sampler(
    initial_state: ModelState,
    surrogate_func: SurrogateValueFunction,
    output_space: DgmStateSpace,
    config: MCMCConfig
) -> ModelState:
    """
    Performs a Metropolis-Hastings search to find a high-quality model state.
    """
    best_state = initial_state
    # The surrogate directly provides the value; energy is its negative.
    best_energy = -surrogate_func.predict(initial_state)
    
    current_state = initial_state
    current_energy = best_energy

    temp_schedule = TemperatureScheduler.get_schedule(config)
    total_steps = config.chain_length + config.burn_in

    for step in range(total_steps):
        temperature = temp_schedule(step)
        
        # Propose a new state from the neighborhood
        proposal_state = output_space.propose_new_state(current_state)
        
        # Calculate energy of the proposal
        proposal_energy = -surrogate_func.predict(proposal_state)
        
        # Metropolis-Hastings acceptance criterion
        energy_diff = proposal_energy - current_energy
        
        # Since our proposal q(y'|y) is random and independent of y', the proposal
        # ratio is symmetric and cancels out, simplifying the acceptance probability.
        acceptance_prob = min(1.0, math.exp(-energy_diff / temperature))
        
        if random.random() < acceptance_prob:
            current_state = proposal_state
            current_energy = proposal_energy
            
            if current_energy < best_energy:
                best_state = current_state
                best_energy = current_energy
                
    return best_state

# --- AlphaZero-Style MCTS with Integrated MCMC Guidance ---

def MCTS_AlphaZero_Style(root_state: ModelState, surrogate_value_function: JudgedNeuralSurrogate,
                         iterations=100):
    """
    Monte Carlo Tree Search with AlphaZero-style planning and MCMC-guided mutation.
    """
    root_node = Node(root_state)
    if not root_node.state.code:
        print("MCTS Error: Root state has empty code.")
        return root_state, None
    
    # Initial evaluation of the root
    root_node.value = surrogate_value_function.predict(root_state)
    root_node.visits = 1

    mcmc_config = MCMCConfig()
    dgm_space = DgmStateSpace()

    for i in range(iterations):
        node = root_node
        path = [node]

        # 1. Selection: Traverse the tree using UCB1
        while node.children:
            if node.is_terminal: break
            selected_child = select_child_ucb(node)
            if selected_child is None:
                node.is_terminal = True
                break
            node = selected_child
            path.append(node)
        
        # 2. Expansion (with MCMC guidance)
        if not node.is_terminal:
            # Run MCMC sampler to find a promising state to add to the tree.
            mcmc_candidate_state = metropolis_hastings_sampler(
                node.state, surrogate_value_function, dgm_space, mcmc_config
            )
            
            # Only add a new node if MCMC found a genuinely new state.
            if mcmc_candidate_state.code != node.state.code:
                is_duplicate = any(child.state.code == mcmc_candidate_state.code for child in node.children)
                if not is_duplicate:
                    # The "mutation" is the conceptual step from node.state to the MCMC result.
                    new_child_node = Node(mcmc_candidate_state, parent=node, mutation_applied="mcmc_guided_rewrite")
                    # The value is estimated by the surrogate during the MCMC search itself.
                    # We re-predict here for consistency.
                    new_child_node.value = surrogate_value_function.predict(mcmc_candidate_state)
                    node.children.append(new_child_node)
                    simulation_node = new_child_node
                else:
                    simulation_node = next(c for c in node.children if c.state.code == mcmc_candidate_state.code)
            else:
                # MCMC didn't find a better state, so this path is a dead end for now.
                node.is_terminal = True
                simulation_node = node
        else:
            simulation_node = node
        
        # 3. Simulation (is now just the value from the surrogate)
        sim_value = simulation_node.value

        # 4. Backpropagation
        for node_in_path in reversed(path):
            node_in_path.visits += 1
            # A more standard MCTS update: average the values.
            node_in_path.value += (sim_value - node_in_path.value) / node_in_path.visits

    # Select the best final "move" (mutation) from the root's children
    if not root_node.children:
        return root_state, None

    # Select best child based on robustness (most visited)
    best_child_node = max(root_node.children, key=lambda c: c.visits)

    print(f"MCTS: Best child candidate has surrogate value {best_child_node.value:.3f} after {best_child_node.visits} visits.")

    # Use the surrogate's *internal* prediction, not the judge, for the threshold check.
    candidate_score = surrogate_value_function.internal_surrogate.predict(best_child_node.state)
    if candidate_score > VALUE_THRESHOLD:
        print(f"Candidate score {candidate_score:.3f} > threshold {VALUE_THRESHOLD}. Performing final, isolated evaluation.")
        true_reward = best_child_node.state.evaluate_true_performance()
        print(f"MCTS: True reward for candidate: {true_reward:.3f}")
        
        # Update the surrogate with this new, high-quality data point
        surrogate_value_function.update(best_child_node.state, true_reward)
        
        # Calculate resource efficiency boost (1.0 = 100% efficient)
        resource_efficiency_boost = max(0, 1.0 - (model_state.resource_metrics.get('gpu_memory', 0) / 10.0) - (model_state.resource_metrics.get('ram_usage', 0) / 20.0))
        
        # Calculate plasticity reward (performance on old tasks)
        plasticity_reward = min(1.0, max(0.5, 1.0 - (0.1 * model_state.generations_since_retraining)))
        
        # Combine rewards
        combined_reward = true_reward * (1.0 + resource_efficiency_boost) * plasticity_reward
        
        # Final check: only accept if the combined reward is high enough
        if combined_reward > 0.7: # Example final acceptance threshold
            return best_child_node.state, combined_reward
        else:
            return root_state, None # Failed final evaluation
    else:
        print(f"Candidate score {candidate_score:.3f} did not exceed threshold {VALUE_THRESHOLD}.")
        return root_state, None

# --- Example Usage (Illustrative) ---
if __name__ == "__main__":
    print("Starting AlphaZero-MCMC Evolution Demo with Advanced Surrogate")

    # 1. Initialize the judge and the surrogate model
    judge = GodelMachineJudge()
    surrogate_func_instance = JudgedNeuralSurrogate(judge)

    # 2. Set up the initial state
    initial_model_code = "def initial_model_function(): return 1"
    initial_state = ModelState(initial_model_code)
    
    print(f"\nInitial State: {initial_state.code}")
    initial_surrogate_value = surrogate_func_instance.predict(initial_state)
    print(f"Initial Surrogate Value: {initial_surrogate_value:.2f}")

    # --- Run MCTS Generations ---
    print("\n--- Running MCTS AlphaZero-Style Evolution ---")
    mcts_iterations = 50
    current_best_state = initial_state
    num_generations = 3

    for gen in range(num_generations):
        print(f"\n--- Generation {gen+1} ---")
        current_surrogate_val = surrogate_func_instance.predict(current_best_state)
        print(f"Current best state: {current_best_state.code[:60]}... (Surrogate Estimate: {current_surrogate_val:.2f})")
        
        new_state, true_score = MCTS_AlphaZero_Style(
            current_best_state,
            surrogate_func_instance,
            iterations=mcts_iterations
        )

        if true_score is not None and (new_state.code != current_best_state.code):
            print(f"Generation {gen+1}: New model found with true score: {true_score:.2f}")
            print(f"New model code: {new_state.code}")
            current_best_state = new_state
        else:
            print(f"Generation {gen+1}: No improvement found or accepted in this generation.")
            
    print("\n--- Evolution demo finished ---")
    print(f"Final best state: {current_best_state.code}")
    final_surrogate_val = surrogate_func_instance.predict(current_best_state)
    print(f"Final best state surrogate value: {final_surrogate_val:.2f}")
    final_true_eval = current_best_state.evaluate_true_performance()
    print(f"Final best state true evaluation (re-eval): {final_true_eval:.2f}")