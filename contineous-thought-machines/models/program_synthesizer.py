import torch
import torch.nn as nn
import math
from typing import Tuple, List

# Import DynamicEntropyPatcher and its dependencies from ctm_Diffusion_NEWNEW
from .ctm_Diffusion_NEWNEW import DynamicEntropyPatcher

class ProgramSynthesizer(nn.Module):
    """
    A transformer-based program synthesizer that generates a program as a sequence of bytes,
    and then segments it into entropy-based patches.
    
    This module takes a high-level state representation, generates a raw byte sequence
    autoregressively, and then uses a DynamicEntropyPatcher to structure the sequence.
    """
    def __init__(self, 
                 d_model: int, 
                 n_heads: int = 4, 
                 n_layers: int = 3,
                 d_ff: int = 1024,
                 dropout: float = 0.1,
                 max_gen_len: int = 512,
                 patcher_config: dict = None):
        super().__init__()
        self.d_model = d_model
        self.max_gen_len = max_gen_len
        self.byte_vocab_size = 256

        # --- Transformer Decoder for Byte Generation ---
        # We need a decoder, not an encoder for generation.
        decoder_layer = nn.TransformerDecoderLayer(
            d_model=d_model,
            nhead=n_heads,
            dim_feedforward=d_ff,
            dropout=dropout,
            batch_first=True
        )
        self.transformer_decoder = nn.TransformerDecoder(decoder_layer, num_layers=n_layers)

        # --- I/O Layers ---
        self.byte_embedding = nn.Embedding(self.byte_vocab_size, d_model)
        self.output_projector = nn.Linear(d_model, self.byte_vocab_size)
        
        # --- Positional Encoding ---
        self.pos_encoder = PositionalEncoding(d_model, dropout, max_len=max_gen_len + 1)
        
        # --- Entropy-based Patcher ---
        # Use provided config or a default for the patcher
        if patcher_config is None:
            patcher_config = {
                'embedding_dim': d_model,
                'patch_cnn_channels': 64,
                'patching_mode': "global",
                'global_threshold': 0.5,
                'relative_threshold': 0.1,
                'min_patch_size': 4,
                'max_patch_size': 128,
                'entropy_byte_vocab_size': 256,
                'entropy_embedding_dim': 64,
                'entropy_hidden_dim': 128,
                'entropy_num_layers': 1,
                'entropy_dropout': 0.1
            }
        self.patcher = DynamicEntropyPatcher(**patcher_config)

    def forward(self, state: torch.Tensor) -> Tuple[torch.Tensor, List[List[Tuple[int, int]]], torch.Tensor]:
        """
        Synthesizes a program as a sequence of byte patches from a given state.
        
        Args:
            state: A tensor representing the high-level state.
                   Shape: (batch_size, d_model)
                   
        Returns:
            A tuple containing:
            - encoded_patches: Tensor of patch embeddings. (B, max_patches, patch_embed_dim)
            - patch_indices: List of (start, end) indices for each patch.
            - entropy_aux_loss: Auxiliary loss from the patcher's entropy model.
        """
        device = state.device
        batch_size = state.size(0)
        
        # The 'state' is the memory for the decoder.
        memory = state.unsqueeze(1) # (B, 1, D)

        # --- Autoregressive Byte Generation ---
        # Start with a BOS token (let's use 0)
        generated_bytes = torch.zeros((batch_size, 1), dtype=torch.long, device=device)

        for _ in range(self.max_gen_len):
            tgt_emb = self.byte_embedding(generated_bytes)
            tgt_emb = self.pos_encoder(tgt_emb)
            
            # Create a causal mask for the decoder
            tgt_mask = nn.Transformer.generate_square_subsequent_mask(generated_bytes.size(1)).to(device)

            # Get output from decoder
            output = self.transformer_decoder(
                tgt=tgt_emb,
                memory=memory,
                tgt_mask=tgt_mask
            )
            
            # Project to byte vocabulary
            logits = self.output_projector(output[:, -1, :])
            
            # Sample the next byte (greedy decoding)
            next_byte = torch.argmax(logits, dim=-1).unsqueeze(1)
            
            # Append the new byte
            generated_bytes = torch.cat([generated_bytes, next_byte], dim=1)

        # Remove the BOS token
        generated_bytes = generated_bytes[:, 1:]
        
        # --- Patching the Generated Byte Sequence ---
        # The DynamicEntropyPatcher expects a uint8 tensor.
        encoded_patches, patch_indices, entropy_aux_loss = self.patcher(generated_bytes.byte())
        
        return encoded_patches, patch_indices, entropy_aux_loss


class PositionalEncoding(nn.Module):
    """Standard positional encoding for transformers."""
    def __init__(self, d_model: int, dropout: float = 0.1, max_len: int = 5000):
        super().__init__()
        self.dropout = nn.Dropout(p=dropout)
        self.d_model = d_model

        position = torch.arange(max_len).unsqueeze(1)
        div_term = torch.exp(torch.arange(0, d_model, 2) * (-math.log(10000.0) / d_model))
        pe = torch.zeros(max_len, 1, d_model)
        pe[:, 0, 0::2] = torch.sin(position * div_term)
        pe[:, 0, 1::2] = torch.cos(position * div_term)
        self.register_buffer('pe', pe)

    def forward(self, x: torch.Tensor) -> torch.Tensor:
        """
        Args:
            x: Tensor, shape [batch_size, seq_len, embedding_dim]
        """
        x = x * math.sqrt(self.d_model) # Scale input embeddings
        x = x + self.pe[:x.size(1)].transpose(0, 1)
        return self.dropout(x)