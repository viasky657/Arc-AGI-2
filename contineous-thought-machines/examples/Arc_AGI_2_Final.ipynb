{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "86502500",
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "print(os.getcwd())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "be570979",
      "metadata": {},
      "outputs": [],
      "source": [
        "        !pip install matplotlib\n",
        "        !pip install mediapy\n",
        "        !pip install numpy\n",
        "        # For advanced optimizations, consider installing the following:\n",
        "        !pip install accelerate\n",
        "        !pip install diffusers\n",
        "        !pip install xformers\n",
        "        !pip install seaborn"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c018e3a1",
      "metadata": {},
      "source": [
        "This section contains the set-up components for training the ctm model with the byte-level encoder with binary patches, ctm processing with synpase system set to multi-objective, \n",
        "\n",
        "\n",
        "\n",
        "and binary patches from the ctm (after 20 rounds of COT thinking) refined and trained with MCMC to encourage the model to have reasoning steps closely related to the best answer, \n",
        "\n",
        "\n",
        "\n",
        "Each epoch is saved as a safetensor checkpoint to preserve training progress."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0990b813",
      "metadata": {},
      "source": [
        "# -----------------------------------------------------------------------------\n",
        "# Dependency Installation Notes\n",
        "# -----------------------------------------------------------------------------\n",
        " The following dependencies are required. Please install them in your Python environment,\n",
        " for example, using pip:\n",
        "\n",
        " pip install mediapy\n",
        " pip install torch\n",
        " pip install safetensors\n",
        " pip install numpy\n",
        "\n",
        " For advanced optimizations, consider installing the following:\n",
        " pip install flash-attn --no-build-isolation\n",
        " pip install deepspeed\n",
        " pip install accelerate\n",
        " pip install xformers\n",
        "\n",
        " It's recommended to use a virtual environment.\n",
        " -----------------------------------------------------------------------------"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0980997a",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "-----------------------------------------------------------------------------\n",
            "Dependency Setup & Imports\n",
            "-----------------------------------------------------------------------------\n",
            "Please ensure all required dependencies are installed.\n",
            "Base dependencies: torchaudio, imageio, mediapy, torch, safetensors, numpy\n",
            "Optional optimization dependencies: flash-attn, deepspeed, accelerate, xformers\n",
            "See comments at the top of the script for installation commands.\n",
            "-----------------------------------------------------------------------------\n",
            "Base Checkpoint Directory: checkpoints\n",
            "Accelerate library found.\n",
            "xFormers library found.\n",
            "-----------------------------------------------------------------------------\n",
            "âœ… Accelerate available\n",
            "âœ… xFormers available - Expected 1.5-2x speedup\n",
            "Warning: mediapy not available. GIF preview will be limited.\n",
            "âœ… safetensors available for model checkpointing.\n",
            "\n",
            "ðŸš€ OPTIMIZATION STATUS:\n",
            "  âš¡ torch.compile: âœ…\n",
            "  ðŸ“ˆ Accelerate: âœ…\n",
            "  âš¡ xFormers: âœ…\n",
            "\n",
            "-----------------------------------------------------------------------------\n",
            "Setting up module paths...\n",
            "-----------------------------------------------------------------------------\n",
            "Added to sys.path: /workspaces/Arc-AGI-2/models\n",
            "Added to sys.path: /workspaces/Arc-AGI-2\n",
            "Added to sys.path: /workspaces/Arc-AGI-2/contineous-thought-machines\n",
            "\n",
            "-----------------------------------------------------------------------------\n",
            "Importing CTM and Dataloader modules...\n",
            "-----------------------------------------------------------------------------\n",
            "âœ“ Successfully imported EnhancedCTMDiffusion with ALL GPU optimizations\n",
            "  - Integration Flow one-step generation\n",
            "  - Task-Aware HiPA frequency enhancement\n",
            "  - CTM-guided diffusion control\n",
            "  - GPU memory optimizations\n",
            "  - Mixed precision training support\n",
            "\n",
            "-----------------------------------------------------------------------------\n",
            "Initializing Configuration for Integrated Diffusion CTM\n",
            "-----------------------------------------------------------------------------\n",
            "Using device: cuda\n",
            "âœ… Mixed precision training enabled (BF16) - Expected ~2x speedup\n"
          ]
        }
      ],
      "source": [
        "print(\"-----------------------------------------------------------------------------\")\n",
        "print(\"Dependency Setup & Imports\")\n",
        "print(\"-----------------------------------------------------------------------------\")\n",
        "print(\"Please ensure all required dependencies are installed.\")\n",
        "print(\"Base dependencies: torchaudio, imageio, mediapy, torch, safetensors, numpy\")\n",
        "print(\"Optional optimization dependencies: flash-attn, deepspeed, accelerate, xformers\")\n",
        "print(\"See comments at the top of the script for installation commands.\")\n",
        "print(\"-----------------------------------------------------------------------------\")\n",
        "\n",
        "import os\n",
        "import sys\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "import json\n",
        "import random\n",
        "import time\n",
        "from datetime import datetime\n",
        "from PIL import Image\n",
        "import matplotlib.pyplot as plt\n",
        "import glob\n",
        "from IPython.display import display, Markdown\n",
        "\n",
        "# Define the base directory for saving checkpoints\n",
        "CHECKPOINT_DIR = \"checkpoints\"\n",
        "os.makedirs(CHECKPOINT_DIR, exist_ok=True)\n",
        "print(f\"Base Checkpoint Directory: {CHECKPOINT_DIR}\")\n",
        "\n",
        "# Attempt to import optional dependencies and set flags\n",
        "try:\n",
        "    from accelerate import Accelerator\n",
        "    from accelerate.utils import DistributedDataParallelKwargs\n",
        "    HAS_ACCELERATE = True\n",
        "    print(\"Accelerate library found.\")\n",
        "except ImportError:\n",
        "    HAS_ACCELERATE = False\n",
        "    print(\"Accelerate library not found. Some features like multi-GPU training might be limited.\")\n",
        "\n",
        "try:\n",
        "    import xformers.ops as xops\n",
        "    HAS_XFORMERS = True\n",
        "    print(\"xFormers library found.\")\n",
        "except ImportError:\n",
        "    HAS_XFORMERS = False\n",
        "    print(\"xFormers library not found.\")\n",
        "print(\"-----------------------------------------------------------------------------\")\n",
        "\n",
        "import numpy as np\n",
        "import math\n",
        "import time\n",
        "\n",
        "# OPTIMIZATION: Advanced optimization imports\n",
        "try:\n",
        "    from accelerate import Accelerator\n",
        "    ACCELERATE_AVAILABLE = True\n",
        "    print(\"âœ… Accelerate available\")\n",
        "except ImportError:\n",
        "    ACCELERATE_AVAILABLE = False\n",
        "    print(\"âš ï¸ Accelerate not available\")\n",
        "\n",
        "try:\n",
        "    import xformers\n",
        "    import xformers.ops\n",
        "    XFORMERS_AVAILABLE = True\n",
        "    print(\"âœ… xFormers available - Expected 1.5-2x speedup\")\n",
        "except ImportError:\n",
        "    XFORMERS_AVAILABLE = False\n",
        "    print(\"âš ï¸ xFormers not available\")\n",
        "\n",
        "# Try to import mediapy, fallback if not available\n",
        "try:\n",
        "    import mediapy\n",
        "    MEDIAPY_AVAILABLE = True\n",
        "except ImportError:\n",
        "    MEDIAPY_AVAILABLE = False\n",
        "    print(\"Warning: mediapy not available. GIF preview will be limited.\")\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from torch.nn import functional as F\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "from torch.utils.checkpoint import checkpoint  # OPTIMIZATION: Gradient checkpointing\n",
        "\n",
        "print(\"\\nðŸš€ OPTIMIZATION STATUS:\")\n",
        "print(f\"  âš¡ torch.compile: {'âœ…' if hasattr(torch, 'compile') else 'âŒ'}\")\n",
        "print(f\"  ðŸ“ˆ Accelerate: {'âœ…' if ACCELERATE_AVAILABLE else 'âŒ'}\")\n",
        "print(f\"  âš¡ xFormers: {'âœ…' if XFORMERS_AVAILABLE else 'âŒ'}\")\n",
        "\n",
        "# Add module paths\n",
        "# IMPORTANT: These paths assume the script is run from a directory where '..'\n",
        "# correctly points to the project root relative to 'models' and 'tasks' folders.\n",
        "# Adjust if your script is located elsewhere.\n",
        "print(\"\\n-----------------------------------------------------------------------------\")\n",
        "print(\"Setting up module paths...\")\n",
        "print(\"-----------------------------------------------------------------------------\")\n",
        "try:\n",
        "    current_script_path = os.path.dirname(os.path.abspath(__file__))\n",
        "except NameError: # __file__ is not defined in interactive shells, use os.getcwd()\n",
        "    current_script_path = os.getcwd()\n",
        "\n",
        "module_paths = [\n",
        "    os.path.abspath(os.path.join(current_script_path, '..', 'models')),\n",
        "    os.path.abspath(os.path.join(current_script_path, '..'))\n",
        "]\n",
        "module_paths.append(os.path.abspath('contineous-thought-machines'))\n",
        "for path in module_paths:\n",
        "    if path not in sys.path:\n",
        "        sys.path.append(path)\n",
        "        print(f\"Added to sys.path: {path}\")\n",
        "\n",
        "# Import Enhanced CTM with diffusion control and all optimizations.\n",
        "# OPTIMIZED_CTM_CONFIG_ARC will be defined below if imports are successful.\n",
        "print(\"\\n-----------------------------------------------------------------------------\")\n",
        "print(\"Importing CTM and Dataloader modules...\")\n",
        "print(\"-----------------------------------------------------------------------------\")\n",
        "EnhancedCTMDiffusion = None # Initialize to None\n",
        "ENHANCED_MCMC_AVAILABLE = False # Initialize\n",
        "\n",
        "try:\n",
        "    from models.ctm_Diffusion_NEWNEW import (\n",
        "        EnhancedCTMDiffusion,\n",
        "        #EnhancedCTMConfig, #This is turned off since it is included in the notebook set-up phase now to avoid undefined errors. \n",
        "        CTMControlledDiffusionProcessor,\n",
        "        FrequencyDomainAwareAttention,\n",
        "        IntegrationFlowHiPASampler,\n",
        "        CTMIntegrationFlowTrainer,\n",
        "    )\n",
        "    print(\"âœ“ Successfully imported EnhancedCTMDiffusion with ALL GPU optimizations\")\n",
        "    print(\"  - Integration Flow one-step generation\")\n",
        "    print(\"  - Task-Aware HiPA frequency enhancement\")\n",
        "    print(\"  - CTM-guided diffusion control\")\n",
        "    print(\"  - GPU memory optimizations\")\n",
        "    print(\"  - Mixed precision training support\")\n",
        "\n",
        "except ImportError as e_ctm:\n",
        "    print(f\"âŒ Error importing Enhanced CTM or related components: {e_ctm}\")\n",
        "    print(\"   Please ensure 'models/ctm_Diffusion_NEWNEW_.py' components exist and are accessible.\")\n",
        "    EnhancedCTMDiffusion = None\n",
        "\n",
        "# -----------------------------------------------------------------------------\n",
        "# Configuration for Integrated Diffusion CTM\n",
        "# -----------------------------------------------------------------------------\n",
        "print(\"\\n-----------------------------------------------------------------------------\")\n",
        "print(\"Initializing Configuration for Integrated Diffusion CTM\")\n",
        "print(\"-----------------------------------------------------------------------------\")\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "# ðŸš€ OPTIMIZATION 1: Enhanced Mixed Precision Training Setup (FP16/BF16)\n",
        "USE_MIXED_PRECISION = torch.cuda.is_available() and hasattr(torch.cuda, 'amp') and device.type == 'cuda'\n",
        "USE_BFLOAT16 = USE_MIXED_PRECISION and torch.cuda.is_bf16_supported()\n",
        "\n",
        "autocast_dtype = torch.float32 # Default\n",
        "if USE_MIXED_PRECISION:\n",
        "    from torch.cuda.amp import GradScaler, autocast\n",
        "    if USE_BFLOAT16:\n",
        "        scaler = torch.amp.GradScaler(\"cuda\", enabled=True)\n",
        "        autocast_dtype = torch.bfloat16\n",
        "        print(\"âœ… Mixed precision training enabled (BF16) - Expected ~2x speedup\")\n",
        "    else:\n",
        "        scaler = torch.amp.GradScaler(\"cuda\", enabled=True)\n",
        "        autocast_dtype = torch.float16\n",
        "        print(\"âœ… Mixed precision training enabled (FP16) - Expected ~2x speedup\")\n",
        "else:\n",
        "    scaler = None\n",
        "    class dummy_autocast:\n",
        "        def __enter__(self): return None\n",
        "        def __exit__(self, exc_type, exc_val, exc_tb): return False\n",
        "    autocast = dummy_autocast\n",
        "    autocast_dtype = torch.float32\n",
        "    print(\"âš ï¸ Mixed precision training not available (CPU or older GPU or torch.cuda.amp not found)\")\n",
        "\n",
        "# ðŸš€ OPTIMIZATION 2: Gradient Accumulation Configuration\n",
        "GRADIENT_ACCUMULATION_STEPS = 4\n",
        "MAX_GRAD_NORM = 1.0\n",
        "\n",
        "# ðŸš€ OPTIMIZATION 4: Data Loading Optimizations\n",
        "OPTIMIZED_DATALOADER_CONFIG = {\n",
        "    'num_workers': min(8, os.cpu_count() if os.cpu_count() else 1),\n",
        "    'pin_memory': torch.cuda.is_available(),\n",
        "    'persistent_workers': True if min(8, os.cpu_count() if os.cpu_count() else 1) > 0 else False,\n",
        "    'prefetch_factor': 4 if min(8, os.cpu_count() if os.cpu_count() else 1) > 0 else None,\n",
        "}\n",
        "\n",
        "# General Training Parameters (can be overridden by specific phases)\n",
        "LEARNING_RATE = 1e-4"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3897a7e9",
      "metadata": {},
      "source": [
        "# Injected MCMC Components and EnhancedCTMFenchelYoungIntegration Initialization\n",
        "\n",
        "This notebook contains the Python code for MCMC components, including ARC-specific output spaces and an enhanced Fenchel-Young integration layer, structured for use in a Jupyter environment."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1609087c",
      "metadata": {},
      "source": [
        "## 1. Imports\n",
        "All necessary libraries and modules are imported here."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "16743809",
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import random\n",
        "from typing import Optional, Callable, Tuple, Dict, Any, List, Union\n",
        "from dataclasses import dataclass, field\n",
        "import math\n",
        "import numpy as np\n",
        "import warnings # For ARCGridOutputSpace warnings"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "02621907",
      "metadata": {},
      "source": [
        "## 2. Utility Functions (from `models.utils`)\n",
        "Helper functions used across various modules."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "96281960",
      "metadata": {},
      "outputs": [],
      "source": [
        "def add_coord_dim(x, scaled=True):\n",
        "    \"\"\"\n",
        "    Adds a final dimension to the tensor representing 2D coordinates.\n",
        "    \"\"\"\n",
        "    B, H, W = x.shape\n",
        "    x_coords = torch.arange(W, device=x.device, dtype=x.dtype).repeat(H, 1)\n",
        "    y_coords = torch.arange(H, device=x.device, dtype=x.dtype).unsqueeze(-1).repeat(1, W)\n",
        "    if scaled:\n",
        "        x_coords = x_coords / (W - 1) if W > 1 else torch.zeros_like(x_coords)\n",
        "        y_coords = y_coords / (H - 1) if H > 1 else torch.zeros_like(y_coords)\n",
        "    coords = torch.stack((x_coords, y_coords), dim=-1)\n",
        "    coords = coords.unsqueeze(0) \n",
        "    coords = coords.repeat(B, 1, 1, 1) \n",
        "    return coords\n",
        "\n",
        "def compute_normalized_entropy(logits, reduction='mean'):\n",
        "    \"\"\"\n",
        "    Calculates the normalized entropy of a PyTorch tensor of logits along the \n",
        "    final dimension.\n",
        "    \"\"\"\n",
        "    preds = F.softmax(logits, dim=-1)\n",
        "    log_preds = torch.log_softmax(logits, dim=-1)\n",
        "    entropy = -torch.sum(preds * log_preds, dim=-1)\n",
        "    num_classes = preds.shape[-1]\n",
        "    if num_classes <= 1: # Avoid log(1)=0 or log(0)\n",
        "        return torch.zeros_like(entropy)\n",
        "    max_entropy = torch.log(torch.tensor(num_classes, dtype=torch.float32, device=logits.device))\n",
        "    if max_entropy == 0: # Should only happen if num_classes is 1\n",
        "        return torch.zeros_like(entropy)\n",
        "    normalized_entropy = entropy / max_entropy\n",
        "    if len(logits.shape) > 2 and reduction == 'mean':\n",
        "        normalized_entropy = normalized_entropy.flatten(1).mean(-1)\n",
        "    return normalized_entropy"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8613871c",
      "metadata": {},
      "source": [
        "## 3. Core Modules (from `models.modules`)\n",
        "Custom neural network layers."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "87193681",
      "metadata": {},
      "outputs": [],
      "source": [
        "class SuperLinear(nn.Module):\n",
        "    def __init__(self,\n",
        "                 in_dims,\n",
        "                 out_dims,\n",
        "                 N,\n",
        "                 T=1.0,\n",
        "                 do_norm=False,\n",
        "                 dropout=0):\n",
        "        super().__init__()\n",
        "        self.dropout = nn.Dropout(dropout) if dropout > 0 else nn.Identity()\n",
        "        self.in_dims = in_dims\n",
        "        self.layernorm = nn.LayerNorm(in_dims, elementwise_affine=True) if do_norm else nn.Identity()\n",
        "        self.do_norm = do_norm\n",
        "        self.register_parameter('w1', nn.Parameter(\n",
        "            torch.empty((in_dims, out_dims, N)).uniform_(\n",
        "                -1/math.sqrt(in_dims + out_dims),\n",
        "                 1/math.sqrt(in_dims + out_dims)\n",
        "            ), requires_grad=True)\n",
        "        )\n",
        "        self.register_parameter('b1', nn.Parameter(torch.zeros((1, N, out_dims)), requires_grad=True))\n",
        "        self.register_parameter('T', nn.Parameter(torch.Tensor([T]))) \n",
        "\n",
        "    def forward(self, x):\n",
        "        out = self.dropout(x)\n",
        "        out = self.layernorm(out)\n",
        "        out = torch.einsum('BDM,MHD->BDH', out, self.w1) + self.b1\n",
        "        out = out.squeeze(-1) / self.T\n",
        "        return out"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "09781309",
      "metadata": {},
      "source": [
        "## 4. MCMC Interpretability Solver Components (from `models.mcmc_interpretability_solver`)\n",
        "Dataclasses and hooks for tracking and interpreting MCMC processes and solver states."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "09183097",
      "metadata": {},
      "outputs": [],
      "source": [
        "@dataclass\n",
        "class ThoughtStep:\n",
        "    step_id: int\n",
        "    layer_name: str\n",
        "    input_state: Optional[torch.Tensor] = None\n",
        "    output_state: Optional[torch.Tensor] = None\n",
        "    attention_weights: Optional[torch.Tensor] = None\n",
        "    mcmc_samples: Optional[torch.Tensor] = None\n",
        "    confidence_score: float = 0.0\n",
        "    reasoning_vector: Optional[torch.Tensor] = None\n",
        "    energy_landscape: Dict[str, float] = field(default_factory=dict)\n",
        "    correction_ratio: Optional[float] = None\n",
        "    metadata: Dict[str, Any] = field(default_factory=dict)\n",
        "\n",
        "@dataclass\n",
        "class ReasoningChain:\n",
        "    input_data: Optional[torch.Tensor] = None\n",
        "    thought_steps: List[ThoughtStep] = field(default_factory=list)\n",
        "    final_output: Optional[torch.Tensor] = None\n",
        "    confidence_trajectory: List[float] = field(default_factory=list)\n",
        "    decision_points: List[int] = field(default_factory=list)\n",
        "    reasoning_summary: str = \"\"\n",
        "    convergence_metrics: Dict[str, float] = field(default_factory=dict)\n",
        "    solver_diagnostics: List[Dict[str, Any]] = field(default_factory=list)\n",
        "\n",
        "class MCMCInterpretabilityHook:\n",
        "    def __init__(self, layer_name: str):\n",
        "        self.layer_name = layer_name\n",
        "        self.activations: List[Dict[str, Any]] = []\n",
        "        self.gradients: List[torch.Tensor] = []\n",
        "        self.attention_maps: List[Optional[torch.Tensor]] = []\n",
        "        self.mcmc_states: List[Optional[torch.Tensor]] = []\n",
        "        self.energy_values: List[float] = []\n",
        "        self.correction_ratios: List[Optional[float]] = []\n",
        "        self.solver_diagnostics: List[Dict[str, Any]] = []\n",
        "\n",
        "    def forward_hook(self, module, input_data, output_data):\n",
        "        input_tensor = input_data[0] if isinstance(input_data, tuple) else input_data\n",
        "        self.activations.append({\n",
        "            'input': input_tensor.detach().clone() if torch.is_tensor(input_tensor) else input_tensor,\n",
        "            'output': output_data.detach().clone() if torch.is_tensor(output_data) else output_data,\n",
        "            'layer': self.layer_name,\n",
        "            'timestamp': len(self.activations)\n",
        "        })\n",
        "        if hasattr(module, 'mcmc_samples') and module.mcmc_samples is not None:\n",
        "            self.mcmc_states.append(module.mcmc_samples.detach().clone())\n",
        "        if hasattr(module, 'attention_weights') and module.attention_weights is not None:\n",
        "            self.attention_maps.append(module.attention_weights.detach().clone())\n",
        "        if hasattr(module, 'correction_ratios_log') and module.correction_ratios_log: # Assuming a log attribute\n",
        "            self.correction_ratios.append(module.correction_ratios_log[-1])\n",
        "        if hasattr(module, 'solver_diagnostics_log') and module.solver_diagnostics_log: # Assuming a log attribute\n",
        "            diagnostics = module.solver_diagnostics_log[-1]\n",
        "            self.solver_diagnostics.append(diagnostics)\n",
        "            if 'last_objective_value' in diagnostics:\n",
        "                self.energy_values.append(diagnostics['last_objective_value'])\n",
        "    \n",
        "    def backward_hook(self, module, grad_input, grad_output):\n",
        "        if grad_output and grad_output[0] is not None:\n",
        "            self.gradients.append(grad_output[0].detach().clone())\n",
        "\n",
        "class BlackBoxSolver:\n",
        "    def __init__(self, model: nn.Module, device: str = 'cpu'): # Default to CPU if not specified\n",
        "        self.model = model\n",
        "        self.device = device\n",
        "        self.hooks: Dict[str, MCMCInterpretabilityHook] = {}\n",
        "        self.reasoning_chains: List[ReasoningChain] = []\n",
        "\n",
        "    def _register_hooks(self):\n",
        "        for name, module in self.model.named_modules():\n",
        "            if any(keyword in name.lower() for keyword in ['mcmc', 'enhanced', 'correction', 'fenchel', 'oracle']):\n",
        "                if name not in self.hooks:\n",
        "                    hook = MCMCInterpretabilityHook(name)\n",
        "                    self.hooks[name] = hook\n",
        "                    module.register_forward_hook(hook.forward_hook)\n",
        "    \n",
        "    def clear_hooks_data(self):\n",
        "        for hook in self.hooks.values():\n",
        "            hook.activations.clear()\n",
        "            hook.gradients.clear()\n",
        "            hook.attention_maps.clear()\n",
        "            hook.mcmc_states.clear()\n",
        "            hook.energy_values.clear()\n",
        "            hook.correction_ratios.clear()\n",
        "            hook.solver_diagnostics.clear()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "07803978",
      "metadata": {},
      "source": [
        "## 5. Base MCMC Components (from `models.fenchel_young_mcmc`)\n",
        "Core classes for MCMC sampling, including configuration, temperature scheduling, and output space representation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0397809a",
      "metadata": {},
      "outputs": [],
      "source": [
        "@dataclass\n",
        "class MCMCConfig:\n",
        "    num_chains: int = 3\n",
        "    chain_length: int = 20\n",
        "    burn_in: int = 5\n",
        "    temperature_schedule: str = \"geometric\"\n",
        "    initial_temp: float = 10.0\n",
        "    final_temp: float = 1.0\n",
        "    decay_rate: float = 0.995\n",
        "    neighborhood_radius: int = 1 # This is a general parameter, interpretation depends on OutputSpace\n",
        "    initialization_method: str = \"persistent\"\n",
        "\n",
        "class TemperatureScheduler:\n",
        "    @staticmethod\n",
        "    def geometric(initial_temp: float, decay_rate: float, final_temp: float):\n",
        "        def schedule(step: int) -> float:\n",
        "            return max(initial_temp * (decay_rate ** step), final_temp)\n",
        "        return schedule\n",
        "\n",
        "    @staticmethod\n",
        "    def linear(initial_temp: float, final_temp: float, total_steps: int):\n",
        "        def schedule(step: int) -> float:\n",
        "            progress = min(step / total_steps, 1.0) if total_steps > 0 else 1.0\n",
        "            return initial_temp * (1 - progress) + final_temp * progress\n",
        "        return schedule\n",
        "\n",
        "    @staticmethod\n",
        "    def constant(temperature: float):\n",
        "        def schedule(step: int) -> float:\n",
        "            return temperature\n",
        "        return schedule\n",
        "\n",
        "class DiscreteOutputSpace:\n",
        "    def __init__(self, dimension: int):\n",
        "        self.dimension = dimension\n",
        "        self._full_output_space_generated = False\n",
        "        self.output_space: List[torch.Tensor] = []\n",
        "        if self.dimension <= 4: \n",
        "            try:\n",
        "                self.output_space = self._generate_space()\n",
        "                self._full_output_space_generated = True\n",
        "            except (NotImplementedError, ValueError):\n",
        "                self.output_space = []\n",
        "\n",
        "    def _generate_space(self) -> List[torch.Tensor]:\n",
        "        raise NotImplementedError(\"Subclasses must implement _generate_space or rely on _generate_random_member_directly\")\n",
        "\n",
        "    def get_available_neighborhood_strategies(self, state: Optional[torch.Tensor] = None) -> List[str]:\n",
        "        raise NotImplementedError(\"Subclasses must implement get_available_neighborhood_strategies\")\n",
        "\n",
        "    def get_neighbors(self, state: torch.Tensor, strategy_name: str, **strategy_params) -> List[torch.Tensor]:\n",
        "        raise NotImplementedError(\"Subclasses must implement get_neighbors\")\n",
        "\n",
        "    def get_proposal_prob(self, current_state: torch.Tensor, proposed_state: torch.Tensor, strategy_name: str, **strategy_params) -> float:\n",
        "        neighbors = self.get_neighbors(current_state, strategy_name, **strategy_params)\n",
        "        if not neighbors: return 0.0\n",
        "        is_neighbor = any(torch.allclose(neighbor, proposed_state) for neighbor in neighbors)\n",
        "        return (1.0 / len(neighbors)) if is_neighbor else 0.0\n",
        "    \n",
        "    def _generate_random_member_directly(self) -> Optional[torch.Tensor]:\n",
        "        return None\n",
        "\n",
        "    def random_state(self) -> torch.Tensor:\n",
        "        direct_sample = self._generate_random_member_directly()\n",
        "        if direct_sample is not None:\n",
        "            return direct_sample\n",
        "        if self._full_output_space_generated and self.output_space:\n",
        "            return random.choice(self.output_space).clone()\n",
        "        if not self.output_space and not self._full_output_space_generated:\n",
        "            try:\n",
        "                self.output_space = self._generate_space()\n",
        "                self._full_output_space_generated = True\n",
        "                if self.output_space:\n",
        "                    return random.choice(self.output_space).clone()\n",
        "            except (NotImplementedError, ValueError) as e:\n",
        "                raise RuntimeError(f\"Cannot generate random_state for {self.__class__.__name__} (dim {self.dimension}). Error: {e}\")\n",
        "        if not self.output_space:\n",
        "             raise RuntimeError(f\"Output space empty for {self.__class__.__name__} (dim {self.dimension}). Cannot sample random_state.\")\n",
        "        return random.choice(self.output_space).clone()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0870397a",
      "metadata": {},
      "source": [
        "## 6. ARC Grid Output Space\n",
        "A specific implementation of `DiscreteOutputSpace` for ARC-like grid environments."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0397a087",
      "metadata": {},
      "outputs": [],
      "source": [
        "class ARCGridOutputSpace(DiscreteOutputSpace):\n",
        "    def __init__(self, dimension: int, grid_shape: Tuple[int, int], num_symbols: int):\n",
        "        super().__init__(dimension)\n",
        "        self.grid_shape = grid_shape\n",
        "        self.num_symbols = num_symbols\n",
        "        if dimension != grid_shape[0] * grid_shape[1]:\n",
        "            raise ValueError(f\"Dimension ({dimension}) must match grid_shape ({grid_shape[0]}*{grid_shape[1]}={grid_shape[0]*grid_shape[1]})\")\n",
        "\n",
        "    def _generate_random_member_directly(self) -> Optional[torch.Tensor]:\n",
        "        random_grid = torch.randint(0, self.num_symbols, self.grid_shape, dtype=torch.long)\n",
        "        return random_grid.view(-1).float()\n",
        "\n",
        "    def get_available_neighborhood_strategies(self, state: Optional[torch.Tensor] = None) -> List[str]:\n",
        "        return [\"flip_one_cell_value\", \"swap_two_cells\"]\n",
        "\n",
        "    def get_neighbors(self, state: torch.Tensor, strategy_name: str, **strategy_params) -> List[torch.Tensor]:\n",
        "        neighbors = []\n",
        "        state_grid = state.view(self.grid_shape).long()\n",
        "\n",
        "        if strategy_name == \"flip_one_cell_value\":\n",
        "            num_neighbors_to_generate = strategy_params.get('num_flips', min(5, self.dimension))\n",
        "            for _ in range(num_neighbors_to_generate):\n",
        "                neighbor_grid = state_grid.clone()\n",
        "                row = random.randint(0, self.grid_shape[0] - 1)\n",
        "                col = random.randint(0, self.grid_shape[1] - 1)\n",
        "                original_value = neighbor_grid[row, col].item()\n",
        "                \n",
        "                if self.num_symbols <= 1:\n",
        "                    new_value = original_value\n",
        "                else:\n",
        "                    new_value = random.randint(0, self.num_symbols - 1)\n",
        "                    while new_value == original_value:\n",
        "                        new_value = random.randint(0, self.num_symbols - 1)\n",
        "                neighbor_grid[row, col] = new_value\n",
        "                neighbors.append(neighbor_grid.view(-1).float())\n",
        "        \n",
        "        elif strategy_name == \"swap_two_cells\":\n",
        "            num_neighbors_to_generate = strategy_params.get('num_swaps', min(5, self.dimension // 2 if self.dimension >=2 else 0))\n",
        "            for _ in range(num_neighbors_to_generate):\n",
        "                if self.dimension < 2: break\n",
        "                neighbor_grid = state_grid.clone()\n",
        "                r1, c1 = random.randint(0, self.grid_shape[0] - 1), random.randint(0, self.grid_shape[1] - 1)\n",
        "                r2, c2 = random.randint(0, self.grid_shape[0] - 1), random.randint(0, self.grid_shape[1] - 1)\n",
        "                while r1 == r2 and c1 == c2:\n",
        "                    r2, c2 = random.randint(0, self.grid_shape[0] - 1), random.randint(0, self.grid_shape[1] - 1)\n",
        "                \n",
        "                val1 = neighbor_grid[r1,c1].item()\n",
        "                neighbor_grid[r1,c1] = neighbor_grid[r2,c2].item()\n",
        "                neighbor_grid[r2,c2] = val1\n",
        "                neighbors.append(neighbor_grid.view(-1).float())\n",
        "        else:\n",
        "            warnings.warn(f\"Unknown strategy: {strategy_name} for ARCGridOutputSpace. Returning empty neighbor list.\")\n",
        "        return neighbors\n",
        "\n",
        "    def _generate_space(self) -> List[torch.Tensor]:\n",
        "        if self.dimension > 6:\n",
        "            warnings.warn(f\"Full space generation for ARCGridOutputSpace with dimension {self.dimension} is too large. Returning empty list.\")\n",
        "            return []\n",
        "        return super()._generate_space()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0790397a",
      "metadata": {},
      "source": [
        "## 7. Enhanced MCMC Layers and Fenchel-Young Integration\n",
        "Includes `ExactOptimizationOracle`, MCMC samplers (`CorrectionRatioMCMC`, `LargeNeighborhoodSearchMCMC`), and the main `EnhancedCTMFenchelYoungIntegration` module."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0397a079",
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "class ExactOptimizationOracle:\n",
        "    def __init__(self, output_space: DiscreteOutputSpace, phi_network: Optional[nn.Module] = None, model: Optional[nn.Module] = None):\n",
        "        self.output_space = output_space\n",
        "        self.phi_network = phi_network\n",
        "        self.solver_state: Dict[str, Any] = {\n",
        "            'last_solution': None, 'last_objective_value': None,\n",
        "            'num_evaluations': 0, 'optimization_history': []\n",
        "        }\n",
        "\n",
        "    def solve(self, theta: torch.Tensor, neighborhood: Optional[List[torch.Tensor]] = None) -> Optional[torch.Tensor]:\n",
        "        search_space = neighborhood\n",
        "        if search_space is None:\n",
        "            if hasattr(self.output_space, '_generate_random_member_directly') and \\\n",
        "               (not hasattr(self.output_space, 'output_space') or not self.output_space.output_space):\n",
        "                # Try to generate a small random search space if the full one is not available/too large\n",
        "                # Ensure getattr has a default for 'dimension' if it might be missing\n",
        "                dimension_val = getattr(self.output_space, 'dimension', 20) \n",
        "                search_space = [self.output_space._generate_random_member_directly() for _ in range(min(dimension_val, 20))]\n",
        "                search_space = [s for s in search_space if s is not None]\n",
        "            elif hasattr(self.output_space, 'output_space'): # Check if output_space attribute exists\n",
        "                search_space = self.output_space.output_space\n",
        "            else: # Fallback if no way to get/generate search space\n",
        "                search_space = []\n",
        "\n",
        "        # Initialize num_evaluations at the beginning of the method.\n",
        "        # It's reset per call to solve.\n",
        "        self.solver_state['num_evaluations'] = 0\n",
        "        \n",
        "        # Ensure optimization_history is initialized if it's not already present\n",
        "        if 'optimization_history' not in self.solver_state:\n",
        "            self.solver_state['optimization_history'] = []\n",
        "\n",
        "        if not search_space:\n",
        "            self.solver_state['last_solution'] = None\n",
        "            self.solver_state['last_objective_value'] = float('-inf')\n",
        "            self.solver_state['optimization_history'].append({\n",
        "                'solution': None,\n",
        "                'value': float('-inf'),\n",
        "                'search_space_size': 0\n",
        "            })\n",
        "            return None\n",
        "\n",
        "        is_batched = theta.ndim == 2\n",
        "        batch_size = theta.shape[0] if is_batched else 1\n",
        "\n",
        "        if is_batched:\n",
        "            # Stores the best candidate tensor for each item in the batch\n",
        "            best_solution_list: List[Optional[torch.Tensor]] = [None] * batch_size \n",
        "            best_value_tensor = torch.full((batch_size,), float('-inf'), device=theta.device, dtype=theta.dtype)\n",
        "        else:\n",
        "            best_solution_single: Optional[torch.Tensor] = None\n",
        "            best_value_scalar = float('-inf')\n",
        "        \n",
        "        for candidate_state_maybe_none in search_space:\n",
        "            if candidate_state_maybe_none is None:\n",
        "                continue\n",
        "            # Ensure candidate is on the same device as theta and has the same dtype\n",
        "            candidate = candidate_state_maybe_none.to(device=theta.device, dtype=theta.dtype)\n",
        "\n",
        "            current_objective_value: Union[torch.Tensor, float] \n",
        "\n",
        "            if is_batched:\n",
        "                # theta is [B, D'], candidate is [D'] -> objective_value_batch is [B]\n",
        "                current_objective_value = torch.mv(theta, candidate)\n",
        "            else:\n",
        "                # theta is [D'], candidate is [D'] -> objective_value_scalar is scalar\n",
        "                current_objective_value = torch.dot(theta, candidate)\n",
        "\n",
        "            if self.phi_network is not None:\n",
        "                # candidate is [D'], phi_network expects [N, D']\n",
        "                phi_input = candidate.unsqueeze(0) # [1, D']\n",
        "                phi_val = self.phi_network(phi_input) # Output [1, 1] or [1]\n",
        "                \n",
        "                # Squeeze to make it a scalar or 1D tensor if it was [1,1] or [1]\n",
        "                phi_val_squeezed = phi_val.squeeze()\n",
        "                \n",
        "                # Ensure phi_val_squeezed is a scalar tensor before adding\n",
        "                if phi_val_squeezed.ndim > 0 and phi_val_squeezed.numel() == 1:\n",
        "                    phi_val_squeezed = phi_val_squeezed.squeeze()\n",
        "\n",
        "                # Add scalar phi_val to objective_value (scalar or [B] tensor)\n",
        "                # This works due to broadcasting if current_objective_value is [B]\n",
        "                current_objective_value = current_objective_value + phi_val_squeezed # Ensure it's an assignment\n",
        "            \n",
        "            if is_batched:\n",
        "                # current_objective_value is a tensor of shape [B]\n",
        "                # best_value_tensor is a tensor of shape [B]\n",
        "                improved_mask = current_objective_value > best_value_tensor\n",
        "                best_value_tensor[improved_mask] = current_objective_value[improved_mask]\n",
        "                for i in range(batch_size):\n",
        "                    if improved_mask[i]:\n",
        "                        best_solution_list[i] = candidate.clone()\n",
        "            else: # not batched, current_objective_value is a scalar float or 0-dim tensor\n",
        "                obj_val_float: float\n",
        "                if isinstance(current_objective_value, torch.Tensor): # Ensure it's a Python float for comparison\n",
        "                    obj_val_float = current_objective_value.item()\n",
        "                else:\n",
        "                    # This case should ideally not happen if operations are tensor-based\n",
        "                    obj_val_float = float(current_objective_value) \n",
        "                \n",
        "                if obj_val_float > best_value_scalar:\n",
        "                    best_value_scalar = obj_val_float\n",
        "                    best_solution_single = candidate.clone()\n",
        "            \n",
        "            self.solver_state['num_evaluations'] += 1\n",
        "\n",
        "        # Update solver_state and determine return value\n",
        "        final_return_solution: Optional[torch.Tensor] = None\n",
        "\n",
        "        if is_batched:\n",
        "            # For solver_state, use the first item of the batch as a compromise\n",
        "            actual_best_solution_for_state = best_solution_list[0] if best_solution_list and best_solution_list[0] is not None else None\n",
        "            actual_best_value_for_state = float(best_value_tensor[0].item()) if best_value_tensor.numel() > 0 else float('-inf')\n",
        "\n",
        "            self.solver_state['last_solution'] = actual_best_solution_for_state.clone() if actual_best_solution_for_state is not None else None\n",
        "            self.solver_state['last_objective_value'] = actual_best_value_for_state\n",
        "            self.solver_state['optimization_history'].append({\n",
        "                'solution': actual_best_solution_for_state.clone().cpu().numpy() if actual_best_solution_for_state is not None else None,\n",
        "                'value': actual_best_value_for_state,\n",
        "                'search_space_size': len(search_space)\n",
        "            })\n",
        "\n",
        "            # For return value: if any item in batch failed to find a solution, return None. Otherwise, stack.\n",
        "            if any(s is None for s in best_solution_list):\n",
        "                final_return_solution = None\n",
        "            else:\n",
        "                # All solutions are tensors, safe to stack.\n",
        "                # Need to cast best_solution_list to List[torch.Tensor] for stack\n",
        "                final_return_solution = torch.stack([s for s in best_solution_list if s is not None])\n",
        "        \n",
        "        else: # not batched\n",
        "            self.solver_state['last_solution'] = best_solution_single.clone() if best_solution_single is not None else None\n",
        "            self.solver_state['last_objective_value'] = float(best_value_scalar)\n",
        "            self.solver_state['optimization_history'].append({\n",
        "                'solution': best_solution_single.clone().cpu().numpy() if best_solution_single is not None else None,\n",
        "                'value': float(best_value_scalar),\n",
        "                'search_space_size': len(search_space)\n",
        "            })\n",
        "            final_return_solution = best_solution_single\n",
        "            \n",
        "        return final_return_solution\n",
        "\n",
        "    def get_solver_state(self) -> Dict[str, Any]:\n",
        "        return self.solver_state.copy()\n",
        "\n",
        "    def set_solver_parameters(self, params: Dict[str, Any]) -> None:\n",
        "        if 'reset_history' in params and params['reset_history']:\n",
        "            self.solver_state['optimization_history'] = []\n",
        "            self.solver_state['num_evaluations'] = 0\n",
        "\n",
        "\n",
        "class CorrectionRatioMCMC(nn.Module):\n",
        "    def __init__(self,\n",
        "                 output_space: DiscreteOutputSpace,\n",
        "                 config: MCMCConfig,\n",
        "                 phi_network: Optional[nn.Module] = None,\n",
        "                 exact_oracle: Optional[ExactOptimizationOracle] = None):\n",
        "        super().__init__()\n",
        "        self.output_space = output_space\n",
        "        self.config = config\n",
        "        self.phi_network = phi_network\n",
        "        self.exact_oracle = exact_oracle\n",
        "        self.temp_scheduler = self._create_temperature_scheduler()\n",
        "        self.persistent_states: Optional[List[Optional[torch.Tensor]]] = None\n",
        "        self.correction_ratios_log: List[float] = []\n",
        "        self.solver_diagnostics_log: List[Dict[str, Any]] = []\n",
        "        self.step_count = 0\n",
        "\n",
        "    def _create_temperature_scheduler(self) -> Callable[[int], float]:\n",
        "        if self.config.temperature_schedule == \"geometric\":\n",
        "            return TemperatureScheduler.geometric(self.config.initial_temp, self.config.decay_rate, self.config.final_temp)\n",
        "        elif self.config.temperature_schedule == \"linear\":\n",
        "            return TemperatureScheduler.linear(self.config.initial_temp, self.config.final_temp, self.config.chain_length)\n",
        "        else:\n",
        "            return TemperatureScheduler.constant(self.config.final_temp)\n",
        "\n",
        "    def phi_function(self, state: torch.Tensor) -> torch.Tensor:\n",
        "        if self.phi_network is not None:\n",
        "            state_for_phi = state.unsqueeze(0) if state.dim() == self.output_space.dimension.bit_length() else state\n",
        "            if state_for_phi.dim() == 1:\n",
        "                state_for_phi = state_for_phi.unsqueeze(0)\n",
        "\n",
        "            return self.phi_network(state_for_phi).squeeze()\n",
        "        return torch.tensor(0.0, device=state.device)\n",
        "\n",
        "    def compute_correction_ratio(self, current: torch.Tensor, proposal: torch.Tensor, theta: torch.Tensor,\n",
        "                                 strategy_name: str, strategy_params: Dict[str, Any]) -> float:\n",
        "        if strategy_name == \"LNS\":\n",
        "            # For LNS, the proposal mechanism is different and typically involves an oracle.\n",
        "            # A common simplification is to assume the correction ratio is 1.0,\n",
        "            # effectively treating the LNS proposal as symmetric for the correction term.\n",
        "            # This means acceptance relies primarily on the energy difference.\n",
        "            # A more rigorous treatment would require defining q_LNS(y'|y) and q_LNS(y|y')\n",
        "            # based on the LNS oracle's behavior.\n",
        "            return 1.0\n",
        "\n",
        "        q_proposal_given_current = self.output_space.get_proposal_prob(current, proposal, strategy_name, **strategy_params)\n",
        "        q_current_given_proposal = self.output_space.get_proposal_prob(proposal, current, strategy_name, **strategy_params)\n",
        "\n",
        "        if q_proposal_given_current == 0: # Cannot propose this move\n",
        "            return 0.0 \n",
        "        if q_current_given_proposal == 0: # Cannot reverse this move via proposal\n",
        "            # If q(y|y') is 0, the detailed balance implies acceptance should be 0 \n",
        "            # unless E(y') is drastically lower than E(y) and the exp term dominates.\n",
        "            # Setting correction to 0 ensures this.\n",
        "            return 0.0\n",
        "        \n",
        "        correction = q_current_given_proposal / q_proposal_given_current\n",
        "        return correction\n",
        "\n",
        "\n",
        "    def enhanced_acceptance_ratio(self, current: torch.Tensor, proposal: torch.Tensor, theta: torch.Tensor,\n",
        "                                temperature: float, strategy_name: str, strategy_params: Dict[str, Any]) -> float:\n",
        "        current_energy = torch.dot(theta, current.squeeze()) + self.phi_function(current.squeeze())\n",
        "        proposal_energy = torch.dot(theta, proposal.squeeze()) + self.phi_function(proposal.squeeze())\n",
        "        energy_diff = proposal_energy - current_energy\n",
        "        \n",
        "        correction_factor = self.compute_correction_ratio(current, proposal, theta, strategy_name, strategy_params)\n",
        "        self.correction_ratios_log.append(correction_factor)\n",
        "\n",
        "        if correction_factor < 0:\n",
        "            correction_factor = 0.0\n",
        "        if temperature <= 1e-9:\n",
        "            return float('inf') if energy_diff <= 0 and correction_factor > 1e-9 else 0.0\n",
        "        \n",
        "        exp_term = torch.exp(energy_diff / temperature)\n",
        "        acceptance_term_pk = float(correction_factor * exp_term)\n",
        "        return max(0.0, acceptance_term_pk)\n",
        "\n",
        "    def large_neighborhood_search_step(self, current_state: torch.Tensor, theta: torch.Tensor,\n",
        "                                     neighborhood_size: int = 5) -> Optional[torch.Tensor]:\n",
        "        if self.exact_oracle is None:\n",
        "            available_strategies = self.output_space.get_available_neighborhood_strategies(current_state)\n",
        "            if not available_strategies:\n",
        "                return current_state\n",
        "            chosen_strategy = random.choice(available_strategies)\n",
        "            s_params = {'radius': 1} if 'radius' in chosen_strategy else {'num_flips':1} if 'flip' in chosen_strategy else {}\n",
        "\n",
        "            neighbors = self.output_space.get_neighbors(current_state, chosen_strategy, **s_params)\n",
        "            return random.choice(neighbors) if neighbors else current_state\n",
        "\n",
        "        large_neighborhood: List[torch.Tensor] = []\n",
        "        strat_params = {'num_flips': neighborhood_size // 2, 'num_swaps': neighborhood_size // 2}\n",
        "        for strat_name in self.output_space.get_available_neighborhood_strategies(current_state):\n",
        "            large_neighborhood.extend(self.output_space.get_neighbors(current_state, strat_name, **strat_params))\n",
        "            if len(large_neighborhood) >= neighborhood_size:\n",
        "                break\n",
        "        \n",
        "        while len(large_neighborhood) < neighborhood_size:\n",
        "            random_s = self.output_space.random_state()\n",
        "            if not any(torch.allclose(random_s, existing) for existing in large_neighborhood):\n",
        "                large_neighborhood.append(random_s)\n",
        "        \n",
        "        large_neighborhood = large_neighborhood[:min(len(large_neighborhood), neighborhood_size * 2)]\n",
        "\n",
        "        best_solution = self.exact_oracle.solve(theta, large_neighborhood)\n",
        "        if self.exact_oracle.solver_state:\n",
        "             self.solver_diagnostics_log.append(self.exact_oracle.get_solver_state())\n",
        "        return best_solution if best_solution is not None else current_state\n",
        "\n",
        "    def sample_chain_corrected(self, theta: torch.Tensor, chain_id: int = 0,\n",
        "                               target_y: Optional[torch.Tensor] = None, #target_state was changed to target_y to avoid errors. \n",
        "                               use_large_neighborhood_step_flag: bool = False\n",
        "                               ) -> Tuple[List[torch.Tensor], Dict[str, float]]:\n",
        "        # theta is now expected to be a 1D tensor for the current chain/batch item.\n",
        "        if self.config.initialization_method == \"persistent\" and self.persistent_states is not None and \\\n",
        "           chain_id < len(self.persistent_states) and self.persistent_states[chain_id] is not None:\n",
        "            current_state = self.persistent_states[chain_id].clone().to(theta.device)\n",
        "        elif self.config.initialization_method == \"data_based\" and target_y is not None:\n",
        "            current_state = target_y.clone().to(theta.device) # target_state is also 1D here\n",
        "        else:\n",
        "            current_state = self.output_space.random_state().to(theta.device)\n",
        "\n",
        "        samples = []\n",
        "        acceptances = 0\n",
        "        total_steps_for_chain = self.config.chain_length + self.config.burn_in\n",
        "        \n",
        "        temperature = self.config.initial_temp # Initialize temperature for the loop\n",
        "\n",
        "        for step_idx in range(total_steps_for_chain):\n",
        "            temperature = self.temp_scheduler(step_idx)\n",
        "            proposal = None\n",
        "            chosen_strategy_name = \"unknown\"\n",
        "            strategy_params: Dict[str, Any] = {}\n",
        "\n",
        "            perform_lns_this_iteration = False\n",
        "            if use_large_neighborhood_step_flag and isinstance(self, LargeNeighborhoodSearchMCMC) and self.exact_oracle:\n",
        "                lns_freq = getattr(self, 'lns_frequency', 10) \n",
        "                if lns_freq > 0 and (step_idx + 1) % lns_freq == 0:\n",
        "                    perform_lns_this_iteration = True\n",
        "            \n",
        "            if perform_lns_this_iteration and isinstance(self, LargeNeighborhoodSearchMCMC):\n",
        "                lns_hood_size = getattr(self, 'lns_neighborhood_size', 5)\n",
        "                proposal = self.large_neighborhood_search_step(current_state, theta, lns_hood_size) # Pass 1D theta\n",
        "                chosen_strategy_name = \"LNS\"\n",
        "                strategy_params = {'lns_generated': True}\n",
        "            else:\n",
        "                available_strategies = self.output_space.get_available_neighborhood_strategies(current_state)\n",
        "                if not available_strategies:\n",
        "                    if step_idx >= self.config.burn_in:\n",
        "                        samples.append(current_state.clone())\n",
        "                    continue\n",
        "                chosen_strategy_name = random.choice(available_strategies)\n",
        "                \n",
        "                if \"radius\" in chosen_strategy_name:\n",
        "                    strategy_params['radius'] = self.config.neighborhood_radius\n",
        "                elif \"flip\" in chosen_strategy_name:\n",
        "                    strategy_params['num_flips'] = 1\n",
        "                elif \"swap\" in chosen_strategy_name:\n",
        "                    strategy_params['num_swaps'] = 1\n",
        "                else:\n",
        "                    strategy_params['radius'] = self.config.neighborhood_radius\n",
        "\n",
        "                neighbors = self.output_space.get_neighbors(current_state, chosen_strategy_name, **strategy_params)\n",
        "                if not neighbors:\n",
        "                    if step_idx >= self.config.burn_in:\n",
        "                        samples.append(current_state.clone())\n",
        "                    continue\n",
        "                proposal = random.choice(neighbors)\n",
        "            \n",
        "            if proposal is None:\n",
        "                if step_idx >= self.config.burn_in:\n",
        "                    samples.append(current_state.clone())\n",
        "                continue\n",
        "            \n",
        "            proposal = proposal.to(theta.device)\n",
        "\n",
        "            # MODIFIED LINE: Pass 1D theta directly\n",
        "            acceptance_term_pk = self.enhanced_acceptance_ratio(current_state, proposal, theta, temperature, chosen_strategy_name, strategy_params)\n",
        "            \n",
        "            if random.random() < min(1.0, acceptance_term_pk):\n",
        "                current_state = proposal\n",
        "                acceptances += 1\n",
        "            \n",
        "            if step_idx >= self.config.burn_in:\n",
        "                samples.append(current_state.clone())\n",
        "        \n",
        "        if self.persistent_states is None or len(self.persistent_states) != self.config.num_chains:\n",
        "             self.persistent_states = [None for _ in range(self.config.num_chains)] # Should match num_chains for indexing\n",
        "        if chain_id < len(self.persistent_states): # Ensure chain_id is a valid index\n",
        "            self.persistent_states[chain_id] = current_state.clone()\n",
        "        else:\n",
        "            # This case should ideally not be reached if chain_id is always < self.config.num_chains\n",
        "            warnings.warn(f\"chain_id {chain_id} is out of bounds for persistent_states (len {len(self.persistent_states)}). Skipping persistence update for this chain.\")\n",
        "\n",
        "        stats = {\n",
        "            'acceptance_rate': acceptances / total_steps_for_chain if total_steps_for_chain > 0 else 0.0,\n",
        "            'final_temperature': temperature,\n",
        "            'chain_length_collected': len(samples)\n",
        "        }\n",
        "        return samples, stats\n",
        "\n",
        "    def estimate_expectation_with_corrections(self, theta_batch: torch.Tensor, target_state_batch: Optional[torch.Tensor] = None,\n",
        "                                              use_large_neighborhood: bool = False\n",
        "                                              ) -> Tuple[torch.Tensor, Dict[str, Any]]:\n",
        "        # theta_batch is expected to be (batch_size, feature_dim)\n",
        "        # target_state_batch is expected to be (batch_size, feature_dim) or None\n",
        "        \n",
        "        if theta_batch.ndim == 1: # If a single theta is passed, unsqueeze to make it a batch of 1\n",
        "            theta_batch = theta_batch.unsqueeze(0)\n",
        "            if target_state_batch is not None and target_state_batch.ndim == 1:\n",
        "                target_state_batch = target_state_batch.unsqueeze(0)\n",
        "\n",
        "        batch_size = theta_batch.shape[0]\n",
        "        all_batch_expectations: List[torch.Tensor] = []\n",
        "        all_batch_stats_collector: List[Dict[str, Any]] = []\n",
        "\n",
        "        # Initialize persistent_states if needed. It's a list of length self.config.num_chains.\n",
        "        # Each call to sample_chain_corrected for a given chain_id will use/update the corresponding persistent state.\n",
        "        if self.persistent_states is None or len(self.persistent_states) != self.config.num_chains:\n",
        "            self.persistent_states = [None for _ in range(self.config.num_chains)]\n",
        "\n",
        "        for i in range(batch_size):\n",
        "            current_theta_item = theta_batch[i]  # This is 1D: (feature_dim,)\n",
        "            current_target_state_item = None\n",
        "            if target_state_batch is not None:\n",
        "                if target_state_batch.shape[0] == batch_size: # Ensure target_state_batch matches batch size\n",
        "                    current_target_state_item = target_state_batch[i] # Also 1D\n",
        "                elif batch_size == 1 and target_state_batch.ndim == 1: # Special case: single item batch, target might be 1D\n",
        "                     current_target_state_item = target_state_batch\n",
        "            # Else, if target_state_batch is not None but dimensions mismatch, current_target_state_item remains None\n",
        "            # or you could raise an error or warning. For now, it defaults to None if not perfectly aligned.\n",
        "\n",
        "            item_all_samples: List[torch.Tensor] = []\n",
        "            item_all_chain_stats: List[Dict[str, float]] = []\n",
        "\n",
        "            for chain_id in range(self.config.num_chains):\n",
        "                is_lns_sampler = isinstance(self, LargeNeighborhoodSearchMCMC)\n",
        "                # sample_chain_corrected now receives 1D theta (current_theta_item)\n",
        "                # and 1D target_state (current_target_state_item)\n",
        "                samples, chain_stats_for_chain = self.sample_chain_corrected(\n",
        "                    current_theta_item,\n",
        "                    chain_id,\n",
        "                    current_target_state_item,\n",
        "                    use_large_neighborhood_step_flag=(use_large_neighborhood and is_lns_sampler)\n",
        "                )\n",
        "                item_all_samples.extend(samples)\n",
        "                item_all_chain_stats.append(chain_stats_for_chain)\n",
        "\n",
        "            if not item_all_samples:\n",
        "                # Fallback for this specific batch item\n",
        "                # Ensure output_space and its dimension attribute are correctly defined\n",
        "                item_expectation = torch.zeros(self.output_space.dimension, device=current_theta_item.device, dtype=current_theta_item.dtype)\n",
        "                warnings.warn(f\"No MCMC samples collected for batch item {i}. Returning zeros for this item.\")\n",
        "                item_stats_dict = {'error': f'No samples collected for batch item {i}', 'num_samples': 0, 'avg_acceptance_rate': 0.0, 'sample_entropy': 0.0, 'chain_stats': []}\n",
        "            else:\n",
        "                item_expectation = torch.mean(torch.stack(item_all_samples).float(), dim=0)\n",
        "                avg_acceptance_item = np.mean([s['acceptance_rate'] for s in item_all_chain_stats if 'acceptance_rate' in s]) if item_all_chain_stats else 0.0\n",
        "                sample_entropy_item = compute_normalized_entropy(torch.stack(item_all_samples).detach().cpu()) if item_all_samples else 0.0\n",
        "                item_stats_dict = {\n",
        "                    'num_samples': len(item_all_samples),\n",
        "                    'avg_acceptance_rate': float(avg_acceptance_item),\n",
        "                    'sample_entropy': sample_entropy_item.tolist() if isinstance(sample_entropy_item, torch.Tensor) else float(sample_entropy_item),\n",
        "                    'chain_stats': item_all_chain_stats\n",
        "                }\n",
        "            \n",
        "            all_batch_expectations.append(item_expectation)\n",
        "            all_batch_stats_collector.append(item_stats_dict)\n",
        "\n",
        "        if not all_batch_expectations: # Handles batch_size = 0\n",
        "            fallback_dim = theta_batch.shape[1] if theta_batch.ndim == 2 and theta_batch.shape[0] == 0 else self.output_space.dimension\n",
        "            final_expectation = torch.empty(0, fallback_dim, device=theta_batch.device, dtype=theta_batch.dtype)\n",
        "            combined_summary_stats = {'error': 'No batch items processed or all failed', 'batch_item_stats': [], 'overall_avg_acceptance_rate': 0.0, 'total_samples_collected': 0}\n",
        "            warnings.warn(f\"No expectations computed for any batch item. Returning empty tensor.\")\n",
        "            return final_expectation, combined_summary_stats\n",
        "\n",
        "        final_expectation = torch.stack(all_batch_expectations) # Stack to get (B, D)\n",
        "\n",
        "        # Aggregate statistics\n",
        "        overall_avg_acceptance = 0.0\n",
        "        total_samples = 0\n",
        "        if all_batch_stats_collector:\n",
        "            rates = [s['avg_acceptance_rate'] for s in all_batch_stats_collector if s.get('num_samples', 0) > 0]\n",
        "            if rates:\n",
        "                overall_avg_acceptance = np.mean(rates)\n",
        "            total_samples = sum(s.get('num_samples', 0) for s in all_batch_stats_collector)\n",
        "    \n",
        "        combined_summary_stats = {\n",
        "            'batch_item_stats': all_batch_stats_collector, # Detailed stats per item\n",
        "            'overall_avg_acceptance_rate': float(overall_avg_acceptance),\n",
        "            'total_samples_collected': total_samples\n",
        "        }\n",
        "        return final_expectation, combined_summary_stats\n",
        "    \n",
        "    def forward(self, theta: torch.Tensor, target: torch.Tensor, use_large_neighborhood: bool = False\n",
        "               ) -> Tuple[torch.Tensor, Dict[str, Any]]:\n",
        "        expectation, stats = self.estimate_expectation_with_corrections(theta, target_y=target, use_large_neighborhood=use_large_neighborhood)\n",
        "        return expectation, stats\n",
        "\n",
        "\n",
        "class LargeNeighborhoodSearchMCMC(CorrectionRatioMCMC):\n",
        "    def __init__(self,\n",
        "                 output_space: DiscreteOutputSpace,\n",
        "                 config: MCMCConfig,\n",
        "                 phi_network: Optional[nn.Module] = None,\n",
        "                 lns_frequency: int = 10,\n",
        "                 lns_neighborhood_size: int = 20):\n",
        "        exact_oracle = ExactOptimizationOracle(output_space, phi_network)\n",
        "        super().__init__(output_space, config, phi_network, exact_oracle)\n",
        "        self.lns_frequency = lns_frequency\n",
        "        self.lns_neighborhood_size = lns_neighborhood_size\n",
        "\n",
        "\n",
        "class EnhancedCTMFenchelYoungIntegration(nn.Module):\n",
        "    def __init__(self,\n",
        "                 input_dim: int,\n",
        "                 output_space: DiscreteOutputSpace,\n",
        "                 mcmc_config: MCMCConfig,\n",
        "                 hidden_dim: int = 256,\n",
        "                 num_thought_steps: int = 5,\n",
        "                 use_large_neighborhood_search: bool = True,\n",
        "                 lns_frequency: int = 10,\n",
        "                 lns_neighborhood_size: int = 20):\n",
        "        super().__init__()\n",
        "        self.output_space_dim = output_space.dimension\n",
        "        \n",
        "        self.thought_network = nn.Sequential(\n",
        "            nn.Linear(input_dim, hidden_dim),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(hidden_dim, hidden_dim),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(hidden_dim, self.output_space_dim)\n",
        "        )\n",
        "        \n",
        "        self.phi_network = nn.Sequential(\n",
        "            nn.Linear(self.output_space_dim, hidden_dim // 2),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(hidden_dim // 2, 1)\n",
        "        )\n",
        "        \n",
        "        if use_large_neighborhood_search:\n",
        "            self.mcmc_sampler: Union[LargeNeighborhoodSearchMCMC, CorrectionRatioMCMC] = LargeNeighborhoodSearchMCMC(\n",
        "                output_space=output_space, config=mcmc_config, phi_network=self.phi_network,\n",
        "                lns_frequency=lns_frequency, lns_neighborhood_size=lns_neighborhood_size\n",
        "            )\n",
        "        else:\n",
        "            self.mcmc_sampler = CorrectionRatioMCMC(\n",
        "                output_space=output_space, config=mcmc_config, phi_network=self.phi_network\n",
        "            )\n",
        "        self.num_thought_steps = num_thought_steps\n",
        "\n",
        "    def forward(self, x: torch.Tensor, target_y: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor, Dict[str, Any]]:\n",
        "        theta = self.thought_network(x)\n",
        "        \n",
        "        expectation_y, mcmc_stats = self.mcmc_sampler.estimate_expectation_with_corrections(\n",
        "            theta, target_y,\n",
        "            use_large_neighborhood=isinstance(self.mcmc_sampler, LargeNeighborhoodSearchMCMC)\n",
        "        )\n",
        "        \n",
        "        # The Fenchel-Young loss is typically <theta, E[y]> - <theta, y_target>\n",
        "        # The gradient w.r.t. theta is simply E[y] - y_target\n",
        "        loss = torch.sum(theta * (expectation_y.detach() - target_y))\n",
        "        \n",
        "        return loss, expectation_y, mcmc_stats"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "070937a0",
      "metadata": {},
      "source": [
        "## 8. Instantiation and Configuration\n",
        "This section defines necessary global configuration variables and then instantiates the core components."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0937a070",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using MAX_GRID_SIZE: (30, 30)\n",
            "Using NUM_ARC_SYMBOLS: 10\n",
            "Using ARC_INPUT_FLAT_DIM: 900\n",
            "Using MCMC_CONFIG_ARC: chains=3, length=20\n"
          ]
        }
      ],
      "source": [
        "# --- Configuration Variables ---\n",
        "# These variables define the ARC environment and MCMC behavior.\n",
        "# NOTE: You must provide the paths to your ARC dataset directories.\n",
        "ARC_TRAIN_DIR = \"../data/training\" # <<< IMPORTANT: SET THIS PATH\n",
        "ARC_EVAL_DIR = \"../data/evaluation\"   # <<< IMPORTANT: SET THIS PATH\n",
        "\n",
        "MAX_GRID_SIZE = (30, 30)\n",
        "NUM_ARC_SYMBOLS = 10\n",
        "PADDING_VALUE = -1 # A value not in 0-9 to be ignored by the loss function\n",
        "MAX_DEMO_PAIRS = 5 # Max number of demonstration pairs to consider for context\n",
        "\n",
        "# Configuration for ARC-AGI-2 Training (shared constants)\n",
        "ARC_INPUT_FLAT_DIM = MAX_GRID_SIZE[0] * MAX_GRID_SIZE[1]\n",
        "\n",
        "# MCMC Configuration for ARC\n",
        "MCMC_OUTPUT_SPACE_DIM = ARC_INPUT_FLAT_DIM\n",
        "MCMC_CONFIG_ARC = MCMCConfig(\n",
        "    num_chains=3, \n",
        "    chain_length=20,\n",
        "    burn_in=5,\n",
        "    initial_temp=5.0,\n",
        "    final_temp=1.0,\n",
        "    temperature_schedule=\"geometric\",\n",
        "    decay_rate=0.95,\n",
        "    neighborhood_radius=1\n",
        ")\n",
        "ENABLE_CTM_MCMC_INTEGRATION_FOR_ARC = True\n",
        "\n",
        "print(f\"Using MAX_GRID_SIZE: {MAX_GRID_SIZE}\")\n",
        "print(f\"Using NUM_ARC_SYMBOLS: {NUM_ARC_SYMBOLS}\")\n",
        "print(f\"Using ARC_INPUT_FLAT_DIM: {ARC_INPUT_FLAT_DIM}\")\n",
        "print(f\"Using MCMC_CONFIG_ARC: chains={MCMC_CONFIG_ARC.num_chains}, length={MCMC_CONFIG_ARC.chain_length}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8467d801",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "EnhancedCTMFenchelYoungIntegration module initialized.\n",
            "  Output space dimension: 900\n",
            "  MCMC sampler type: LargeNeighborhoodSearchMCMC\n",
            "    LNS Frequency: 5\n",
            "    LNS Neighborhood Size: 10\n",
            "\n",
            "ENHANCED_MCMC_AVAILABLE set to: True\n"
          ]
        }
      ],
      "source": [
        "# --- Instantiation ---\n",
        "arc_grid_output_space = ARCGridOutputSpace(\n",
        "    dimension=ARC_INPUT_FLAT_DIM,\n",
        "    grid_shape=MAX_GRID_SIZE,\n",
        "    num_symbols=NUM_ARC_SYMBOLS\n",
        ")\n",
        "\n",
        "ctm_encoder_output_dim = ARC_INPUT_FLAT_DIM \n",
        "\n",
        "enhanced_ctm_mcmc = None\n",
        "if ENABLE_CTM_MCMC_INTEGRATION_FOR_ARC:\n",
        "    enhanced_ctm_mcmc = EnhancedCTMFenchelYoungIntegration(\n",
        "        input_dim=ctm_encoder_output_dim, \n",
        "        output_space=arc_grid_output_space,\n",
        "        mcmc_config=MCMC_CONFIG_ARC,\n",
        "        use_large_neighborhood_search=True,\n",
        "        lns_frequency=5,\n",
        "        lns_neighborhood_size=10\n",
        "    )\n",
        "\n",
        "    print(f\"\\nEnhancedCTMFenchelYoungIntegration module initialized.\")\n",
        "    print(f\"  Output space dimension: {enhanced_ctm_mcmc.output_space_dim}\")\n",
        "    if isinstance(enhanced_ctm_mcmc.mcmc_sampler, LargeNeighborhoodSearchMCMC):\n",
        "        print(f\"  MCMC sampler type: LargeNeighborhoodSearchMCMC\")\n",
        "        print(f\"    LNS Frequency: {enhanced_ctm_mcmc.mcmc_sampler.lns_frequency}\")\n",
        "        print(f\"    LNS Neighborhood Size: {enhanced_ctm_mcmc.mcmc_sampler.lns_neighborhood_size}\")\n",
        "    else:\n",
        "        print(\"  MCMC sampler type: CorrectionRatioMCMC\")\n",
        "    ENHANCED_MCMC_AVAILABLE = True\n",
        "    print(f\"\\nENHANCED_MCMC_AVAILABLE set to: {ENHANCED_MCMC_AVAILABLE}\")\n",
        "else:\n",
        "    print(\"\\nMCMC Integration is disabled for ARC.\")\n",
        "    ENHANCED_MCMC_AVAILABLE = False"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "073178b6",
      "metadata": {},
      "source": [
        "# --- ARC Dataset and Dataloader Logic --- #All Module Paths should now be defined since the modules that are not on path are automatically added to path. \n",
        "\n",
        "# Note: The function `pad_grid` is called but not defined in the original source.\n",
        "# It is required for the NewCustomARCGridDataset to function correctly.\n",
        "# You must provide its definition. A placeholder is provided below."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0b678317",
      "metadata": {},
      "outputs": [],
      "source": [
        "# --- Context: 2D Grid Padding (from original code) ---\n",
        "# This function handles padding at the 2D grid level, before serialization.\n",
        "def pad_grid(grid_list, max_dims, pad_value):\n",
        "    \"\"\"Pads a 2D grid to specified maximum dimensions.\"\"\"\n",
        "    grid_np = np.array(grid_list, dtype=np.int32)\n",
        "    padded_grid = np.full(max_dims, pad_value, dtype=np.int32)\n",
        "    h, w = grid_np.shape\n",
        "    padded_grid[:h, :w] = grid_np\n",
        "    return padded_grid\n",
        "\n",
        "# --- Fix: Byte Sequence Padding for the Model --- #\n",
        "# According to the model explanation, the key step is to pad the *serialized byte sequence*\n",
        "# to `config.max_sequence_length`. The function below implements this logic.\n",
        "\n",
        "# Define the model's expected input dimension from the configuration.\n",
        "MAX_SEQUENCE_LENGTH = 8192\n",
        "PADDING_BYTE_VALUE = 0\n",
        "\n",
        "def serialize_and_pad_grid(grid, max_len=MAX_SEQUENCE_LENGTH, pad_value=PADDING_BYTE_VALUE):\n",
        "    \"\"\"\n",
        "    Serializes a grid into a byte sequence and pads it to a fixed length.\n",
        "\n",
        "    This function implements the required padding logic for the LearnedBytePatcherEncoder.\n",
        "    It takes a grid, converts it to a flat byte sequence, and then pads or truncates\n",
        "    it to `max_sequence_length` (8192 bytes), ensuring a fixed-size input for the model.\n",
        "    \n",
        "    Args:\n",
        "        grid (list or np.ndarray): The input ARC grid.\n",
        "        max_len (int): The target length for the byte sequence, corresponding to\n",
        "                       `config.max_sequence_length`.\n",
        "        pad_value (int): The byte value to use for padding (0-255).\n",
        "\n",
        "    Returns:\n",
        "        bytes: The padded byte sequence of length `max_len`.\n",
        "    \"\"\"\n",
        "    # Convert the grid to a NumPy array of single bytes (uint8) and flatten it.\n",
        "    # ARC values (0-9) fit perfectly within a single byte.\n",
        "    flat_array = np.array(grid, dtype=np.uint8).flatten()\n",
        "\n",
        "    # Serialize the flattened array into a raw byte sequence.\n",
        "    byte_sequence = flat_array.tobytes()\n",
        "\n",
        "    # Calculate the number of padding bytes needed.\n",
        "    padding_len = max_len - len(byte_sequence)\n",
        "\n",
        "    if padding_len < 0:\n",
        "        # If the original sequence is too long, truncate it.\n",
        "        padded_sequence = byte_sequence[:max_len]\n",
        "    else:\n",
        "        # If the sequence is shorter, create padding and append it.\n",
        "        padding = bytes([pad_value] * padding_len)\n",
        "        padded_sequence = byte_sequence + padding\n",
        "        \n",
        "    return padded_sequence\n",
        "\n",
        "class NewCustomARCGridDataset(Dataset):\n",
        "    def __init__(self, data_dir, max_grid_size=MAX_GRID_SIZE, padding_value=PADDING_VALUE):\n",
        "        self.data_dir = data_dir\n",
        "        self.task_files = glob.glob(os.path.join(data_dir, \"*.json\"))\n",
        "        self.max_grid_size = max_grid_size\n",
        "        self.padding_value = padding_value\n",
        "        self.tasks = []\n",
        "        print(f\"NewCustomARCGridDataset: Looking for tasks in: {data_dir}\")\n",
        "        if not self.task_files:\n",
        "            print(f\"NewCustomARCGridDataset Warning: No JSON files found in {data_dir}. Dataset will be empty.\")\n",
        "        for task_file in self.task_files:\n",
        "            try:\n",
        "                with open(task_file, 'r') as f:\n",
        "                    self.tasks.append(json.load(f))\n",
        "            except Exception as e:\n",
        "                print(f\"NewCustomARCGridDataset Warning: Could not load or parse {task_file}: {e}\")\n",
        "        if not self.tasks:\n",
        "            print(f\"NewCustomARCGridDataset Warning: No tasks successfully loaded from {data_dir}.\")\n",
        "        else:\n",
        "            print(f\"NewCustomARCGridDataset: Loaded {len(self.tasks)} ARC tasks from {data_dir}.\")\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.tasks)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        task_data = self.tasks[idx]\n",
        "        processed_task = {'train': [], 'test': [], 'id': os.path.basename(self.task_files[idx]) if idx < len(self.task_files) else 'unknown_task'}\n",
        "\n",
        "        for pair_type in ['train', 'test']:\n",
        "            for item in task_data.get(pair_type, []):\n",
        "                input_grid_list = item.get('input', [])\n",
        "                output_grid_list = item.get('output', [])\n",
        "                \n",
        "                original_input_dims = (len(input_grid_list), len(input_grid_list[0]) if input_grid_list and input_grid_list[0] else (0,0))\n",
        "                original_output_dims = (len(output_grid_list), len(output_grid_list[0]) if output_grid_list and output_grid_list[0] else (0,0))\n",
        "\n",
        "                padded_input_np = pad_grid(input_grid_list, self.max_grid_size, self.padding_value)\n",
        "                padded_output_np = pad_grid(output_grid_list, self.max_grid_size, self.padding_value)\n",
        "                \n",
        "                processed_task[pair_type].append({\n",
        "                    'input': torch.from_numpy(padded_input_np).long(),\n",
        "                    'output': torch.from_numpy(padded_output_np).long(),\n",
        "                    'original_input_dims': original_input_dims,\n",
        "                    'original_output_dims': original_output_dims\n",
        "                })\n",
        "        return processed_task\n",
        "\n",
        "def collate_fn_new_custom_arc(batch_of_tasks):\n",
        "    input_byte_sequences_list = []\n",
        "    target_byte_sequences_for_diffusion_list = []\n",
        "    original_target_grids_for_ce_loss_list = []\n",
        "\n",
        "    for task in batch_of_tasks:\n",
        "        if not isinstance(task, dict):\n",
        "            continue\n",
        "\n",
        "        # Process 'train' pairs from the task\n",
        "        for train_pair in task.get('train', []):\n",
        "            if not isinstance(train_pair, dict) or 'input' not in train_pair or 'output' not in train_pair:\n",
        "                continue\n",
        "\n",
        "            # train_pair['input'] and train_pair['output'] are already padded 2D LongTensors from NewCustomARCGridDataset\n",
        "            input_grid_np = train_pair['input'].numpy() # Convert to numpy for serialize_and_pad_grid\n",
        "            target_grid_np = train_pair['output'].numpy()\n",
        "\n",
        "            # 1. Create input_byte_sequences (uint8)\n",
        "            input_bytes = serialize_and_pad_grid(input_grid_np, max_len=MAX_SEQUENCE_LENGTH, pad_value=PADDING_BYTE_VALUE)\n",
        "            input_byte_sequences_list.append(torch.tensor(list(input_bytes), dtype=torch.uint8))\n",
        "\n",
        "            # 2. Create target_byte_sequences_for_diffusion (uint8)\n",
        "            target_bytes_for_diffusion = serialize_and_pad_grid(target_grid_np, max_len=MAX_SEQUENCE_LENGTH, pad_value=PADDING_BYTE_VALUE)\n",
        "            target_byte_sequences_for_diffusion_list.append(torch.tensor(list(target_bytes_for_diffusion), dtype=torch.uint8))\n",
        "\n",
        "            # 3. Keep original_target_grids_for_ce_loss (long tensor, flattened)\n",
        "            original_target_grids_for_ce_loss_list.append(train_pair['output'].view(-1)) # Flattened LongTensor\n",
        "            \n",
        "    if not input_byte_sequences_list:\n",
        "        return {\n",
        "            'input_byte_sequences': torch.empty(0, MAX_SEQUENCE_LENGTH, dtype=torch.uint8),\n",
        "            'target_byte_sequences_for_diffusion': torch.empty(0, MAX_SEQUENCE_LENGTH, dtype=torch.uint8),\n",
        "            'original_target_grids_for_ce_loss': torch.empty(0, ARC_INPUT_FLAT_DIM, dtype=torch.long),\n",
        "        }\n",
        "\n",
        "    # Stack all collected tensors\n",
        "    final_input_byte_sequences = torch.stack(input_byte_sequences_list)\n",
        "    final_target_byte_sequences_for_diffusion = torch.stack(target_byte_sequences_for_diffusion_list)\n",
        "    final_original_target_grids_for_ce_loss = torch.stack(original_target_grids_for_ce_loss_list)\n",
        "    \n",
        "    return {\n",
        "        'input_byte_sequences': final_input_byte_sequences,\n",
        "        'target_byte_sequences_for_diffusion': final_target_byte_sequences_for_diffusion,\n",
        "        'original_target_grids_for_ce_loss': final_original_target_grids_for_ce_loss,\n",
        "    }\n",
        "\n",
        "# --- ARC Training Setup ---\n",
        "ARC_OUTPUT_HEAD_DIM = ARC_INPUT_FLAT_DIM * NUM_ARC_SYMBOLS\n",
        "ARC_TASK_ID = 3\n",
        "print(f\"ARC Output Head Dim: {ARC_OUTPUT_HEAD_DIM}\")\n",
        "\n",
        "ctm_model_arc, arc_output_head, optimizer_arc, ctm_mcmc_integration_arc, accelerator_arc = None, None, None, None, None\n",
        "\n",
        "print(\"\\n-----------------------------------------------------------------------------\")\n",
        "print(\"Initializing Configuration and Model for ARC with EnhancedCTMDiffusion\")\n",
        "print(\"-----------------------------------------------------------------------------\")\n",
        "\n",
        "'''\n",
        "You do not need to add any of the variables again from the ctm_Diffusion_NEWNEW.py file to your config_arc_diffusion in the Arc_AGI_2_Final.ipynb file. All the parameters you listed are already explicitly defined when config_arc_diffusion is created (between lines 2200 and 2378 approximately).\n",
        "\n",
        "The EnhancedCTMConfig class in ctm_Diffusion_NEWNEW.py provides default values for its fields. When you create an instance like config_arc_diffusion, any parameters you explicitly set will override these defaults. Since all the parameters in your list are already set in your notebook, those are the values that will be used for training.\n",
        "\n",
        "For example:\n",
        "\n",
        "attention_type is set to \"subquadratic\" on line 2260.\n",
        "positional_embedding_type is set to 'multi-learnable-fourier' on line 2276.\n",
        "enable_pipeline_parallelism is set to True on line 2288.\n",
        "And so on for all the other parameters you mentioned.\n",
        "If you wish to change any of these settings, you should modify their values directly in the existing config_arc_diffusion definition within your Arc_AGI_2_Final.ipynb file.\n",
        "'''\n",
        "\n",
        "# Define EnhancedCTMConfig for ARC with EnhancedCTMDiffusion\n",
        "# Assuming EnhancedCTMConfig is a defined class and MAX_SEQUENCE_LENGTH is a defined variable\n",
        "# For example:\n",
        "# from your_model_library import EnhancedCTMConfig\n",
        "# MAX_SEQUENCE_LENGTH = 8192\n",
        "\n",
        "# From contineous-thought-machines/models/constants.py\n",
        "VALID_NEURON_SELECT_TYPES = [\n",
        "    'first-last', 'random', 'random-pairing',  # Legacy\n",
        "    # Biologically-inspired types\n",
        "    'bio_hebbian', 'bio_plasticity', 'bio_competitive', 'bio_homeostatic',\n",
        "    'bio_evolutionary', 'bio_stdp', 'bio_criticality', 'bio_multi_objective',\n",
        "    # Hybrid approaches\n",
        "    'adaptive_random', 'performance_guided', 'task_aware'\n",
        "]\n",
        "\n",
        "VALID_POSITIONAL_EMBEDDING_TYPES = [\n",
        "    'learnable-fourier', 'multi-learnable-fourier',\n",
        "    'custom-rotational', 'custom-rotational-1d'\n",
        "]\n",
        "\n",
        "# From contineous-thought-machines/models/ctm_Diffusion_NEWNEW.py\n",
        "from dataclasses import dataclass, field\n",
        "from typing import Dict, Optional, Tuple, Union, Any, List\n",
        "\n",
        "@dataclass\n",
        "class EnhancedCTMConfig: # Renamed from ContinualLearningConfig for consistency in the target file\n",
        "    \"\"\"Enhanced configuration for continual learning CTM-diffusion model,\n",
        "    incorporating binary processing, multi-task learning, and advanced CTM features.\"\"\"\n",
        "    \n",
        "    # Model architecture (General Transformer/Diffusion settings)\n",
        "    d_model: int = 512  # Main model dimensionality\n",
        "    n_heads: int = 8\n",
        "    n_layers: int = 24\n",
        "    max_sequence_length: int = 8192 # Max input sequence length in terms of bytes or patches\n",
        "    dropout: float = 0.1\n",
        "    \n",
        "    # --- Byte Processing Options ---\n",
        "    patch_embedding_dim: int = 256         # <<< NEW: Output embedding dimension per patch from patcher\n",
        "    patch_encoder_cnn_channels: int = 64   # <<< NEW: Intermediate channels for CNN patch encoder\n",
        "\n",
        "    # --- Dynamic Entropy Patching Options (Inspired by BLT paper) ---\n",
        "    use_dynamic_entropy_patcher: bool = True # Flag to enable dynamic entropy-based patching\n",
        "    entropy_patcher_threshold_type: str = \"global\"  # 'global' or 'relative_monotonic'\n",
        "    entropy_patcher_global_threshold: float = 0.75 # Entropy threshold for 'global' type\n",
        "    entropy_patcher_relative_threshold: float = 0.1 # Entropy diff threshold for 'relative_monotonic'\n",
        "    entropy_patcher_min_patch_size: int = 4      # Minimum number of bytes in a dynamic patch\n",
        "    entropy_patcher_max_patch_size: int = 128    # Maximum number of bytes in a dynamic patch (for CNN encoder)\n",
        "    \n",
        "    # --- Learnable Entropy Model Parameters (for _EntropyProxyModel) ---\n",
        "    entropy_model_byte_vocab_size: int = 256\n",
        "    entropy_model_embedding_dim: int = 64\n",
        "    entropy_model_hidden_dim: int = 128\n",
        "    entropy_model_num_layers: int = 1\n",
        "    entropy_model_dropout: float = 0.1\n",
        "    entropy_model_loss_weight: float = 0.1 # Weight for its auxiliary loss contribution\n",
        "    # Note: These parameters are used if use_dynamic_entropy_patcher is True,\n",
        "    # as LearnedBytePatcherEncoder now instantiates the learnable _EntropyProxyModel.\n",
        "    \n",
        "    # Fallback if not using learned_patch_encoder or dynamic_entropy_patcher\n",
        "    byte_embedding_dim: int = 256\n",
        "    multi_granularity: bool = False # Default to False if patcher is preferred\n",
        "    # multi_granularity_output_dim is complex to predefine, MGP should expose its output dim.\n",
        "    # For now, if multi_granularity is True AND use_learned_patch_encoder is False, this would be used.\n",
        "    multi_granularity_output_dim: int = 256 # Placeholder if MGP is used.\n",
        "    \n",
        "    hierarchical_processing: bool = True # General flag, could apply to patcher or MGP\n",
        "    \n",
        "    # CTM Core Parameters (Specific to the OriginalCTMCore module)\n",
        "    # These are prefixed with 'ctm_' to distinguish from general model params\n",
        "    ctm_iterations: int = 5  # Original 'iterations'\n",
        "    ctm_d_model: int = 512   # Original 'd_model' for CTM's internal latent space\n",
        "    ctm_input_dim: int = 256 # Dimensionality of inputs to CTM (e.g., from byte embeddings or other features)\n",
        "                             # This was 'd_input' in OriginalCTMCore if it took external features.\n",
        "                             # If CTM processes outputs of byte_embedding, this might be byte_embedding_dim.\n",
        "    ctm_heads: int = 8       # Attention heads within CTM\n",
        "    ctm_n_synch_out: int = 64\n",
        "    ctm_n_synch_action: int = 64\n",
        "    ctm_synapse_depth: int = 3\n",
        "    ctm_memory_length: int = 10\n",
        "    ctm_deep_nlms: bool = True\n",
        "    ctm_memory_hidden_dims: int = 2048\n",
        "    ctm_do_layernorm_nlm: bool = False\n",
        "    ctm_out_dims: int = 512  # Output dimension of CTM's own projector\n",
        "    ctm_prediction_reshaper: list = field(default_factory=lambda: [-1])\n",
        "    ctm_dropout: float = 0.1\n",
        "    ctm_dropout_nlm: Optional[float] = None\n",
        "    # Neuron selection strategy. Available options:\n",
        "    # Legacy: 'first-last', 'random', 'random-pairing'\n",
        "    # Biologically-inspired: 'bio_hebbian', 'bio_plasticity', 'bio_competitive',\n",
        "    #                        'bio_homeostatic', 'bio_evolutionary', 'bio_stdp',\n",
        "    #                        'bio_criticality', 'bio_multi_objective'\n",
        "    # Hybrid: 'adaptive_random', 'performance_guided', 'task_aware'\n",
        "    ctm_neuron_select_type: str = 'bio_multi_objective'\n",
        "    ctm_n_random_pairing_self: int = 0\n",
        "    \n",
        "    # Diffusion Parameters\n",
        "    diffusion_steps: int = 1000\n",
        "    noise_schedule: str = \"cosine\" # e.g., \"linear\", \"cosine\"\n",
        "    diffusion_beta_start: float = 0.0001\n",
        "    diffusion_beta_end: float = 0.02\n",
        "    diffusion_timesteps: int = 1000 # Number of timesteps for the diffusion process\n",
        "    ctm_diffusion_coupling_strength: float = 0.8 # How CTM influences diffusion\n",
        "    adaptive_scheduling: bool = True  # CTM-adaptive diffusion timestep scheduling\n",
        "    iterative_refinement: bool = True # Iterative CTM-diffusion refinement for sampling\n",
        "    \n",
        "\n",
        "    \n",
        "    # Training Efficiency\n",
        "    mixed_precision: bool = True\n",
        "    gradient_checkpointing: bool = True\n",
        "    sparse_attention: bool = True  # Now implemented with BinarySparseAttention\n",
        "    adaptive_depth: bool = False   # Defaulting to False, can be enabled if implemented\n",
        "    \n",
        "    # Sparse Attention Parameters\n",
        "    sparse_attention_ratio: float = 0.1  # Keep only 10% of attention connections\n",
        "    binary_pattern_size: int = 8  # Size of binary patterns to detect\n",
        "\n",
        "    # Attention Mechanism Type\n",
        "    attention_type: str = \"subquadratic\"  # Options: \"standard\", \"binary_sparse\", \"subquadratic\"\n",
        "    \n",
        "    # Subquadratic Attention Parameters (if attention_type is \"subquadratic\")\n",
        "    subquadratic_attn_epsilon: float = 1e-6\n",
        "    subquadratic_attn_poly_degree: int = 5\n",
        "    attention_qkv_bias: bool = True # General QKV bias for attention mechanisms like Subquadratic or standard MHA\n",
        "    # attn_drop and proj_drop for subquadratic_attn will be mapped from ctm_dropout\n",
        "\n",
        "    # Positional Embedding Parameters\n",
        "    positional_embedding_type: Optional[str] = 'multi-learnable-fourier' # e.g., 'custom-rotational-1d', 'learnable-fourier', multi-learnable-fourier' #Can set the value here. \n",
        "    positional_embedding_dim: Optional[int] = None  # Dimension of the positional embedding, defaults to ctm_input_dim if None\n",
        "    reshape_patch_sequence_to_grid: bool = True # If True, reshape patch sequence to a 2D grid for 2D PEs. Must set to true if using 2D Grid for Positional Embeddings.\n",
        "    patch_grid_width: Optional[int] = None       # Desired width of the patch grid if reshaping\n",
        "\n",
        "    # Pipeline Parallelism Parameters\n",
        "    enable_pipeline_parallelism: bool = True\n",
        "    pipeline_stages: int = 4  # CTM, MCMC, Diffusion prep, Diffusion exec\n",
        "    pipeline_overlap_ratio: float = 0.7  # Target overlap ratio\n",
        "    \n",
        "    # Adaptive Batch Sizing Parameters\n",
        "    enable_adaptive_batching: bool = True\n",
        "    initial_batch_size: int = 32\n",
        "    min_batch_size: int = 8\n",
        "    max_batch_size: int = 256\n",
        "    batch_adaptation_frequency: int = 100\n",
        "    memory_threshold_high: float = 0.85\n",
        "    memory_threshold_low: float = 0.6\n",
        "    \n",
        "    # Smart Data Sampling Parameters\n",
        "    enable_smart_sampling: bool = True\n",
        "    sample_importance_weight: float = 0.6\n",
        "    sample_diversity_weight: float = 0.4\n",
        "    initial_sample_ratio: float = 0.3\n",
        "    complexity_analysis_enabled: bool = True\n",
        "    \n",
        "    # Multi-input/output parameters\n",
        "    num_inputs: int = 1  # Number of input streams\n",
        "    num_outputs: int = 1  # Number of output heads\n",
        "    output_dims: List[int] = field(default_factory=lambda: [64])  # Dimensions for each output head\n",
        "    \n",
        "    # Self-supervised learning\n",
        "    ssl_dim: int = 128  # Dimension for self-supervised projection\n",
        "    ssl_weight: float = 0.1  # Weight for self-supervised loss\n",
        "    ssl_temperature: float = 0.07  # Temperature for contrastive loss\n",
        "    ssl_noise_std: float = 0.1  # Noise standard deviation for contrastive augmentation\n",
        "    \n",
        "    # Spatiotemporal Processing\n",
        "    use_spatial: bool = True  # Enable spatial processing for image/video data\n",
        "    \n",
        "    # WINA Attention\n",
        "    use_wina_attention: bool = True  # Enable WINA sparse attention\n",
        "    \n",
        "    # Multi-task Learning Parameters\n",
        "    max_tasks: int = 50  # Maximum number of tasks for continual learning\n",
        "    # Added to resolve TypeError for unexpected keyword arguments\n",
        "    vocab_size: Optional[int] = None\n",
        "    output_audio_bytes: bool = False\n",
        "    inferred_task_latent_dim: Optional[int] = None # Default to None, __post_init__ handles it\n",
        "    use_hipa_attention: bool = False # Default to False\n",
        "    hipa_num_heads: Optional[int] = None # Default to None\n",
        "    audio_output_dtype_str: Optional[str] = \"float32\" # Default as per __post_init__ logic\n",
        "    unet_input_feature_dim: Optional[int] = None # Default to None, __post_init__ calculates it\n",
        "\n",
        "    # --- JEPA Training Parameters (Integrated with LearnedBytePatcherEncoder) ---\n",
        "    use_jepa_training: bool = False\n",
        "    # jepa_embed_dim will be derived from patch_embedding_dim if dynamic_entropy_patcher is used\n",
        "    jepa_predictor_hidden_dim: int = 512 # Hidden dimension of JEPA predictor MLP\n",
        "    jepa_mask_ratio_min: float = 0.15 # Min proportion of patch sequence to mask for target\n",
        "    jepa_mask_ratio_max: float = 0.75 # Max proportion of patch sequence to mask for target\n",
        "    jepa_context_scale_min: float = 0.3 # Min proportion of patches for context\n",
        "    jepa_context_scale_max: float = 0.7 # Max proportion of patches for context\n",
        "    jepa_momentum_beta: float = 0.996 # Momentum for target encoder update\n",
        "    jepa_loss_weight: float = 0.1 # Weight for the JEPA loss component\n",
        "    jepa_num_target_blocks: int = 1 # Number of target blocks to predict\n",
        "\n",
        "    # --- Knowledge Store Parameters ---\n",
        "\n",
        "    def __post_init__(self):\n",
        "        # Validate output dimensions\n",
        "        if len(self.output_dims) != self.num_outputs:\n",
        "            raise ValueError(f\"output_dims length ({len(self.output_dims)}) must match num_outputs ({self.num_outputs})\")\n",
        "\n",
        "        # Merged content from the second __post_init__\n",
        "        if hasattr(self, 'ctm_prediction_reshaper') and self.ctm_prediction_reshaper == [-1] and self.vocab_size is not None:\n",
        "            pass\n",
        "        if hasattr(self, 'ctm_dropout_nlm') and self.ctm_dropout_nlm is None and hasattr(self, 'ctm_dropout'):\n",
        "            self.ctm_dropout_nlm = self.ctm_dropout\n",
        "        if hasattr(self, 'mcmc_output_space_dim') and self.mcmc_output_space_dim is None and hasattr(self, 'ctm_out_dims'):\n",
        "            self.mcmc_output_space_dim = self.ctm_out_dims\n",
        "        \n",
        "        if hasattr(self, 'ctm_neuron_select_type') and \\\n",
        "           VALID_NEURON_SELECT_TYPES is not None and self.ctm_neuron_select_type not in VALID_NEURON_SELECT_TYPES:\n",
        "            print(f\"Warning: ctm_neuron_select_type '{self.ctm_neuron_select_type}' is not in VALID_NEURON_SELECT_TYPES ({VALID_NEURON_SELECT_TYPES}).\")\n",
        "\n",
        "        if hasattr(self, 'positional_embedding_type') and self.positional_embedding_type is not None:\n",
        "            if VALID_POSITIONAL_EMBEDDING_TYPES is None: # Fallback if import failed\n",
        "                print(f\"Warning: VALID_POSITIONAL_EMBEDDING_TYPES not available for validation.\")\n",
        "            elif self.positional_embedding_type not in VALID_POSITIONAL_EMBEDDING_TYPES:\n",
        "                print(f\"Warning: positional_embedding_type '{self.positional_embedding_type}' is not in VALID_POSITIONAL_EMBEDDING_TYPES ({VALID_POSITIONAL_EMBEDDING_TYPES}).\")\n",
        "            if self.positional_embedding_dim is not None and self.positional_embedding_dim <= 0:\n",
        "                raise ValueError(\"positional_embedding_dim must be positive if set.\")\n",
        "            \n",
        "            if self.reshape_patch_sequence_to_grid:\n",
        "                if self.patch_grid_width is None or self.patch_grid_width <= 0:\n",
        "                    raise ValueError(\"patch_grid_width must be a positive integer if reshape_patch_sequence_to_grid is True.\")\n",
        "                if self.positional_embedding_type not in ['learnable-fourier', 'multi-learnable-fourier', 'custom-rotational']:\n",
        "                    print(f\"Warning: reshape_patch_sequence_to_grid is True, but positional_embedding_type ('{self.positional_embedding_type}') is not a typical 2D PE. Ensure compatibility.\")\n",
        "\n",
        "        # Validations for new patch encoder\n",
        "        if self.use_dynamic_entropy_patcher:\n",
        "            if self.patch_embedding_dim <= 0:\n",
        "                raise ValueError(\"patch_embedding_dim must be positive if use_dynamic_entropy_patcher is True.\")\n",
        "            if self.entropy_patcher_min_patch_size <= 0:\n",
        "                raise ValueError(\"entropy_patcher_min_patch_size must be positive.\")\n",
        "            if self.entropy_patcher_max_patch_size < self.entropy_patcher_min_patch_size:\n",
        "                raise ValueError(\"entropy_patcher_max_patch_size must be >= entropy_patcher_min_patch_size.\")\n",
        "            if self.entropy_patcher_threshold_type not in [\"global\", \"relative_monotonic\"]:\n",
        "                raise ValueError(\"entropy_patcher_threshold_type must be 'global' or 'relative_monotonic'.\")\n",
        "        elif self.multi_granularity and self.multi_granularity_output_dim <= 0:\n",
        "            print(\"Warning: multi_granularity_output_dim might not be correctly set for validation if not using a patcher and MGP is active.\")\n",
        "        \n",
        "        if not hasattr(self, 'inferred_task_latent_dim') or self.inferred_task_latent_dim is None:\n",
        "            print(\"Warning: inferred_task_latent_dim not found or is None in config, defaulting to 64.\")\n",
        "            self.inferred_task_latent_dim = 512\n",
        "        elif self.inferred_task_latent_dim <= 0: # This check is now safe\n",
        "            raise ValueError(\"inferred_task_latent_dim must be positive.\")\n",
        " \n",
        "        if hasattr(self, 'use_hipa_attention') and self.use_hipa_attention and \\\n",
        "            (not hasattr(self, 'hipa_num_heads') or self.hipa_num_heads <= 0):\n",
        "             raise ValueError(\"hipa_num_heads must be positive if use_hipa_attention is True.\")\n",
        " \n",
        "        if hasattr(self, 'audio_output_dtype_str'):\n",
        "            if self.audio_output_dtype_str == \"float32\":\n",
        "                self.audio_output_item_size = 4\n",
        "            elif self.audio_output_dtype_str == \"int16\":\n",
        "                self.audio_output_item_size = 2\n",
        "            else:\n",
        "                if hasattr(self, 'output_audio_bytes') and self.output_audio_bytes:\n",
        "                    raise ValueError(f\"Unsupported audio_output_dtype_str: {self.audio_output_dtype_str} when output_audio_bytes is True.\")\n",
        "                else:\n",
        "                    self.audio_output_item_size = 4\n",
        "        elif hasattr(self, 'output_audio_bytes') and self.output_audio_bytes:\n",
        "            if not hasattr(self, 'audio_output_dtype_str') or self.audio_output_dtype_str is None:\n",
        "                raise ValueError(\"audio_output_dtype_str must be defined in config if output_audio_bytes is True.\")\n",
        "        else:\n",
        "            self.audio_output_item_size = 4\n",
        "\n",
        "        # Calculate unet_input_feature_dim if not set\n",
        "        if self.unet_input_feature_dim is None:\n",
        "            if self.max_sequence_length <= 0 or self.audio_output_item_size <= 0:\n",
        "                raise ValueError(\"max_sequence_length and audio_output_item_size must be positive to calculate unet_input_feature_dim.\")\n",
        "            self.unet_input_feature_dim = self.max_sequence_length // self.audio_output_item_size\n",
        "            if self.unet_input_feature_dim <= 0:\n",
        "                raise ValueError(f\"Calculated unet_input_feature_dim ({self.unet_input_feature_dim}) must be positive. Check max_sequence_length and audio_output_item_size.\")\n",
        "        elif self.unet_input_feature_dim <= 0:\n",
        "            raise ValueError(\"unet_input_feature_dim, if set, must be positive.\")\n",
        "\n",
        "        if self.use_jepa_training:\n",
        "            if not (0 < self.jepa_mask_ratio_min < 1 and 0 < self.jepa_mask_ratio_max < 1 and self.jepa_mask_ratio_min <= self.jepa_mask_ratio_max):\n",
        "                raise ValueError(\"JEPA mask ratios must be between 0 and 1, with min <= max.\")\n",
        "            if not (0 < self.jepa_context_scale_min < 1 and 0 < self.jepa_context_scale_max < 1 and self.jepa_context_scale_min <= self.jepa_context_scale_max):\n",
        "                raise ValueError(\"JEPA context scales must be between 0 and 1, with min <= max.\")\n",
        "            if not (0 <= self.jepa_momentum_beta < 1):\n",
        "                raise ValueError(\"jepa_momentum_beta must be between 0 and 1.\")\n",
        "            if self.jepa_num_target_blocks <= 0:\n",
        "                raise ValueError(\"jepa_num_target_blocks must be positive.\")\n",
        "            if not self.use_dynamic_entropy_patcher:\n",
        "                print(\"Warning: JEPA training is enabled but use_dynamic_entropy_patcher is False. JEPA relies on the patch embeddings from LearnedBytePatcherEncoder.\")\n",
        "\n",
        "# Define EnhancedCTMConfig for ARC with EnhancedCTMDiffusion\n",
        "config_arc_diffusion = EnhancedCTMConfig(\n",
        "    d_model=512,\n",
        "    #inferred_task_latent_dim=64, # This line remains commented out\n",
        "    n_heads=8,\n",
        "    n_layers=24, \n",
        "    max_sequence_length=MAX_SEQUENCE_LENGTH,\n",
        "    dropout=0.1,\n",
        "    use_dynamic_entropy_patcher=True,\n",
        "    patch_embedding_dim=256,\n",
        "    patch_grid_width=16,\n",
        "    patch_encoder_cnn_channels=64,\n",
        "    entropy_patcher_threshold_type=\"global\",\n",
        "    entropy_patcher_global_threshold=0.75,\n",
        "    entropy_patcher_relative_threshold=0.1,\n",
        "    entropy_patcher_min_patch_size=4,\n",
        "    entropy_patcher_max_patch_size=128,\n",
        "    # Parameters for the learnable entropy model within LearnedBytePatcherEncoder\n",
        "    entropy_model_byte_vocab_size=256,\n",
        "    entropy_model_embedding_dim=64,\n",
        "    entropy_model_hidden_dim=128,\n",
        "    entropy_model_num_layers=1,\n",
        "    entropy_model_dropout=0.1,\n",
        "    entropy_model_loss_weight=0.1,\n",
        "    \n",
        "    ctm_input_dim=256,\n",
        "    ctm_d_model=512,\n",
        "    ctm_iterations=5,\n",
        "    ctm_heads=8,\n",
        "    ctm_out_dims=512,\n",
        "    ctm_neuron_select_type='bio_multi_objective',\n",
        "    \n",
        "    # Attention Mechanism Type\n",
        "    attention_type=\"subquadratic\",  # Options: \"standard\", \"binary_sparse\", \"subquadratic\"\n",
        "    \n",
        "    # Subquadratic Attention Parameters\n",
        "    subquadratic_attn_epsilon=1e-6,\n",
        "    subquadratic_attn_poly_degree=5,\n",
        "    attention_qkv_bias=True, # Corrected capitalization\n",
        "    \n",
        "    # Positional Embedding Parameters\n",
        "    positional_embedding_type='multi-learnable-fourier',\n",
        "    positional_embedding_dim=None,\n",
        "    reshape_patch_sequence_to_grid=True,\n",
        "    #patch_grid_width=None, #Already defined in the byte patch section of this config. \n",
        "\n",
        "    # Pipeline Parallelism Parameters\n",
        "    enable_pipeline_parallelism=True,\n",
        "    pipeline_stages=4,\n",
        "    pipeline_overlap_ratio=0.7,\n",
        "    \n",
        "    # Adaptive Batch Sizing Parameters\n",
        "    enable_adaptive_batching=True,\n",
        "    initial_batch_size=32,\n",
        "    min_batch_size=8,\n",
        "    max_batch_size=256,\n",
        "    batch_adaptation_frequency=100,\n",
        "    memory_threshold_high=0.85,\n",
        "    memory_threshold_low=0.6,\n",
        "    \n",
        "    # Smart Data Sampling Parameters\n",
        "    enable_smart_sampling=True,\n",
        "    sample_importance_weight=0.6,\n",
        "    sample_diversity_weight=0.4,\n",
        "    initial_sample_ratio=0.3,\n",
        "    complexity_analysis_enabled=True,\n",
        "    \n",
        "    # Multi-input/output parameters\n",
        "    num_inputs=1,\n",
        "    num_outputs=1,\n",
        "    output_dims=[64],  # Directly pass the list value\n",
        "    \n",
        "    # Self-supervised learning\n",
        "    ssl_dim=128,\n",
        "    ssl_weight=0.1,\n",
        "    ssl_temperature=0.07,\n",
        "    ssl_noise_std=0.1,\n",
        "    \n",
        "    # Spatiotemporal Processing\n",
        "    use_spatial=True,\n",
        "    \n",
        "    # WINA Attention\n",
        "    use_wina_attention=True,\n",
        "    \n",
        "    # Multi-task Learning Parameters\n",
        "    max_tasks=50,\n",
        "    diffusion_steps=1000,\n",
        "    ctm_diffusion_coupling_strength=0.8,\n",
        "    vocab_size=None,\n",
        "    #enable_enhanced_mcmc=False, #ONLY USE THE ARC_AGI NOTEBOOK VERSION AND NOT THE ONE IMPORTED FROM THE DIFFUSION_NEWNEW file (This needs to be false). This flie cannot use this variable.\n",
        "    #mcmc_config=MCMC_CONFIG_ARC, #I don't think this is needed. \n",
        "    output_audio_bytes=False\n",
        ")\n",
        "\n",
        "print(\"âœ“ EnhancedCTMConfig for ARC (config_arc_diffusion) created.\")\n",
        "\n",
        "if 'enhanced_ctm_mcmc' not in globals():\n",
        "    print(\"Warning: 'enhanced_ctm_mcmc' not found in globals. Defaulting to None. Ensure the cell defining it (approx. lines 1820-1866) was run successfully.\")\n",
        "    enhanced_ctm_mcmc = None\n",
        "    \n",
        "if 'EnhancedCTMDiffusion' in globals() and EnhancedCTMDiffusion is not None:\n",
        "    ctm_model_arc = EnhancedCTMDiffusion(config=config_arc_diffusion).to(device)\n",
        "    print(\"âœ“ EnhancedCTMDiffusion model for ARC (ctm_model_arc) initialized.\")\n",
        "\n",
        "    # The external ARC output head will take features from the CTM core part of EnhancedCTMDiffusion\n",
        "    arc_output_head_input_dim = config_arc_diffusion.output_dims[0]\n",
        "    arc_output_head = nn.Linear(arc_output_head_input_dim, ARC_OUTPUT_HEAD_DIM).to(device)\n",
        "    print(f\"âœ“ ARC Output Head initialized (input_dim: {arc_output_head_input_dim}, output_dim: {ARC_OUTPUT_HEAD_DIM}).\")\n",
        "\n",
        "    # Handle external MCMC integration if enabled\n",
        "    if ENABLE_CTM_MCMC_INTEGRATION_FOR_ARC and enhanced_ctm_mcmc:\n",
        "        # Ensure the external MCMC module's input_dim matches the new CTM's output\n",
        "        if enhanced_ctm_mcmc.thought_network[0].in_features != config_arc_diffusion.output_dims[0]:\n",
        "            print(f\"Re-initializing external enhanced_ctm_mcmc for new input_dim {config_arc_diffusion.output_dims[0]}\")\n",
        "            enhanced_ctm_mcmc = EnhancedCTMFenchelYoungIntegration(\n",
        "                input_dim=config_arc_diffusion.output_dims[0], # Use output dim of CTM core\n",
        "                output_space=arc_grid_output_space,\n",
        "                mcmc_config=MCMC_CONFIG_ARC,\n",
        "                use_large_neighborhood_search=True,\n",
        "                lns_frequency=5,\n",
        "                lns_neighborhood_size=10\n",
        "            )\n",
        "        ctm_mcmc_integration_arc = enhanced_ctm_mcmc.to(device) if enhanced_ctm_mcmc else None\n",
        "        print(f\"âœ“ External MCMC Integration for ARC is {'enabled' if ctm_mcmc_integration_arc else 'FAILED to enable'}.\")\n",
        "    \n",
        "    arc_trainable_params = list(ctm_model_arc.parameters()) # EnhancedCTMDiffusion parameters\n",
        "    if arc_output_head: arc_trainable_params.extend(list(arc_output_head.parameters()))\n",
        "    if ctm_mcmc_integration_arc:\n",
        "        arc_trainable_params.extend(list(ctm_mcmc_integration_arc.parameters()))\n",
        "\n",
        "    optimizer_arc = optim.AdamW([p for p in arc_trainable_params if p.requires_grad], lr=LEARNING_RATE, weight_decay=1e-4)\n",
        "    \n",
        "    if ACCELERATE_AVAILABLE:\n",
        "        accelerator_arc = Accelerator()\n",
        "        models_to_prepare = [ctm_model_arc] # Start with the main model\n",
        "        if arc_output_head: models_to_prepare.append(arc_output_head)\n",
        "        if ctm_mcmc_integration_arc: models_to_prepare.append(ctm_mcmc_integration_arc)\n",
        "        \n",
        "        prepared_components = accelerator_arc.prepare(*models_to_prepare, optimizer_arc)\n",
        "        \n",
        "        optimizer_arc = prepared_components[-1] # Last element is the optimizer\n",
        "        prepared_models_tuple = prepared_components[:-1] # All other elements are models\n",
        "\n",
        "        ctm_model_arc = prepared_models_tuple[0]\n",
        "        model_idx = 1\n",
        "        if arc_output_head:\n",
        "            arc_output_head = prepared_models_tuple[model_idx]\n",
        "            model_idx +=1\n",
        "        if ctm_mcmc_integration_arc:\n",
        "            ctm_mcmc_integration_arc = prepared_models_tuple[model_idx]\n",
        "        print(\"âœ“ ARC models (EnhancedCTMDiffusion) and optimizer prepared with Accelerate.\")\n",
        "else:\n",
        "    print(\"âš ï¸ EnhancedCTMDiffusion model or its config for ARC-AGI-2 could not be initialized. Check imports.\")\n",
        "\n",
        "CHECKPOINT_DIR_ARC = os.path.join(CHECKPOINT_DIR, \"ctm_arc_agi_2_enhanced_diffusion\") # New checkpoint dir\n",
        "os.makedirs(CHECKPOINT_DIR_ARC, exist_ok=True)\n",
        "print(f\"ARC Checkpoints will be saved to: {CHECKPOINT_DIR_ARC}\")\n",
        "\n",
        "NUM_EPOCHS_ARC = 20\n",
        "ARC_BATCH_SIZE = 8\n",
        "\n",
        "arc_train_dataset = NewCustomARCGridDataset(ARC_TRAIN_DIR)\n",
        "arc_eval_dataset = NewCustomARCGridDataset(ARC_EVAL_DIR)\n",
        "\n",
        "arc_train_loader, arc_eval_loader = None, None\n",
        "if arc_train_dataset and len(arc_train_dataset) > 0:\n",
        "    arc_train_loader = DataLoader(\n",
        "        arc_train_dataset, batch_size=ARC_BATCH_SIZE, shuffle=True,\n",
        "        collate_fn=collate_fn_new_custom_arc, **OPTIMIZED_DATALOADER_CONFIG\n",
        "    )\n",
        "    if accelerator_arc: arc_train_loader = accelerator_arc.prepare(arc_train_loader)\n",
        "    print(f\"âœ“ ARC Training DataLoader initialized with {len(arc_train_dataset)} tasks.\")\n",
        "else:\n",
        "    print(\"âš ï¸ ARC Training DataLoader could not be initialized.\")\n",
        "\n",
        "if arc_eval_dataset and len(arc_eval_dataset) > 0:\n",
        "    arc_eval_loader = DataLoader(\n",
        "        arc_eval_dataset, batch_size=1, shuffle=False,\n",
        "        collate_fn=collate_fn_new_custom_arc, **OPTIMIZED_DATALOADER_CONFIG\n",
        "    )\n",
        "    if accelerator_arc: arc_eval_loader = accelerator_arc.prepare(arc_eval_loader)\n",
        "    print(f\"âœ“ ARC Evaluation DataLoader initialized with {len(arc_eval_dataset)} tasks.\")\n",
        "else:\n",
        "    print(\"âš ï¸ ARC Evaluation DataLoader could not be initialized.\")\n",
        "\n",
        "arc_criterion = nn.CrossEntropyLoss(ignore_index=PADDING_VALUE)\n",
        "print(\"\\nâœ“ ARC-AGI-2 Setup Complete.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d887b26b",
      "metadata": {},
      "outputs": [],
      "source": [
        "# --- ARC-AGI-2 Training Loop ---\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(f\"ðŸš€ STARTING PHASE 4: ARC-AGI-2 Training\")\n",
        "print(f\"   Epochs: {NUM_EPOCHS_ARC}, Batch Size: {ARC_BATCH_SIZE}, Task ID: {ARC_TASK_ID}\")\n",
        "print(f\"   Device: {device if not accelerator_arc else accelerator_arc.device}\")\n",
        "print(\"=\"*60 + \"\\n\")\n",
        "\n",
        "if not all([ctm_model_arc, arc_output_head, optimizer_arc, arc_train_loader, arc_criterion]):\n",
        "    print(\"âš ï¸ Skipping ARC-AGI-2 training due to missing components.\")\n",
        "else:\n",
        "    for epoch in range(NUM_EPOCHS_ARC):\n",
        "        ctm_model_arc.train()\n",
        "        arc_output_head.train()\n",
        "        if ctm_mcmc_integration_arc: ctm_mcmc_integration_arc.train()\n",
        "\n",
        "        total_arc_loss = 0\n",
        "        processed_batches = 0\n",
        "\n",
        "        for batch_idx, batch_data in enumerate(arc_train_loader):\n",
        "            if not batch_data or batch_data['input_byte_sequences'].numel() == 0:\n",
        "                print(f\"Skipping empty batch {batch_idx}\")\n",
        "                continue\n",
        "\n",
        "            # Get data from the updated collate_fn\n",
        "            input_bytes = batch_data['input_byte_sequences'].to(device if not accelerator_arc else accelerator_arc.device)\n",
        "            target_bytes_for_diffusion = batch_data['target_byte_sequences_for_diffusion'].to(device if not accelerator_arc else accelerator_arc.device)\n",
        "            original_target_grids_for_ce = batch_data['original_target_grids_for_ce_loss'].to(device if not accelerator_arc else accelerator_arc.device)\n",
        "\n",
        "            current_batch_size = input_bytes.size(0)\n",
        "\n",
        "            optimizer_arc.zero_grad()\n",
        "\n",
        "            with autocast(enabled=USE_MIXED_PRECISION, dtype=autocast_dtype) if not accelerator_arc else accelerator_arc.autocast():\n",
        "                # Forward pass through EnhancedCTMDiffusion\n",
        "                # The model internally handles patching, CTM core, diffusion (if target provided), and entropy aux loss.\n",
        "                model_output_dict = ctm_model_arc(\n",
        "                    byte_sequence=input_bytes,\n",
        "                    target_diffusion_output=target_bytes_for_diffusion, # Provide target for diffusion loss component\n",
        "                    mode='ctm_controlled_diffusion', # Ensure diffusion part is active for loss calculation\n",
        "                    timestep=torch.randint(0, config_arc_diffusion.diffusion_steps, (current_batch_size,), device=input_bytes.device).long(), # Random timesteps for diffusion training\n",
        "                    target_mcmc_output=None, # Internal MCMC is disabled in config_arc_diffusion\n",
        "                    task_name=\"ARC_AGI_2\", # Optional task name\n",
        "                    current_epoch=epoch # Pass current epoch\n",
        "                )\n",
        "\n",
        "                # Loss from EnhancedCTMDiffusion (includes entropy aux loss, diffusion loss, etc.)\n",
        "                enhanced_ctm_loss = model_output_dict.get('total_loss', torch.tensor(0.0, device=input_bytes.device))\n",
        "                loss = enhanced_ctm_loss\n",
        "\n",
        "                # Get CTM core output for the external ARC head\n",
        "                ctm_core_output_data = model_output_dict.get('ctm_core_data')\n",
        "                ctm_backbone_output = None\n",
        "                if ctm_core_output_data and 'final_sync_out' in ctm_core_output_data:\n",
        "                    ctm_backbone_output = ctm_core_output_data['final_sync_out']\n",
        "                elif ctm_core_output_data and 'ctm_latent_representation' in ctm_core_output_data: # Fallback key\n",
        "                    ctm_backbone_output = ctm_core_output_data['ctm_latent_representation']\n",
        "                else:\n",
        "                    print(\"Warning: CTM core output ('final_sync_out' or 'ctm_latent_representation') not found. Using zeros for ARC head input.\")\n",
        "                    ctm_backbone_output = torch.zeros(current_batch_size, config_arc_diffusion.ctm_out_dims, device=input_bytes.device)\n",
        "                \n",
        "                # External ARC Output Head for CrossEntropy loss on original grid prediction\n",
        "                if arc_output_head and ctm_backbone_output is not None:\n",
        "                    if ctm_backbone_output.ndim > 2 and ctm_backbone_output.shape[1] > 0:\n",
        "                         ctm_features_for_head = ctm_backbone_output.mean(dim=1)\n",
        "                    else:\n",
        "                         ctm_features_for_head = ctm_backbone_output\n",
        "                    \n",
        "                    predicted_logits = arc_output_head(ctm_features_for_head)\n",
        "                    predicted_logits_reshaped = predicted_logits.view(current_batch_size * ARC_INPUT_FLAT_DIM, NUM_ARC_SYMBOLS)\n",
        "                    target_grids_reshaped = original_target_grids_for_ce.view(current_batch_size * ARC_INPUT_FLAT_DIM)\n",
        "                    ce_loss = arc_criterion(predicted_logits_reshaped, target_grids_reshaped)\n",
        "                    loss += ce_loss # Add CE loss to the total loss\n",
        "\n",
        "                # External MCMC Integration (if enabled)\n",
        "                if ctm_mcmc_integration_arc and ctm_backbone_output is not None:\n",
        "                    target_grids_for_mcmc = (original_target_grids_for_ce > 0).float()\n",
        "                    mcmc_input_features = ctm_backbone_output.detach()\n",
        "                    if mcmc_input_features.ndim > 2 and mcmc_input_features.shape[1] > 0:\n",
        "                        mcmc_input_features = mcmc_input_features.mean(dim=1)\n",
        "\n",
        "                    mcmc_loss_val, _, _ = ctm_mcmc_integration_arc(\n",
        "                        x=mcmc_input_features,\n",
        "                        target_y=target_grids_for_mcmc \n",
        "                    )\n",
        "                    loss += mcmc_loss_val\n",
        "\n",
        "            if scaler: # Mixed precision (manual, without Accelerate)\n",
        "                scaler.scale(loss).backward()\n",
        "                if (batch_idx + 1) % GRADIENT_ACCUMULATION_STEPS == 0:\n",
        "                    scaler.unscale_(optimizer_arc)\n",
        "                    torch.nn.utils.clip_grad_norm_(ctm_model_arc.parameters(), MAX_GRAD_NORM)\n",
        "                    scaler.step(optimizer_arc)\n",
        "                    scaler.update()\n",
        "                    optimizer_arc.zero_grad()\n",
        "            elif accelerator_arc: # Using Hugging Face Accelerate\n",
        "                 accelerator_arc.backward(loss)\n",
        "                 if (batch_idx + 1) % GRADIENT_ACCUMULATION_STEPS == 0:\n",
        "                    optimizer_arc.step()\n",
        "                    optimizer_arc.zero_grad()\n",
        "            else: # Standard training\n",
        "                loss.backward()\n",
        "                if (batch_idx + 1) % GRADIENT_ACCUMULATION_STEPS == 0:\n",
        "                    torch.nn.utils.clip_grad_norm_(ctm_model_arc.parameters(), MAX_GRAD_NORM)\n",
        "                    optimizer_arc.step()\n",
        "                    optimizer_arc.zero_grad()\n",
        "            \n",
        "            total_arc_loss += loss.item()\n",
        "            processed_batches += 1\n",
        "\n",
        "            if (batch_idx + 1) % 50 == 0:\n",
        "                print(f\"  Epoch [{epoch+1}/{NUM_EPOCHS_ARC}], Batch [{batch_idx+1}/{len(arc_train_loader)}], Loss: {loss.item():.4f}\")\n",
        "        \n",
        "        avg_epoch_loss = total_arc_loss / processed_batches if processed_batches > 0 else 0\n",
        "        print(f\"Epoch [{epoch+1}/{NUM_EPOCHS_ARC}] completed. Average Loss: {avg_epoch_loss:.4f}\")\n",
        "\n",
        "        if CHECKPOINT_DIR_ARC:\n",
        "            model_to_save_ctm = accelerator_arc.unwrap_model(ctm_model_arc) if accelerator_arc else ctm_model_arc\n",
        "            model_to_save_head = accelerator_arc.unwrap_model(arc_output_head) if accelerator_arc else arc_output_head\n",
        "\n",
        "            # Save the models' state_dict and optimizer state using torch.save\n",
        "            torch.save(model_to_save_ctm.state_dict(), os.path.join(CHECKPOINT_DIR_ARC, f\"ctm_model_arc_epoch_{epoch+1}.pt\"))\n",
        "            torch.save(model_to_save_head.state_dict(), os.path.join(CHECKPOINT_DIR_ARC, f\"arc_output_head_epoch_{epoch+1}.pt\"))\n",
        "            torch.save(optimizer_arc.state_dict(), os.path.join(CHECKPOINT_DIR_ARC, f\"optimizer_arc_epoch_{epoch+1}.pt\"))\n",
        "            \n",
        "            print(f\"  âœ“ Checkpoint saved for epoch {epoch+1} to {CHECKPOINT_DIR_ARC}\")\n",
        "\n",
        "    print(\"\\nðŸŽ‰ ARC-AGI-2 Training Phase Completed!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "228a61ee",
      "metadata": {},
      "outputs": [],
      "source": [
        "# ## ARC-AGI-2 Evaluation\n",
        "\n",
        "import traceback\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(f\"ðŸ”¬ STARTING ARC-AGI-2 Evaluation\")\n",
        "print(\"=\"*60 + \"\\n\")\n",
        "\n",
        "if not all([ctm_model_arc is not None, arc_output_head is not None, arc_eval_loader is not None]):\n",
        "    print(\"âš ï¸ Skipping ARC-AGI-2 evaluation due to missing components.\")\n",
        "else:\n",
        "    latest_epoch = NUM_EPOCHS_ARC\n",
        "    ctm_checkpoint_path_eval = os.path.join(CHECKPOINT_DIR_ARC, f\"ctm_model_arc_epoch_{latest_epoch}.safetensors\")\n",
        "    head_checkpoint_path_eval = os.path.join(CHECKPOINT_DIR_ARC, f\"arc_output_head_epoch_{latest_epoch}.safetensors\")\n",
        "\n",
        "    try:\n",
        "        if os.path.exists(ctm_checkpoint_path_eval) and callable(load_model):\n",
        "            unwrapped_ctm_model = accelerator_arc.unwrap_model(ctm_model_arc) if accelerator_arc else ctm_model_arc\n",
        "            load_model(unwrapped_ctm_model, ctm_checkpoint_path_eval, device=device if not accelerator_arc else accelerator_arc.device)\n",
        "            \n",
        "            # FIX: Flatten LSTM parameters after loading to ensure contiguous memory, matching the saving process.\n",
        "            # This is good practice and can prevent errors in distributed/parallel environments.\n",
        "            if hasattr(unwrapped_ctm_model, 'dynamic_entropy_patcher') and \\\n",
        "               hasattr(unwrapped_ctm_model.dynamic_entropy_patcher, 'entropy_model') and \\\n",
        "               hasattr(unwrapped_ctm_model.dynamic_entropy_patcher.entropy_model, 'lstm') and \\\n",
        "               isinstance(unwrapped_ctm_model.dynamic_entropy_patcher.entropy_model.lstm, torch.nn.LSTM):\n",
        "                print(\"    > Flattening LSTM parameters for ctm_model_arc after loading...\")\n",
        "                unwrapped_ctm_model.dynamic_entropy_patcher.entropy_model.lstm.flatten_parameters()\n",
        "\n",
        "            print(f\"âœ“ Loaded CTM checkpoint from epoch {latest_epoch}.\")\n",
        "        else:\n",
        "            print(f\"âš ï¸ CTM Checkpoint not found. Evaluating with current model state.\")\n",
        "\n",
        "        if os.path.exists(head_checkpoint_path_eval) and callable(load_model):\n",
        "            unwrapped_head_model = accelerator_arc.unwrap_model(arc_output_head) if accelerator_arc else arc_output_head\n",
        "            load_model(unwrapped_head_model, head_checkpoint_path_eval, device=device if not accelerator_arc else accelerator_arc.device)\n",
        "            print(f\"âœ“ Loaded ARC Output Head checkpoint from epoch {latest_epoch}.\")\n",
        "        else:\n",
        "            print(f\"âš ï¸ ARC Output Head Checkpoint not found. Evaluating with current model state.\")\n",
        "        \n",
        "        ctm_model_arc.eval()\n",
        "        arc_output_head.eval()\n",
        "        if ctm_mcmc_integration_arc: ctm_mcmc_integration_arc.eval()\n",
        "\n",
        "        total_tasks = 0\n",
        "        solved_tasks = 0\n",
        "\n",
        "        with torch.inference_mode():\n",
        "            for task_idx, task_batch in enumerate(arc_eval_loader):\n",
        "                if not task_batch: continue\n",
        "                \n",
        "                current_task_data = task_batch # Dataloader batch_size=1, so task_batch is the task dict\n",
        "                \n",
        "                total_tasks += 1\n",
        "                task_solved_overall = True\n",
        "\n",
        "                if 'test' not in current_task_data or not current_task_data['test']:\n",
        "                    print(f\"Task {task_idx + 1} ({current_task_data.get('id', 'N/A')}): No test cases found. Skipping.\")\n",
        "                    task_solved_overall = False\n",
        "                    continue\n",
        "\n",
        "                for test_pair_idx, test_pair in enumerate(current_task_data['test']):\n",
        "                    # Input for evaluation is a single grid, needs to be converted to byte sequence\n",
        "                    input_grid_np_eval = test_pair['input'].numpy() # Get numpy array from tensor\n",
        "                    input_bytes_eval_single = serialize_and_pad_grid(input_grid_np_eval, max_len=MAX_SEQUENCE_LENGTH, pad_value=PADDING_BYTE_VALUE)\n",
        "                    input_bytes_eval = torch.tensor(list(input_bytes_eval_single), dtype=torch.uint8).unsqueeze(0).to(device if not accelerator_arc else accelerator_arc.device)\n",
        "\n",
        "                    target_grid_np = test_pair['output'].cpu().numpy()\n",
        "                    original_dims = test_pair['original_output_dims']\n",
        "\n",
        "                    test_input_solved = False\n",
        "                    for trial in range(3): # ARC rules allow 3 trials\n",
        "                        # Forward pass with EnhancedCTMDiffusion using CTM-controlled diffusion for generation\n",
        "                        # Assuming timestep 0 is appropriate for one-step or final-step generation\n",
        "                        current_batch_size_eval = input_bytes_eval.size(0) # Should be 1 for evaluation\n",
        "                        eval_timestep = torch.zeros(current_batch_size_eval, device=input_bytes_eval.device).long()\n",
        "\n",
        "                        eval_model_output_dict = ctm_model_arc(\n",
        "                            byte_sequence=input_bytes_eval,\n",
        "                            mode='ctm_controlled_diffusion', # Use CTM-controlled diffusion\n",
        "                            target_diffusion_output=None,   # No target during generation\n",
        "                            timestep=eval_timestep,\n",
        "                            task_name=\"ARC_AGI_2_EVAL_DIFFUSION\"\n",
        "                        )\n",
        "                        \n",
        "                        # ASSUMPTION: The generated output is a byte sequence under the key 'diffusion_output_pred'\n",
        "                        # The shape is expected to be (batch_size, MAX_SEQUENCE_LENGTH)\n",
        "                        predicted_byte_sequence = eval_model_output_dict.get('diffusion_output_pred') \n",
        "                        \n",
        "                        if predicted_byte_sequence is None:\n",
        "                            print(\"Warning: Key 'diffusion_output_pred' not found in model output. Trying 'generated_output'.\")\n",
        "                            predicted_byte_sequence = eval_model_output_dict.get('generated_output') # Common alternative\n",
        "                        \n",
        "                        if predicted_byte_sequence is None:\n",
        "                            print(\"Warning: Generated output key not found. Using zeros as prediction.\")\n",
        "                            # Fallback: create a zero tensor of the expected grid size if generation fails to be found\n",
        "                            preds_grid = np.zeros(MAX_GRID_SIZE, dtype=int)\n",
        "                        else:\n",
        "                            # Ensure the sequence has the correct batch dimension (should be 1)\n",
        "                            if predicted_byte_sequence.ndim == 1 and current_batch_size_eval == 1:\n",
        "                                predicted_byte_sequence = predicted_byte_sequence.unsqueeze(0)\n",
        "\n",
        "                            # Extract the part of the sequence corresponding to the flattened grid\n",
        "                            # ARC_INPUT_FLAT_DIM = MAX_GRID_SIZE[0] * MAX_GRID_SIZE[1]\n",
        "                            if predicted_byte_sequence.shape[1] >= ARC_INPUT_FLAT_DIM:\n",
        "                                preds_flat_bytes = predicted_byte_sequence[0, :ARC_INPUT_FLAT_DIM] # Get first item in batch, first ARC_INPUT_FLAT_DIM bytes\n",
        "                                # Convert byte values (0-9 for ARC symbols) to long tensor and reshape\n",
        "                                preds_grid = preds_flat_bytes.view(MAX_GRID_SIZE).long().cpu().numpy()\n",
        "                            else:\n",
        "                                print(f\"Warning: Generated byte sequence too short ({predicted_byte_sequence.shape[1]} vs {ARC_INPUT_FLAT_DIM}). Using zeros.\")\n",
        "                                preds_grid = np.zeros(MAX_GRID_SIZE, dtype=int)\n",
        "                        \n",
        "                        # Unpad to original dimensions\n",
        "                        h, w = original_dims\n",
        "                        final_pred = preds_grid[:h, :w]\n",
        "                        final_target = target_grid_np[:h, :w]\n",
        "\n",
        "                        if np.array_equal(final_pred, final_target):\n",
        "                            test_input_solved = True\n",
        "                            break\n",
        "\n",
        "                    if not test_input_solved:\n",
        "                        task_solved_overall = False\n",
        "                        break\n",
        "                \n",
        "                if task_solved_overall:\n",
        "                    solved_tasks += 1\n",
        "                    print(f\"  Task {task_idx + 1}/{len(arc_eval_loader)} ({current_task_data.get('id', 'N/A')}): SOLVED\")\n",
        "                else:\n",
        "                    print(f\"  Task {task_idx + 1}/{len(arc_eval_loader)} ({current_task_data.get('id', 'N/A')}): FAILED\")\n",
        "\n",
        "        if total_tasks > 0:\n",
        "            accuracy = (solved_tasks / total_tasks) * 100\n",
        "            summary = f\"ARC-AGI-2 Evaluation Summary:\\n  Total tasks evaluated: {total_tasks}\\n  Tasks solved: {solved_tasks}\\n  Accuracy: {accuracy:.2f}%\"\n",
        "            print(f\"\\n{summary}\")\n",
        "            with open('arc_agi_2_evaluation_summary.txt', 'w') as f:\n",
        "                f.write(summary)\n",
        "        else:\n",
        "            print(\"\\nARC-AGI-2 Evaluation: No tasks were evaluated.\")\n",
        "            \n",
        "    except Exception as e:\n",
        "        print(f\"âŒ Error during ARC-AGI-2 evaluation: {e}\")\n",
        "        traceback.print_exc()\n",
        "\n",
        "print(\"\\nðŸ”¬ ARC-AGI-2 Evaluation Phase Completed.\")\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.14"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
