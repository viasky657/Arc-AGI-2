{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0990b813",
   "metadata": {},
   "source": [
    "# -----------------------------------------------------------------------------\n",
    "# Dependency Installation Notes\n",
    "# -----------------------------------------------------------------------------\n",
    " The following dependencies are required. Please install them in your Python environment,\n",
    " for example, using pip:\n",
    "\n",
    " pip install mediapy\n",
    " pip install torch\n",
    " pip install safetensors\n",
    " pip install numpy\n",
    "\n",
    " For advanced optimizations, consider installing the following:\n",
    " pip install flash-attn --no-build-isolation\n",
    " pip install deepspeed\n",
    " pip install accelerate\n",
    " pip install xformers\n",
    "\n",
    " It's recommended to use a virtual environment.\n",
    " -----------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c018e3a1",
   "metadata": {},
   "source": [
    "This section contains the set-up components for training the ctm model with the byte-level encoder with binary patches, ctm processing with synpase system set to multi-objective, \n",
    "\n",
    "\n",
    "\n",
    "and binary patches from the ctm (after 20 rounds of COT thinking) refined and trained with MCMC to encourage the model to have reasoning steps closely related to the best answer, \n",
    "\n",
    "\n",
    "\n",
    "Each epoch is saved as a safetensor checkpoint to preserve training progress."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f95c4b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Save the original working directory Before LFS to avoid python corruption errors.\n",
    "original_dir = os.getcwd()\n",
    "\n",
    "# Add the Git LFS APT repository\n",
    "!curl -s https://packagecloud.io/install/repositories/github/git-lfs/script.deb.sh | bash #You need to be the root user or else this won't work. \n",
    "!apt install git-lfs -y \n",
    "!git lfs install\n",
    "!pip install pickleshare\n",
    "!git clone https://github.com/viasky657/Arc-AGI-2.git\n",
    "%cd Arc-AGI-2\n",
    "!git lfs pull #This is to download the actual model tensors and not the Github LFS pointer files.  #This is to download the actual model tensors and not the Github LFS pointer files. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f3c357f",
   "metadata": {},
   "outputs": [],
   "source": [
    "!git lfs fetch --all\n",
    "!git lfs checkout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56296414",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.chdir(\"/workspace/Arc-AGI-2\")  # Set your root working directory\n",
    "print(\"Current directory:\", os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3acd51d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "!ls -lh *.safetensors #Check for successful LFS pull for the safetensors and the .pt files. \n",
    "!git rev-parse --is-inside-work-tree\n",
    "!git lfs ls-files\n",
    "!head -n 10 contineous-thought-machines/examples/checkpoints/ctm_arc_agi_2_enhanced_diffusion/arc_output_head_epoch_20.safetensors #The below output should be binary \n",
    "#and metadata if it is a real tensor and \n",
    "#not a LFS pointer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3f07f68",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Change Back to Original directory to avoid errors. \n",
    "# Safely return to the original directory\n",
    "%cd $original_di #Probably don't need to use this since the code above automatically creates the correct directory. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "be570979",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting matplotlib\n",
      "  Downloading matplotlib-3.10.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (11 kB)\n",
      "Collecting contourpy>=1.0.1 (from matplotlib)\n",
      "  Downloading contourpy-1.3.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.5 kB)\n",
      "Collecting cycler>=0.10 (from matplotlib)\n",
      "  Downloading cycler-0.12.1-py3-none-any.whl.metadata (3.8 kB)\n",
      "Collecting fonttools>=4.22.0 (from matplotlib)\n",
      "  Downloading fonttools-4.58.4-cp310-cp310-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (106 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m106.6/106.6 kB\u001b[0m \u001b[31m5.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting kiwisolver>=1.3.1 (from matplotlib)\n",
      "  Downloading kiwisolver-1.4.8-cp310-cp310-manylinux_2_12_x86_64.manylinux2010_x86_64.whl.metadata (6.2 kB)\n",
      "Requirement already satisfied: numpy>=1.23 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (1.24.1)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (23.2)\n",
      "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (9.3.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /usr/lib/python3/dist-packages (from matplotlib) (2.4.7)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (2.8.2)\n",
      "Requirement already satisfied: six>=1.5 in /usr/lib/python3/dist-packages (from python-dateutil>=2.7->matplotlib) (1.16.0)\n",
      "Downloading matplotlib-3.10.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (8.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.6/8.6 MB\u001b[0m \u001b[31m23.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading contourpy-1.3.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (325 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m325.0/325.0 kB\u001b[0m \u001b[31m87.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading cycler-0.12.1-py3-none-any.whl (8.3 kB)\n",
      "Downloading fonttools-4.58.4-cp310-cp310-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (4.8 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.8/4.8 MB\u001b[0m \u001b[31m61.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading kiwisolver-1.4.8-cp310-cp310-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (1.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m77.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: kiwisolver, fonttools, cycler, contourpy, matplotlib\n",
      "Successfully installed contourpy-1.3.2 cycler-0.12.1 fonttools-4.58.4 kiwisolver-1.4.8 matplotlib-3.10.3\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.1.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython -m pip install --upgrade pip\u001b[0m\n",
      "Collecting mediapy\n",
      "  Downloading mediapy-1.2.4-py3-none-any.whl.metadata (4.8 kB)\n",
      "Requirement already satisfied: ipython in /usr/local/lib/python3.10/dist-packages (from mediapy) (8.17.2)\n",
      "Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (from mediapy) (3.10.3)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from mediapy) (1.24.1)\n",
      "Requirement already satisfied: Pillow in /usr/local/lib/python3.10/dist-packages (from mediapy) (9.3.0)\n",
      "Requirement already satisfied: decorator in /usr/local/lib/python3.10/dist-packages (from ipython->mediapy) (5.1.1)\n",
      "Requirement already satisfied: jedi>=0.16 in /usr/local/lib/python3.10/dist-packages (from ipython->mediapy) (0.19.1)\n",
      "Requirement already satisfied: matplotlib-inline in /usr/local/lib/python3.10/dist-packages (from ipython->mediapy) (0.1.6)\n",
      "Requirement already satisfied: prompt-toolkit!=3.0.37,<3.1.0,>=3.0.30 in /usr/local/lib/python3.10/dist-packages (from ipython->mediapy) (3.0.39)\n",
      "Requirement already satisfied: pygments>=2.4.0 in /usr/local/lib/python3.10/dist-packages (from ipython->mediapy) (2.16.1)\n",
      "Requirement already satisfied: stack-data in /usr/local/lib/python3.10/dist-packages (from ipython->mediapy) (0.6.3)\n",
      "Requirement already satisfied: traitlets>=5 in /usr/local/lib/python3.10/dist-packages (from ipython->mediapy) (5.13.0)\n",
      "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from ipython->mediapy) (1.1.3)\n",
      "Requirement already satisfied: pexpect>4.3 in /usr/local/lib/python3.10/dist-packages (from ipython->mediapy) (4.8.0)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->mediapy) (1.3.2)\n",
      "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib->mediapy) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->mediapy) (4.58.4)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->mediapy) (1.4.8)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->mediapy) (23.2)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /usr/lib/python3/dist-packages (from matplotlib->mediapy) (2.4.7)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib->mediapy) (2.8.2)\n",
      "Requirement already satisfied: parso<0.9.0,>=0.8.3 in /usr/local/lib/python3.10/dist-packages (from jedi>=0.16->ipython->mediapy) (0.8.3)\n",
      "Requirement already satisfied: ptyprocess>=0.5 in /usr/local/lib/python3.10/dist-packages (from pexpect>4.3->ipython->mediapy) (0.7.0)\n",
      "Requirement already satisfied: wcwidth in /usr/local/lib/python3.10/dist-packages (from prompt-toolkit!=3.0.37,<3.1.0,>=3.0.30->ipython->mediapy) (0.2.9)\n",
      "Requirement already satisfied: six>=1.5 in /usr/lib/python3/dist-packages (from python-dateutil>=2.7->matplotlib->mediapy) (1.16.0)\n",
      "Requirement already satisfied: executing>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from stack-data->ipython->mediapy) (2.0.1)\n",
      "Requirement already satisfied: asttokens>=2.1.0 in /usr/local/lib/python3.10/dist-packages (from stack-data->ipython->mediapy) (2.4.1)\n",
      "Requirement already satisfied: pure-eval in /usr/local/lib/python3.10/dist-packages (from stack-data->ipython->mediapy) (0.2.2)\n",
      "Downloading mediapy-1.2.4-py3-none-any.whl (26 kB)\n",
      "Installing collected packages: mediapy\n",
      "Successfully installed mediapy-1.2.4\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.1.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython -m pip install --upgrade pip\u001b[0m\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (1.24.1)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.1.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython -m pip install --upgrade pip\u001b[0m\n",
      "Collecting accelerate\n",
      "  Downloading accelerate-1.7.0-py3-none-any.whl.metadata (19 kB)\n",
      "Requirement already satisfied: numpy<3.0.0,>=1.17 in /usr/local/lib/python3.10/dist-packages (from accelerate) (1.24.1)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from accelerate) (23.2)\n",
      "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from accelerate) (5.9.6)\n",
      "Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from accelerate) (6.0.1)\n",
      "Requirement already satisfied: torch>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from accelerate) (2.1.0+cu118)\n",
      "Collecting huggingface-hub>=0.21.0 (from accelerate)\n",
      "  Downloading huggingface_hub-0.33.0-py3-none-any.whl.metadata (14 kB)\n",
      "Collecting safetensors>=0.4.3 (from accelerate)\n",
      "  Downloading safetensors-0.5.3-cp38-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.8 kB)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.21.0->accelerate) (3.9.0)\n",
      "Collecting fsspec>=2023.5.0 (from huggingface-hub>=0.21.0->accelerate)\n",
      "  Downloading fsspec-2025.5.1-py3-none-any.whl.metadata (11 kB)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.21.0->accelerate) (2.31.0)\n",
      "Collecting tqdm>=4.42.1 (from huggingface-hub>=0.21.0->accelerate)\n",
      "  Downloading tqdm-4.67.1-py3-none-any.whl.metadata (57 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m57.7/57.7 kB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.21.0->accelerate) (4.4.0)\n",
      "Collecting hf-xet<2.0.0,>=1.1.2 (from huggingface-hub>=0.21.0->accelerate)\n",
      "  Downloading hf_xet-1.1.4-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (879 bytes)\n",
      "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=2.0.0->accelerate) (1.12)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=2.0.0->accelerate) (3.0)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=2.0.0->accelerate) (3.1.2)\n",
      "Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch>=2.0.0->accelerate) (2.1.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=2.0.0->accelerate) (2.1.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.21.0->accelerate) (2.1.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.21.0->accelerate) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.21.0->accelerate) (1.26.13)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.21.0->accelerate) (2022.12.7)\n",
      "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=2.0.0->accelerate) (1.3.0)\n",
      "Downloading accelerate-1.7.0-py3-none-any.whl (362 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m362.1/362.1 kB\u001b[0m \u001b[31m18.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading huggingface_hub-0.33.0-py3-none-any.whl (514 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m514.8/514.8 kB\u001b[0m \u001b[31m56.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading safetensors-0.5.3-cp38-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (471 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m471.6/471.6 kB\u001b[0m \u001b[31m185.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading fsspec-2025.5.1-py3-none-any.whl (199 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m199.1/199.1 kB\u001b[0m \u001b[31m101.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading hf_xet-1.1.4-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.1/3.1 MB\u001b[0m \u001b[31m156.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading tqdm-4.67.1-py3-none-any.whl (78 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m78.5/78.5 kB\u001b[0m \u001b[31m46.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: tqdm, safetensors, hf-xet, fsspec, huggingface-hub, accelerate\n",
      "  Attempting uninstall: fsspec\n",
      "    Found existing installation: fsspec 2023.4.0\n",
      "    Uninstalling fsspec-2023.4.0:\n",
      "      Successfully uninstalled fsspec-2023.4.0\n",
      "Successfully installed accelerate-1.7.0 fsspec-2025.5.1 hf-xet-1.1.4 huggingface-hub-0.33.0 safetensors-0.5.3 tqdm-4.67.1\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.1.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython -m pip install --upgrade pip\u001b[0m\n",
      "Collecting diffusers\n",
      "  Downloading diffusers-0.33.1-py3-none-any.whl.metadata (19 kB)\n",
      "Requirement already satisfied: importlib-metadata in /usr/lib/python3/dist-packages (from diffusers) (4.6.4)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from diffusers) (3.9.0)\n",
      "Requirement already satisfied: huggingface-hub>=0.27.0 in /usr/local/lib/python3.10/dist-packages (from diffusers) (0.33.0)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from diffusers) (1.24.1)\n",
      "Collecting regex!=2019.12.17 (from diffusers)\n",
      "  Downloading regex-2024.11.6-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (40 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m40.5/40.5 kB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from diffusers) (2.31.0)\n",
      "Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from diffusers) (0.5.3)\n",
      "Requirement already satisfied: Pillow in /usr/local/lib/python3.10/dist-packages (from diffusers) (9.3.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.27.0->diffusers) (2025.5.1)\n",
      "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.27.0->diffusers) (23.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.27.0->diffusers) (6.0.1)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.27.0->diffusers) (4.67.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.27.0->diffusers) (4.4.0)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.27.0->diffusers) (1.1.4)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->diffusers) (2.1.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->diffusers) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->diffusers) (1.26.13)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->diffusers) (2022.12.7)\n",
      "Downloading diffusers-0.33.1-py3-none-any.whl (3.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.6/3.6 MB\u001b[0m \u001b[31m14.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading regex-2024.11.6-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (781 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m781.7/781.7 kB\u001b[0m \u001b[31m27.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: regex, diffusers\n",
      "Successfully installed diffusers-0.33.1 regex-2024.11.6\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.1.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython -m pip install --upgrade pip\u001b[0m\n",
      "Collecting xformers\n",
      "  Downloading xformers-0.0.30-cp310-cp310-manylinux_2_28_x86_64.whl.metadata (1.0 kB)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from xformers) (1.24.1)\n",
      "Collecting torch==2.7.0 (from xformers)\n",
      "  Downloading torch-2.7.0-cp310-cp310-manylinux_2_28_x86_64.whl.metadata (29 kB)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch==2.7.0->xformers) (3.9.0)\n",
      "Collecting typing-extensions>=4.10.0 (from torch==2.7.0->xformers)\n",
      "  Downloading typing_extensions-4.14.0-py3-none-any.whl.metadata (3.0 kB)\n",
      "Collecting sympy>=1.13.3 (from torch==2.7.0->xformers)\n",
      "  Downloading sympy-1.14.0-py3-none-any.whl.metadata (12 kB)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch==2.7.0->xformers) (3.0)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch==2.7.0->xformers) (3.1.2)\n",
      "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch==2.7.0->xformers) (2025.5.1)\n",
      "Collecting nvidia-cuda-nvrtc-cu12==12.6.77 (from torch==2.7.0->xformers)\n",
      "  Downloading nvidia_cuda_nvrtc_cu12-12.6.77-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cuda-runtime-cu12==12.6.77 (from torch==2.7.0->xformers)\n",
      "  Downloading nvidia_cuda_runtime_cu12-12.6.77-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cuda-cupti-cu12==12.6.80 (from torch==2.7.0->xformers)\n",
      "  Downloading nvidia_cuda_cupti_cu12-12.6.80-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-cudnn-cu12==9.5.1.17 (from torch==2.7.0->xformers)\n",
      "  Downloading nvidia_cudnn_cu12-9.5.1.17-py3-none-manylinux_2_28_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-cublas-cu12==12.6.4.1 (from torch==2.7.0->xformers)\n",
      "  Downloading nvidia_cublas_cu12-12.6.4.1-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cufft-cu12==11.3.0.4 (from torch==2.7.0->xformers)\n",
      "  Downloading nvidia_cufft_cu12-11.3.0.4-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-curand-cu12==10.3.7.77 (from torch==2.7.0->xformers)\n",
      "  Downloading nvidia_curand_cu12-10.3.7.77-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cusolver-cu12==11.7.1.2 (from torch==2.7.0->xformers)\n",
      "  Downloading nvidia_cusolver_cu12-11.7.1.2-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-cusparse-cu12==12.5.4.2 (from torch==2.7.0->xformers)\n",
      "  Downloading nvidia_cusparse_cu12-12.5.4.2-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-cusparselt-cu12==0.6.3 (from torch==2.7.0->xformers)\n",
      "  Downloading nvidia_cusparselt_cu12-0.6.3-py3-none-manylinux2014_x86_64.whl.metadata (6.8 kB)\n",
      "Collecting nvidia-nccl-cu12==2.26.2 (from torch==2.7.0->xformers)\n",
      "  Downloading nvidia_nccl_cu12-2.26.2-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (2.0 kB)\n",
      "Collecting nvidia-nvtx-cu12==12.6.77 (from torch==2.7.0->xformers)\n",
      "  Downloading nvidia_nvtx_cu12-12.6.77-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-nvjitlink-cu12==12.6.85 (from torch==2.7.0->xformers)\n",
      "  Downloading nvidia_nvjitlink_cu12-12.6.85-py3-none-manylinux2010_x86_64.manylinux_2_12_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cufile-cu12==1.11.1.6 (from torch==2.7.0->xformers)\n",
      "  Downloading nvidia_cufile_cu12-1.11.1.6-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting triton==3.3.0 (from torch==2.7.0->xformers)\n",
      "  Downloading triton-3.3.0-cp310-cp310-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (1.5 kB)\n",
      "Requirement already satisfied: setuptools>=40.8.0 in /usr/local/lib/python3.10/dist-packages (from triton==3.3.0->torch==2.7.0->xformers) (68.2.2)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy>=1.13.3->torch==2.7.0->xformers) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch==2.7.0->xformers) (2.1.2)\n",
      "Downloading xformers-0.0.30-cp310-cp310-manylinux_2_28_x86_64.whl (31.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m31.5/31.5 MB\u001b[0m \u001b[31m117.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading torch-2.7.0-cp310-cp310-manylinux_2_28_x86_64.whl (865.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m865.2/865.2 MB\u001b[0m \u001b[31m27.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cublas_cu12-12.6.4.1-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (393.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m393.1/393.1 MB\u001b[0m \u001b[31m51.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.6.80-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (8.9 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.9/8.9 MB\u001b[0m \u001b[31m207.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.6.77-py3-none-manylinux2014_x86_64.whl (23.7 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m23.7/23.7 MB\u001b[0m \u001b[31m228.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.6.77-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (897 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m897.7/897.7 kB\u001b[0m \u001b[31m252.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cudnn_cu12-9.5.1.17-py3-none-manylinux_2_28_x86_64.whl (571.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m571.0/571.0 MB\u001b[0m \u001b[31m37.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cufft_cu12-11.3.0.4-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (200.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m200.2/200.2 MB\u001b[0m \u001b[31m81.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cufile_cu12-1.11.1.6-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (1.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m242.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_curand_cu12-10.3.7.77-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (56.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m158.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cusolver_cu12-11.7.1.2-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (158.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m158.2/158.2 MB\u001b[0m \u001b[31m94.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cusparse_cu12-12.5.4.2-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (216.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m216.6/216.6 MB\u001b[0m \u001b[31m74.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cusparselt_cu12-0.6.3-py3-none-manylinux2014_x86_64.whl (156.8 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m156.8/156.8 MB\u001b[0m \u001b[31m96.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_nccl_cu12-2.26.2-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (201.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m201.3/201.3 MB\u001b[0m \u001b[31m83.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.6.85-py3-none-manylinux2010_x86_64.manylinux_2_12_x86_64.whl (19.7 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m19.7/19.7 MB\u001b[0m \u001b[31m205.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_nvtx_cu12-12.6.77-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (89 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m89.3/89.3 kB\u001b[0m \u001b[31m52.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading triton-3.3.0-cp310-cp310-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (156.4 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m156.4/156.4 MB\u001b[0m \u001b[31m105.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading sympy-1.14.0-py3-none-any.whl (6.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.3/6.3 MB\u001b[0m \u001b[31m275.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading typing_extensions-4.14.0-py3-none-any.whl (43 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.8/43.8 kB\u001b[0m \u001b[31m24.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: nvidia-cusparselt-cu12, typing-extensions, triton, sympy, nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufile-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cufft-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, torch, xformers\n",
      "  Attempting uninstall: typing-extensions\n",
      "    Found existing installation: typing_extensions 4.4.0\n",
      "    Uninstalling typing_extensions-4.4.0:\n",
      "      Successfully uninstalled typing_extensions-4.4.0\n",
      "  Attempting uninstall: triton\n",
      "    Found existing installation: triton 2.1.0\n",
      "    Uninstalling triton-2.1.0:\n",
      "      Successfully uninstalled triton-2.1.0\n",
      "  Attempting uninstall: sympy\n",
      "    Found existing installation: sympy 1.12\n",
      "    Uninstalling sympy-1.12:\n",
      "      Successfully uninstalled sympy-1.12\n",
      "  Attempting uninstall: torch\n",
      "    Found existing installation: torch 2.1.0+cu118\n",
      "    Uninstalling torch-2.1.0+cu118:\n",
      "      Successfully uninstalled torch-2.1.0+cu118\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "torchaudio 2.1.0+cu118 requires torch==2.1.0, but you have torch 2.7.0 which is incompatible.\n",
      "torchvision 0.16.0+cu118 requires torch==2.1.0, but you have torch 2.7.0 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed nvidia-cublas-cu12-12.6.4.1 nvidia-cuda-cupti-cu12-12.6.80 nvidia-cuda-nvrtc-cu12-12.6.77 nvidia-cuda-runtime-cu12-12.6.77 nvidia-cudnn-cu12-9.5.1.17 nvidia-cufft-cu12-11.3.0.4 nvidia-cufile-cu12-1.11.1.6 nvidia-curand-cu12-10.3.7.77 nvidia-cusolver-cu12-11.7.1.2 nvidia-cusparse-cu12-12.5.4.2 nvidia-cusparselt-cu12-0.6.3 nvidia-nccl-cu12-2.26.2 nvidia-nvjitlink-cu12-12.6.85 nvidia-nvtx-cu12-12.6.77 sympy-1.14.0 torch-2.7.0 triton-3.3.0 typing-extensions-4.14.0 xformers-0.0.30\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.1.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython -m pip install --upgrade pip\u001b[0m\n",
      "Collecting seaborn\n",
      "  Downloading seaborn-0.13.2-py3-none-any.whl.metadata (5.4 kB)\n",
      "Requirement already satisfied: numpy!=1.24.0,>=1.20 in /usr/local/lib/python3.10/dist-packages (from seaborn) (1.24.1)\n",
      "Collecting pandas>=1.2 (from seaborn)\n",
      "  Downloading pandas-2.3.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (91 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m91.2/91.2 kB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: matplotlib!=3.6.1,>=3.4 in /usr/local/lib/python3.10/dist-packages (from seaborn) (3.10.3)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (1.3.2)\n",
      "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (4.58.4)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (1.4.8)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (23.2)\n",
      "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.10/dist-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (9.3.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /usr/lib/python3/dist-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (2.4.7)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (2.8.2)\n",
      "Collecting pytz>=2020.1 (from pandas>=1.2->seaborn)\n",
      "  Downloading pytz-2025.2-py2.py3-none-any.whl.metadata (22 kB)\n",
      "Collecting tzdata>=2022.7 (from pandas>=1.2->seaborn)\n",
      "  Downloading tzdata-2025.2-py2.py3-none-any.whl.metadata (1.4 kB)\n",
      "Requirement already satisfied: six>=1.5 in /usr/lib/python3/dist-packages (from python-dateutil>=2.7->matplotlib!=3.6.1,>=3.4->seaborn) (1.16.0)\n",
      "Downloading seaborn-0.13.2-py3-none-any.whl (294 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m294.9/294.9 kB\u001b[0m \u001b[31m12.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading pandas-2.3.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (12.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.3/12.3 MB\u001b[0m \u001b[31m43.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m0:01\u001b[0m\n",
      "\u001b[?25hDownloading pytz-2025.2-py2.py3-none-any.whl (509 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m509.2/509.2 kB\u001b[0m \u001b[31m117.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading tzdata-2025.2-py2.py3-none-any.whl (347 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m347.8/347.8 kB\u001b[0m \u001b[31m114.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: pytz, tzdata, pandas, seaborn\n",
      "Successfully installed pandas-2.3.0 pytz-2025.2 seaborn-0.13.2 tzdata-2025.2\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.1.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython -m pip install --upgrade pip\u001b[0m\n",
      "Requirement already satisfied: safetensors in /usr/local/lib/python3.10/dist-packages (0.5.3)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.1.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython -m pip install --upgrade pip\u001b[0m\n",
      "Collecting deepspeed\n",
      "  Downloading deepspeed-0.17.1.tar.gz (1.5 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.5/1.5 MB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting einops (from deepspeed)\n",
      "  Downloading einops-0.8.1-py3-none-any.whl.metadata (13 kB)\n",
      "Collecting hjson (from deepspeed)\n",
      "  Downloading hjson-3.1.0-py3-none-any.whl.metadata (2.6 kB)\n",
      "Collecting msgpack (from deepspeed)\n",
      "  Downloading msgpack-1.1.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (8.4 kB)\n",
      "Collecting ninja (from deepspeed)\n",
      "  Downloading ninja-1.11.1.4-py3-none-manylinux_2_12_x86_64.manylinux2010_x86_64.whl.metadata (5.0 kB)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from deepspeed) (1.24.1)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from deepspeed) (23.2)\n",
      "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from deepspeed) (5.9.6)\n",
      "Collecting py-cpuinfo (from deepspeed)\n",
      "  Downloading py_cpuinfo-9.0.0-py3-none-any.whl.metadata (794 bytes)\n",
      "Collecting pydantic>=2.0.0 (from deepspeed)\n",
      "  Downloading pydantic-2.11.7-py3-none-any.whl.metadata (67 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m68.0/68.0 kB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from deepspeed) (2.7.0)\n",
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from deepspeed) (4.67.1)\n",
      "Collecting nvidia-ml-py (from deepspeed)\n",
      "  Downloading nvidia_ml_py-12.575.51-py3-none-any.whl.metadata (9.3 kB)\n",
      "Collecting annotated-types>=0.6.0 (from pydantic>=2.0.0->deepspeed)\n",
      "  Downloading annotated_types-0.7.0-py3-none-any.whl.metadata (15 kB)\n",
      "Collecting pydantic-core==2.33.2 (from pydantic>=2.0.0->deepspeed)\n",
      "  Downloading pydantic_core-2.33.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.8 kB)\n",
      "Requirement already satisfied: typing-extensions>=4.12.2 in /usr/local/lib/python3.10/dist-packages (from pydantic>=2.0.0->deepspeed) (4.14.0)\n",
      "Collecting typing-inspection>=0.4.0 (from pydantic>=2.0.0->deepspeed)\n",
      "  Downloading typing_inspection-0.4.1-py3-none-any.whl.metadata (2.6 kB)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch->deepspeed) (3.9.0)\n",
      "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.10/dist-packages (from torch->deepspeed) (1.14.0)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch->deepspeed) (3.0)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch->deepspeed) (3.1.2)\n",
      "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch->deepspeed) (2025.5.1)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.10/dist-packages (from torch->deepspeed) (12.6.77)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.10/dist-packages (from torch->deepspeed) (12.6.77)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.10/dist-packages (from torch->deepspeed) (12.6.80)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.5.1.17 in /usr/local/lib/python3.10/dist-packages (from torch->deepspeed) (9.5.1.17)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.10/dist-packages (from torch->deepspeed) (12.6.4.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.10/dist-packages (from torch->deepspeed) (11.3.0.4)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.10/dist-packages (from torch->deepspeed) (10.3.7.77)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.10/dist-packages (from torch->deepspeed) (11.7.1.2)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.10/dist-packages (from torch->deepspeed) (12.5.4.2)\n",
      "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.3 in /usr/local/lib/python3.10/dist-packages (from torch->deepspeed) (0.6.3)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.26.2 in /usr/local/lib/python3.10/dist-packages (from torch->deepspeed) (2.26.2)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.10/dist-packages (from torch->deepspeed) (12.6.77)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.10/dist-packages (from torch->deepspeed) (12.6.85)\n",
      "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.10/dist-packages (from torch->deepspeed) (1.11.1.6)\n",
      "Requirement already satisfied: triton==3.3.0 in /usr/local/lib/python3.10/dist-packages (from torch->deepspeed) (3.3.0)\n",
      "Requirement already satisfied: setuptools>=40.8.0 in /usr/local/lib/python3.10/dist-packages (from triton==3.3.0->torch->deepspeed) (68.2.2)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy>=1.13.3->torch->deepspeed) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch->deepspeed) (2.1.2)\n",
      "Downloading pydantic-2.11.7-py3-none-any.whl (444 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m444.8/444.8 kB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading pydantic_core-2.33.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m7.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading einops-0.8.1-py3-none-any.whl (64 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m64.4/64.4 kB\u001b[0m \u001b[31m11.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading hjson-3.1.0-py3-none-any.whl (54 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m54.0/54.0 kB\u001b[0m \u001b[31m12.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading msgpack-1.1.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (408 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m408.6/408.6 kB\u001b[0m \u001b[31m8.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading ninja-1.11.1.4-py3-none-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (422 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m422.8/422.8 kB\u001b[0m \u001b[31m8.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_ml_py-12.575.51-py3-none-any.whl (47 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m47.5/47.5 kB\u001b[0m \u001b[31m11.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading py_cpuinfo-9.0.0-py3-none-any.whl (22 kB)\n",
      "Downloading annotated_types-0.7.0-py3-none-any.whl (13 kB)\n",
      "Downloading typing_inspection-0.4.1-py3-none-any.whl (14 kB)\n",
      "Building wheels for collected packages: deepspeed\n",
      "  Building wheel for deepspeed (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for deepspeed: filename=deepspeed-0.17.1-py3-none-any.whl size=1690737 sha256=d7afc03e84b137c30d38e62d8d2bccc65a2186b1db4ffa5cc4cbb45526d09365\n",
      "  Stored in directory: /root/.cache/pip/wheels/4e/95/62/b46aa29e5aad38d5188313030fa9e911ddc0ef8e89642d7eb0\n",
      "Successfully built deepspeed\n",
      "Installing collected packages: py-cpuinfo, nvidia-ml-py, hjson, typing-inspection, pydantic-core, ninja, msgpack, einops, annotated-types, pydantic, deepspeed\n",
      "Successfully installed annotated-types-0.7.0 deepspeed-0.17.1 einops-0.8.1 hjson-3.1.0 msgpack-1.1.1 ninja-1.11.1.4 nvidia-ml-py-12.575.51 py-cpuinfo-9.0.0 pydantic-2.11.7 pydantic-core-2.33.2 typing-inspection-0.4.1\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.1.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython -m pip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "        !pip install matplotlib\n",
    "        !pip install mediapy\n",
    "        !pip install numpy\n",
    "        # For advanced optimizations, consider installing the following:\n",
    "        !pip install accelerate\n",
    "        !pip install diffusers\n",
    "        !pip install xformers\n",
    "        !pip install seaborn\n",
    "        !pip install safetensors\n",
    "        !pip install deepspeed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f940a53d-a8c0-49a4-9a32-fc571004ee4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hit:1 http://archive.ubuntu.com/ubuntu jammy InRelease\n",
      "Hit:2 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  InRelease\n",
      "Hit:3 http://archive.ubuntu.com/ubuntu jammy-updates InRelease                 \n",
      "Get:4 http://security.ubuntu.com/ubuntu jammy-security InRelease [129 kB]      \n",
      "Hit:5 http://archive.ubuntu.com/ubuntu jammy-backports InRelease               \n",
      "Hit:6 https://packagecloud.io/github/git-lfs/ubuntu jammy InRelease            \n",
      "Hit:7 https://ppa.launchpadcontent.net/deadsnakes/ppa/ubuntu jammy InRelease   \n",
      "Fetched 129 kB in 1s (239 kB/s)\n",
      "Reading package lists... Done\n",
      "Reading package lists... Done\n",
      "Building dependency tree... Done\n",
      "Reading state information... Done\n",
      "The following additional packages will be installed:\n",
      "  libaio1\n",
      "The following NEW packages will be installed:\n",
      "  libaio-dev libaio1\n",
      "0 upgraded, 2 newly installed, 0 to remove and 143 not upgraded.\n",
      "Need to get 28.4 kB of archives.\n",
      "After this operation, 110 kB of additional disk space will be used.\n",
      "Get:1 http://archive.ubuntu.com/ubuntu jammy/main amd64 libaio1 amd64 0.3.112-13build1 [7176 B]\n",
      "Get:2 http://archive.ubuntu.com/ubuntu jammy/main amd64 libaio-dev amd64 0.3.112-13build1 [21.2 kB]\n",
      "Fetched 28.4 kB in 0s (385 kB/s)     \n",
      "debconf: delaying package configuration, since apt-utils is not installed\n",
      "Selecting previously unselected package libaio1:amd64.\n",
      "(Reading database ... 20774 files and directories currently installed.)\n",
      "Preparing to unpack .../libaio1_0.3.112-13build1_amd64.deb ...\n",
      "Unpacking libaio1:amd64 (0.3.112-13build1) ...\n",
      "Selecting previously unselected package libaio-dev:amd64.\n",
      "Preparing to unpack .../libaio-dev_0.3.112-13build1_amd64.deb ...\n",
      "Unpacking libaio-dev:amd64 (0.3.112-13build1) ...\n",
      "Setting up libaio1:amd64 (0.3.112-13build1) ...\n",
      "Setting up libaio-dev:amd64 (0.3.112-13build1) ...\n",
      "Processing triggers for libc-bin (2.35-0ubuntu3.4) ...\n"
     ]
    }
   ],
   "source": [
    "!apt-get update && apt-get install -y libaio-dev\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "72c4316f-e85a-4be6-9641-3300230a135d",
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir -p /root/.triton/autotune\n",
    "#Triton (used by xFormers or FlashAttention) is trying to autotune kernels.\n",
    "\n",
    "#df command is likely being used internally to check disk usage of Triton cache.\n",
    "\n",
    "#Safe to ignore unless you rely on persistent Triton tuning across sessions.\n",
    "\n",
    "#🛠️ Fix (optional): Create the directory manually using the code above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0980997a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------------------------------------------------------\n",
      "Dependency Setup & Imports\n",
      "-----------------------------------------------------------------------------\n",
      "Please ensure all required dependencies are installed.\n",
      "Base dependencies: torchaudio, imageio, mediapy, torch, safetensors, numpy\n",
      "Optional optimization dependencies: flash-attn, deepspeed, accelerate, xformers\n",
      "See comments at the top of the script for installation commands.\n",
      "-----------------------------------------------------------------------------\n",
      "Base Checkpoint Directory: checkpoints\n",
      "Accelerate library found.\n",
      "xFormers library found.\n",
      "-----------------------------------------------------------------------------\n",
      "✅ Accelerate available\n",
      "✅ xFormers available - Expected 1.5-2x speedup\n",
      "\n",
      "🚀 OPTIMIZATION STATUS:\n",
      "  ⚡ torch.compile: ✅\n",
      "  📈 Accelerate: ✅\n",
      "  ⚡ xFormers: ✅\n",
      "\n",
      "-----------------------------------------------------------------------------\n",
      "Setting up module paths...\n",
      "-----------------------------------------------------------------------------\n",
      "Added to sys.path: /workspace/Arc-AGI-2/contineous-thought-machines/models\n",
      "Added to sys.path: /workspace/Arc-AGI-2/contineous-thought-machines\n",
      "Added to sys.path: /workspace/Arc-AGI-2/contineous-thought-machines/examples/contineous-thought-machines\n",
      "\n",
      "-----------------------------------------------------------------------------\n",
      "Importing CTM and Dataloader modules...\n",
      "-----------------------------------------------------------------------------\n",
      "✓ Successfully imported EnhancedCTMDiffusion with ALL GPU optimizations\n",
      "  - Integration Flow one-step generation\n",
      "  - Task-Aware HiPA frequency enhancement\n",
      "  - CTM-guided diffusion control\n",
      "  - GPU memory optimizations\n",
      "  - Mixed precision training support\n",
      "\n",
      "-----------------------------------------------------------------------------\n",
      "Initializing Configuration for Integrated Diffusion CTM\n",
      "-----------------------------------------------------------------------------\n",
      "Using device: cuda\n",
      "✅ Mixed precision training enabled (BF16) - Expected ~2x speedup\n"
     ]
    }
   ],
   "source": [
    "print(\"-----------------------------------------------------------------------------\")\n",
    "print(\"Dependency Setup & Imports\")\n",
    "print(\"-----------------------------------------------------------------------------\")\n",
    "print(\"Please ensure all required dependencies are installed.\")\n",
    "print(\"Base dependencies: torchaudio, imageio, mediapy, torch, safetensors, numpy\")\n",
    "print(\"Optional optimization dependencies: flash-attn, deepspeed, accelerate, xformers\")\n",
    "print(\"See comments at the top of the script for installation commands.\")\n",
    "print(\"-----------------------------------------------------------------------------\")\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import json\n",
    "import random\n",
    "import time\n",
    "from datetime import datetime\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import glob\n",
    "from IPython.display import display, Markdown\n",
    "\n",
    "# Define the base directory for saving checkpoints\n",
    "CHECKPOINT_DIR = \"checkpoints\"\n",
    "os.makedirs(CHECKPOINT_DIR, exist_ok=True)\n",
    "print(f\"Base Checkpoint Directory: {CHECKPOINT_DIR}\")\n",
    "\n",
    "# Attempt to import optional dependencies and set flags\n",
    "try:\n",
    "    from accelerate import Accelerator\n",
    "    from accelerate.utils import DistributedDataParallelKwargs\n",
    "    HAS_ACCELERATE = True\n",
    "    print(\"Accelerate library found.\")\n",
    "except ImportError:\n",
    "    HAS_ACCELERATE = False\n",
    "    print(\"Accelerate library not found. Some features like multi-GPU training might be limited.\")\n",
    "\n",
    "try:\n",
    "    import xformers.ops as xops\n",
    "    HAS_XFORMERS = True\n",
    "    print(\"xFormers library found.\")\n",
    "except ImportError:\n",
    "    HAS_XFORMERS = False\n",
    "    print(\"xFormers library not found.\")\n",
    "print(\"-----------------------------------------------------------------------------\")\n",
    "\n",
    "import numpy as np\n",
    "import math\n",
    "import time\n",
    "\n",
    "# OPTIMIZATION: Advanced optimization imports\n",
    "try:\n",
    "    from accelerate import Accelerator\n",
    "    ACCELERATE_AVAILABLE = True\n",
    "    print(\"✅ Accelerate available\")\n",
    "except ImportError:\n",
    "    ACCELERATE_AVAILABLE = False\n",
    "    print(\"⚠️ Accelerate not available\")\n",
    "\n",
    "try:\n",
    "    import xformers\n",
    "    import xformers.ops\n",
    "    XFORMERS_AVAILABLE = True\n",
    "    print(\"✅ xFormers available - Expected 1.5-2x speedup\")\n",
    "except ImportError:\n",
    "    XFORMERS_AVAILABLE = False\n",
    "    print(\"⚠️ xFormers not available\")\n",
    "\n",
    "# Try to import mediapy, fallback if not available\n",
    "try:\n",
    "    import mediapy\n",
    "    MEDIAPY_AVAILABLE = True\n",
    "except ImportError:\n",
    "    MEDIAPY_AVAILABLE = False\n",
    "    print(\"Warning: mediapy not available. GIF preview will be limited.\")\n",
    "\n",
    "    # Try to import mediapy, fallback if not available\n",
    "try:\n",
    "    import safetensors\n",
    "    SAFETENSORS_AVAILABLE = True\n",
    "except ImportError:\n",
    "    SAFETENSORS_AVAILABLE = False\n",
    "    print(\"Warning: safetensors not available. GIF preview will be limited.\")\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from torch.nn import functional as F\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from torch.utils.checkpoint import checkpoint  # OPTIMIZATION: Gradient checkpointing\n",
    "\n",
    "print(\"\\n🚀 OPTIMIZATION STATUS:\")\n",
    "print(f\"  ⚡ torch.compile: {'✅' if hasattr(torch, 'compile') else '❌'}\")\n",
    "print(f\"  📈 Accelerate: {'✅' if ACCELERATE_AVAILABLE else '❌'}\")\n",
    "print(f\"  ⚡ xFormers: {'✅' if XFORMERS_AVAILABLE else '❌'}\")\n",
    "\n",
    "# Add module paths\n",
    "# IMPORTANT: These paths assume the script is run from a directory where '..'\n",
    "# correctly points to the project root relative to 'models' and 'tasks' folders.\n",
    "# Adjust if your script is located elsewhere.\n",
    "print(\"\\n-----------------------------------------------------------------------------\")\n",
    "print(\"Setting up module paths...\")\n",
    "print(\"-----------------------------------------------------------------------------\")\n",
    "try:\n",
    "    current_script_path = os.path.dirname(os.path.abspath(__file__))\n",
    "except NameError: # __file__ is not defined in interactive shells, use os.getcwd()\n",
    "    current_script_path = os.getcwd()\n",
    "\n",
    "module_paths = [\n",
    "    os.path.abspath(os.path.join(current_script_path, '..', 'models')),\n",
    "    os.path.abspath(os.path.join(current_script_path, '..'))\n",
    "]\n",
    "module_paths.append(os.path.abspath('contineous-thought-machines'))\n",
    "for path in module_paths:\n",
    "    if path not in sys.path:\n",
    "        sys.path.append(path)\n",
    "        print(f\"Added to sys.path: {path}\")\n",
    "\n",
    "# Import Enhanced CTM with diffusion control and all optimizations.\n",
    "# OPTIMIZED_CTM_CONFIG_ARC will be defined below if imports are successful.\n",
    "print(\"\\n-----------------------------------------------------------------------------\")\n",
    "print(\"Importing CTM and Dataloader modules...\")\n",
    "print(\"-----------------------------------------------------------------------------\")\n",
    "EnhancedCTMDiffusion = None # Initialize to None\n",
    "ENHANCED_MCMC_AVAILABLE = False # Initialize\n",
    "\n",
    "try:\n",
    "    from models.ctm_Diffusion_NEWNEW import (\n",
    "        EnhancedCTMDiffusion,\n",
    "        #EnhancedCTMConfig, #This is turned off since it is included in the notebook set-up phase now to avoid undefined errors. \n",
    "        CTMControlledDiffusionProcessor,\n",
    "        FrequencyDomainAwareAttention,\n",
    "        IntegrationFlowHiPASampler,\n",
    "        CTMIntegrationFlowTrainer,\n",
    "    )\n",
    "    print(\"✓ Successfully imported EnhancedCTMDiffusion with ALL GPU optimizations\")\n",
    "    print(\"  - Integration Flow one-step generation\")\n",
    "    print(\"  - Task-Aware HiPA frequency enhancement\")\n",
    "    print(\"  - CTM-guided diffusion control\")\n",
    "    print(\"  - GPU memory optimizations\")\n",
    "    print(\"  - Mixed precision training support\")\n",
    "\n",
    "except ImportError as e_ctm:\n",
    "    print(f\"❌ Error importing Enhanced CTM or related components: {e_ctm}\")\n",
    "    print(\"   Please ensure 'models/ctm_Diffusion_NEWNEW_.py' components exist and are accessible.\")\n",
    "    EnhancedCTMDiffusion = None\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# Configuration for Integrated Diffusion CTM\n",
    "# -----------------------------------------------------------------------------\n",
    "print(\"\\n-----------------------------------------------------------------------------\")\n",
    "print(\"Initializing Configuration for Integrated Diffusion CTM\")\n",
    "print(\"-----------------------------------------------------------------------------\")\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# 🚀 OPTIMIZATION 1: Enhanced Mixed Precision Training Setup (FP16/BF16)\n",
    "USE_MIXED_PRECISION = torch.cuda.is_available() and hasattr(torch.cuda, 'amp') and device.type == 'cuda'\n",
    "USE_BFLOAT16 = USE_MIXED_PRECISION and torch.cuda.is_bf16_supported()\n",
    "\n",
    "autocast_dtype = torch.float32 # Default\n",
    "if USE_MIXED_PRECISION:\n",
    "    from torch.cuda.amp import GradScaler, autocast\n",
    "    if USE_BFLOAT16:\n",
    "        scaler = torch.amp.GradScaler(\"cuda\", enabled=True)\n",
    "        autocast_dtype = torch.bfloat16\n",
    "        print(\"✅ Mixed precision training enabled (BF16) - Expected ~2x speedup\")\n",
    "    else:\n",
    "        scaler = torch.amp.GradScaler(\"cuda\", enabled=True)\n",
    "        autocast_dtype = torch.float16\n",
    "        print(\"✅ Mixed precision training enabled (FP16) - Expected ~2x speedup\")\n",
    "else:\n",
    "    scaler = None\n",
    "    class dummy_autocast:\n",
    "        def __enter__(self): return None\n",
    "        def __exit__(self, exc_type, exc_val, exc_tb): return False\n",
    "    autocast = dummy_autocast\n",
    "    autocast_dtype = torch.float32\n",
    "    print(\"⚠️ Mixed precision training not available (CPU or older GPU or torch.cuda.amp not found)\")\n",
    "\n",
    "# 🚀 OPTIMIZATION 2: Gradient Accumulation Configuration\n",
    "GRADIENT_ACCUMULATION_STEPS = 4\n",
    "MAX_GRAD_NORM = 1.0\n",
    "\n",
    "# 🚀 OPTIMIZATION 4: Data Loading Optimizations\n",
    "OPTIMIZED_DATALOADER_CONFIG = {\n",
    "    'num_workers': min(8, os.cpu_count() if os.cpu_count() else 1),\n",
    "    'pin_memory': torch.cuda.is_available(),\n",
    "    'persistent_workers': True if min(8, os.cpu_count() if os.cpu_count() else 1) > 0 else False,\n",
    "    'prefetch_factor': 4 if min(8, os.cpu_count() if os.cpu_count() else 1) > 0 else None,\n",
    "}\n",
    "\n",
    "# General Training Parameters (can be overridden by specific phases)\n",
    "LEARNING_RATE = 1e-4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3897a7e9",
   "metadata": {},
   "source": [
    "# Injected MCMC Components and EnhancedCTMFenchelYoungIntegration Initialization\n",
    "\n",
    "This notebook contains the Python code for MCMC components, including ARC-specific output spaces and an enhanced Fenchel-Young integration layer, structured for use in a Jupyter environment."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1609087c",
   "metadata": {},
   "source": [
    "## 1. Imports\n",
    "All necessary libraries and modules are imported here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "16743809",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import random\n",
    "from typing import Optional, Callable, Tuple, Dict, Any, List, Union\n",
    "from dataclasses import dataclass, field\n",
    "import math\n",
    "import numpy as np\n",
    "import warnings # For ARCGridOutputSpace warnings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02621907",
   "metadata": {},
   "source": [
    "## 2. Utility Functions (from `models.utils`)\n",
    "Helper functions used across various modules."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "96281960",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_coord_dim(x, scaled=True):\n",
    "    \"\"\"\n",
    "    Adds a final dimension to the tensor representing 2D coordinates.\n",
    "    \"\"\"\n",
    "    B, H, W = x.shape\n",
    "    x_coords = torch.arange(W, device=x.device, dtype=x.dtype).repeat(H, 1)\n",
    "    y_coords = torch.arange(H, device=x.device, dtype=x.dtype).unsqueeze(-1).repeat(1, W)\n",
    "    if scaled:\n",
    "        x_coords = x_coords / (W - 1) if W > 1 else torch.zeros_like(x_coords)\n",
    "        y_coords = y_coords / (H - 1) if H > 1 else torch.zeros_like(y_coords)\n",
    "    coords = torch.stack((x_coords, y_coords), dim=-1)\n",
    "    coords = coords.unsqueeze(0) \n",
    "    coords = coords.repeat(B, 1, 1, 1) \n",
    "    return coords\n",
    "\n",
    "def compute_normalized_entropy(logits, reduction='mean'):\n",
    "    \"\"\"\n",
    "    Calculates the normalized entropy of a PyTorch tensor of logits along the \n",
    "    final dimension.\n",
    "    \"\"\"\n",
    "    preds = F.softmax(logits, dim=-1)\n",
    "    log_preds = torch.log_softmax(logits, dim=-1)\n",
    "    entropy = -torch.sum(preds * log_preds, dim=-1)\n",
    "    num_classes = preds.shape[-1]\n",
    "    if num_classes <= 1: # Avoid log(1)=0 or log(0)\n",
    "        return torch.zeros_like(entropy)\n",
    "    max_entropy = torch.log(torch.tensor(num_classes, dtype=torch.float32, device=logits.device))\n",
    "    if max_entropy == 0: # Should only happen if num_classes is 1\n",
    "        return torch.zeros_like(entropy)\n",
    "    normalized_entropy = entropy / max_entropy\n",
    "    if len(logits.shape) > 2 and reduction == 'mean':\n",
    "        normalized_entropy = normalized_entropy.flatten(1).mean(-1)\n",
    "    return normalized_entropy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8613871c",
   "metadata": {},
   "source": [
    "## 3. Core Modules (from `models.modules`)\n",
    "Custom neural network layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "87193681",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SuperLinear(nn.Module):\n",
    "    def __init__(self,\n",
    "                 in_dims,\n",
    "                 out_dims,\n",
    "                 N,\n",
    "                 T=1.0,\n",
    "                 do_norm=False,\n",
    "                 dropout=0):\n",
    "        super().__init__()\n",
    "        self.dropout = nn.Dropout(dropout) if dropout > 0 else nn.Identity()\n",
    "        self.in_dims = in_dims\n",
    "        self.layernorm = nn.LayerNorm(in_dims, elementwise_affine=True) if do_norm else nn.Identity()\n",
    "        self.do_norm = do_norm\n",
    "        self.register_parameter('w1', nn.Parameter(\n",
    "            torch.empty((in_dims, out_dims, N)).uniform_(\n",
    "                -1/math.sqrt(in_dims + out_dims),\n",
    "                 1/math.sqrt(in_dims + out_dims)\n",
    "            ), requires_grad=True)\n",
    "        )\n",
    "        self.register_parameter('b1', nn.Parameter(torch.zeros((1, N, out_dims)), requires_grad=True))\n",
    "        self.register_parameter('T', nn.Parameter(torch.Tensor([T]))) \n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.dropout(x)\n",
    "        out = self.layernorm(out)\n",
    "        out = torch.einsum('BDM,MHD->BDH', out, self.w1) + self.b1\n",
    "        out = out.squeeze(-1) / self.T\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09781309",
   "metadata": {},
   "source": [
    "## 4. MCMC Interpretability Solver Components (from `models.mcmc_interpretability_solver`)\n",
    "Dataclasses and hooks for tracking and interpreting MCMC processes and solver states."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "09183097",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class ThoughtStep:\n",
    "    step_id: int\n",
    "    layer_name: str\n",
    "    input_state: Optional[torch.Tensor] = None\n",
    "    output_state: Optional[torch.Tensor] = None\n",
    "    attention_weights: Optional[torch.Tensor] = None\n",
    "    mcmc_samples: Optional[torch.Tensor] = None\n",
    "    confidence_score: float = 0.0\n",
    "    reasoning_vector: Optional[torch.Tensor] = None\n",
    "    energy_landscape: Dict[str, float] = field(default_factory=dict)\n",
    "    correction_ratio: Optional[float] = None\n",
    "    metadata: Dict[str, Any] = field(default_factory=dict)\n",
    "\n",
    "@dataclass\n",
    "class ReasoningChain:\n",
    "    input_data: Optional[torch.Tensor] = None\n",
    "    thought_steps: List[ThoughtStep] = field(default_factory=list)\n",
    "    final_output: Optional[torch.Tensor] = None\n",
    "    confidence_trajectory: List[float] = field(default_factory=list)\n",
    "    decision_points: List[int] = field(default_factory=list)\n",
    "    reasoning_summary: str = \"\"\n",
    "    convergence_metrics: Dict[str, float] = field(default_factory=dict)\n",
    "    solver_diagnostics: List[Dict[str, Any]] = field(default_factory=list)\n",
    "\n",
    "class MCMCInterpretabilityHook:\n",
    "    def __init__(self, layer_name: str):\n",
    "        self.layer_name = layer_name\n",
    "        self.activations: List[Dict[str, Any]] = []\n",
    "        self.gradients: List[torch.Tensor] = []\n",
    "        self.attention_maps: List[Optional[torch.Tensor]] = []\n",
    "        self.mcmc_states: List[Optional[torch.Tensor]] = []\n",
    "        self.energy_values: List[float] = []\n",
    "        self.correction_ratios: List[Optional[float]] = []\n",
    "        self.solver_diagnostics: List[Dict[str, Any]] = []\n",
    "\n",
    "    def forward_hook(self, module, input_data, output_data):\n",
    "        input_tensor = input_data[0] if isinstance(input_data, tuple) else input_data\n",
    "        self.activations.append({\n",
    "            'input': input_tensor.detach().clone() if torch.is_tensor(input_tensor) else input_tensor,\n",
    "            'output': output_data.detach().clone() if torch.is_tensor(output_data) else output_data,\n",
    "            'layer': self.layer_name,\n",
    "            'timestamp': len(self.activations)\n",
    "        })\n",
    "        if hasattr(module, 'mcmc_samples') and module.mcmc_samples is not None:\n",
    "            self.mcmc_states.append(module.mcmc_samples.detach().clone())\n",
    "        if hasattr(module, 'attention_weights') and module.attention_weights is not None:\n",
    "            self.attention_maps.append(module.attention_weights.detach().clone())\n",
    "        if hasattr(module, 'correction_ratios_log') and module.correction_ratios_log: # Assuming a log attribute\n",
    "            self.correction_ratios.append(module.correction_ratios_log[-1])\n",
    "        if hasattr(module, 'solver_diagnostics_log') and module.solver_diagnostics_log: # Assuming a log attribute\n",
    "            diagnostics = module.solver_diagnostics_log[-1]\n",
    "            self.solver_diagnostics.append(diagnostics)\n",
    "            if 'last_objective_value' in diagnostics:\n",
    "                self.energy_values.append(diagnostics['last_objective_value'])\n",
    "    \n",
    "    def backward_hook(self, module, grad_input, grad_output):\n",
    "        if grad_output and grad_output[0] is not None:\n",
    "            self.gradients.append(grad_output[0].detach().clone())\n",
    "\n",
    "class BlackBoxSolver:\n",
    "    def __init__(self, model: nn.Module, device: str = 'cpu'): # Default to CPU if not specified\n",
    "        self.model = model\n",
    "        self.device = device\n",
    "        self.hooks: Dict[str, MCMCInterpretabilityHook] = {}\n",
    "        self.reasoning_chains: List[ReasoningChain] = []\n",
    "\n",
    "    def _register_hooks(self):\n",
    "        for name, module in self.model.named_modules():\n",
    "            if any(keyword in name.lower() for keyword in ['mcmc', 'enhanced', 'correction', 'fenchel', 'oracle']):\n",
    "                if name not in self.hooks:\n",
    "                    hook = MCMCInterpretabilityHook(name)\n",
    "                    self.hooks[name] = hook\n",
    "                    module.register_forward_hook(hook.forward_hook)\n",
    "    \n",
    "    def clear_hooks_data(self):\n",
    "        for hook in self.hooks.values():\n",
    "            hook.activations.clear()\n",
    "            hook.gradients.clear()\n",
    "            hook.attention_maps.clear()\n",
    "            hook.mcmc_states.clear()\n",
    "            hook.energy_values.clear()\n",
    "            hook.correction_ratios.clear()\n",
    "            hook.solver_diagnostics.clear()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07803978",
   "metadata": {},
   "source": [
    "## 5. Base MCMC Components (from `models.fenchel_young_mcmc`)\n",
    "Core classes for MCMC sampling, including configuration, temperature scheduling, and output space representation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0397809a",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class MCMCConfig:\n",
    "    num_chains: int = 3\n",
    "    chain_length: int = 20\n",
    "    burn_in: int = 5\n",
    "    temperature_schedule: str = \"geometric\"\n",
    "    initial_temp: float = 10.0\n",
    "    final_temp: float = 1.0\n",
    "    decay_rate: float = 0.995\n",
    "    neighborhood_radius: int = 1 # This is a general parameter, interpretation depends on OutputSpace\n",
    "    initialization_method: str = \"persistent\"\n",
    "\n",
    "class TemperatureScheduler:\n",
    "    @staticmethod\n",
    "    def geometric(initial_temp: float, decay_rate: float, final_temp: float):\n",
    "        def schedule(step: int) -> float:\n",
    "            return max(initial_temp * (decay_rate ** step), final_temp)\n",
    "        return schedule\n",
    "\n",
    "    @staticmethod\n",
    "    def linear(initial_temp: float, final_temp: float, total_steps: int):\n",
    "        def schedule(step: int) -> float:\n",
    "            progress = min(step / total_steps, 1.0) if total_steps > 0 else 1.0\n",
    "            return initial_temp * (1 - progress) + final_temp * progress\n",
    "        return schedule\n",
    "\n",
    "    @staticmethod\n",
    "    def constant(temperature: float):\n",
    "        def schedule(step: int) -> float:\n",
    "            return temperature\n",
    "        return schedule\n",
    "\n",
    "class DiscreteOutputSpace:\n",
    "    def __init__(self, dimension: int):\n",
    "        self.dimension = dimension\n",
    "        self._full_output_space_generated = False\n",
    "        self.output_space: List[torch.Tensor] = []\n",
    "        if self.dimension <= 4: \n",
    "            try:\n",
    "                self.output_space = self._generate_space()\n",
    "                self._full_output_space_generated = True\n",
    "            except (NotImplementedError, ValueError):\n",
    "                self.output_space = []\n",
    "\n",
    "    def _generate_space(self) -> List[torch.Tensor]:\n",
    "        raise NotImplementedError(\"Subclasses must implement _generate_space or rely on _generate_random_member_directly\")\n",
    "\n",
    "    def get_available_neighborhood_strategies(self, state: Optional[torch.Tensor] = None) -> List[str]:\n",
    "        raise NotImplementedError(\"Subclasses must implement get_available_neighborhood_strategies\")\n",
    "\n",
    "    def get_neighbors(self, state: torch.Tensor, strategy_name: str, **strategy_params) -> List[torch.Tensor]:\n",
    "        raise NotImplementedError(\"Subclasses must implement get_neighbors\")\n",
    "\n",
    "    def get_proposal_prob(self, current_state: torch.Tensor, proposed_state: torch.Tensor, strategy_name: str, **strategy_params) -> float:\n",
    "        neighbors = self.get_neighbors(current_state, strategy_name, **strategy_params)\n",
    "        if not neighbors: return 0.0\n",
    "        is_neighbor = any(torch.allclose(neighbor, proposed_state) for neighbor in neighbors)\n",
    "        return (1.0 / len(neighbors)) if is_neighbor else 0.0\n",
    "    \n",
    "    def _generate_random_member_directly(self) -> Optional[torch.Tensor]:\n",
    "        return None\n",
    "\n",
    "    def random_state(self) -> torch.Tensor:\n",
    "        direct_sample = self._generate_random_member_directly()\n",
    "        if direct_sample is not None:\n",
    "            return direct_sample\n",
    "        if self._full_output_space_generated and self.output_space:\n",
    "            return random.choice(self.output_space).clone()\n",
    "        if not self.output_space and not self._full_output_space_generated:\n",
    "            try:\n",
    "                self.output_space = self._generate_space()\n",
    "                self._full_output_space_generated = True\n",
    "                if self.output_space:\n",
    "                    return random.choice(self.output_space).clone()\n",
    "            except (NotImplementedError, ValueError) as e:\n",
    "                raise RuntimeError(f\"Cannot generate random_state for {self.__class__.__name__} (dim {self.dimension}). Error: {e}\")\n",
    "        if not self.output_space:\n",
    "             raise RuntimeError(f\"Output space empty for {self.__class__.__name__} (dim {self.dimension}). Cannot sample random_state.\")\n",
    "        return random.choice(self.output_space).clone()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0870397a",
   "metadata": {},
   "source": [
    "## 6. ARC Grid Output Space\n",
    "A specific implementation of `DiscreteOutputSpace` for ARC-like grid environments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0397a087",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ARCGridOutputSpace(DiscreteOutputSpace):\n",
    "    def __init__(self, dimension: int, grid_shape: Tuple[int, int], num_symbols: int):\n",
    "        super().__init__(dimension)\n",
    "        self.grid_shape = grid_shape\n",
    "        self.num_symbols = num_symbols\n",
    "        if dimension != grid_shape[0] * grid_shape[1]:\n",
    "            raise ValueError(f\"Dimension ({dimension}) must match grid_shape ({grid_shape[0]}*{grid_shape[1]}={grid_shape[0]*grid_shape[1]})\")\n",
    "\n",
    "    def _generate_random_member_directly(self) -> Optional[torch.Tensor]:\n",
    "        random_grid = torch.randint(0, self.num_symbols, self.grid_shape, dtype=torch.long)\n",
    "        return random_grid.view(-1).float()\n",
    "\n",
    "    def get_available_neighborhood_strategies(self, state: Optional[torch.Tensor] = None) -> List[str]:\n",
    "        return [\"flip_one_cell_value\", \"swap_two_cells\"]\n",
    "\n",
    "    def get_neighbors(self, state: torch.Tensor, strategy_name: str, **strategy_params) -> List[torch.Tensor]:\n",
    "        neighbors = []\n",
    "        state_grid = state.view(self.grid_shape).long()\n",
    "\n",
    "        if strategy_name == \"flip_one_cell_value\":\n",
    "            num_neighbors_to_generate = strategy_params.get('num_flips', min(5, self.dimension))\n",
    "            for _ in range(num_neighbors_to_generate):\n",
    "                neighbor_grid = state_grid.clone()\n",
    "                row = random.randint(0, self.grid_shape[0] - 1)\n",
    "                col = random.randint(0, self.grid_shape[1] - 1)\n",
    "                original_value = neighbor_grid[row, col].item()\n",
    "                \n",
    "                if self.num_symbols <= 1:\n",
    "                    new_value = original_value\n",
    "                else:\n",
    "                    new_value = random.randint(0, self.num_symbols - 1)\n",
    "                    while new_value == original_value:\n",
    "                        new_value = random.randint(0, self.num_symbols - 1)\n",
    "                neighbor_grid[row, col] = new_value\n",
    "                neighbors.append(neighbor_grid.view(-1).float())\n",
    "        \n",
    "        elif strategy_name == \"swap_two_cells\":\n",
    "            num_neighbors_to_generate = strategy_params.get('num_swaps', min(5, self.dimension // 2 if self.dimension >=2 else 0))\n",
    "            for _ in range(num_neighbors_to_generate):\n",
    "                if self.dimension < 2: break\n",
    "                neighbor_grid = state_grid.clone()\n",
    "                r1, c1 = random.randint(0, self.grid_shape[0] - 1), random.randint(0, self.grid_shape[1] - 1)\n",
    "                r2, c2 = random.randint(0, self.grid_shape[0] - 1), random.randint(0, self.grid_shape[1] - 1)\n",
    "                while r1 == r2 and c1 == c2:\n",
    "                    r2, c2 = random.randint(0, self.grid_shape[0] - 1), random.randint(0, self.grid_shape[1] - 1)\n",
    "                \n",
    "                val1 = neighbor_grid[r1,c1].item()\n",
    "                neighbor_grid[r1,c1] = neighbor_grid[r2,c2].item()\n",
    "                neighbor_grid[r2,c2] = val1\n",
    "                neighbors.append(neighbor_grid.view(-1).float())\n",
    "        else:\n",
    "            warnings.warn(f\"Unknown strategy: {strategy_name} for ARCGridOutputSpace. Returning empty neighbor list.\")\n",
    "        return neighbors\n",
    "\n",
    "    def _generate_space(self) -> List[torch.Tensor]:\n",
    "        if self.dimension > 6:\n",
    "            warnings.warn(f\"Full space generation for ARCGridOutputSpace with dimension {self.dimension} is too large. Returning empty list.\")\n",
    "            return []\n",
    "        return super()._generate_space()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0790397a",
   "metadata": {},
   "source": [
    "## 7. Enhanced MCMC Layers and Fenchel-Young Integration\n",
    "Includes `ExactOptimizationOracle`, MCMC samplers (`CorrectionRatioMCMC`, `LargeNeighborhoodSearchMCMC`), and the main `EnhancedCTMFenchelYoungIntegration` module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0397a079",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class ExactOptimizationOracle:\n",
    "    def __init__(self, output_space: DiscreteOutputSpace, phi_network: Optional[nn.Module] = None, model: Optional[nn.Module] = None):\n",
    "        self.output_space = output_space\n",
    "        self.phi_network = phi_network\n",
    "        self.solver_state: Dict[str, Any] = {\n",
    "            'last_solution': None, 'last_objective_value': None,\n",
    "            'num_evaluations': 0, 'optimization_history': []\n",
    "        }\n",
    "\n",
    "    def solve(self, theta: torch.Tensor, neighborhood: Optional[List[torch.Tensor]] = None) -> Optional[torch.Tensor]:\n",
    "        search_space = neighborhood\n",
    "        if search_space is None:\n",
    "            if hasattr(self.output_space, '_generate_random_member_directly') and \\\n",
    "               (not hasattr(self.output_space, 'output_space') or not self.output_space.output_space):\n",
    "                # Try to generate a small random search space if the full one is not available/too large\n",
    "                # Ensure getattr has a default for 'dimension' if it might be missing\n",
    "                dimension_val = getattr(self.output_space, 'dimension', 20) \n",
    "                search_space = [self.output_space._generate_random_member_directly() for _ in range(min(dimension_val, 20))]\n",
    "                search_space = [s for s in search_space if s is not None]\n",
    "            elif hasattr(self.output_space, 'output_space'): # Check if output_space attribute exists\n",
    "                search_space = self.output_space.output_space\n",
    "            else: # Fallback if no way to get/generate search space\n",
    "                search_space = []\n",
    "\n",
    "        # Initialize num_evaluations at the beginning of the method.\n",
    "        # It's reset per call to solve.\n",
    "        self.solver_state['num_evaluations'] = 0\n",
    "        \n",
    "        # Ensure optimization_history is initialized if it's not already present\n",
    "        if 'optimization_history' not in self.solver_state:\n",
    "            self.solver_state['optimization_history'] = []\n",
    "\n",
    "        if not search_space:\n",
    "            self.solver_state['last_solution'] = None\n",
    "            self.solver_state['last_objective_value'] = float('-inf')\n",
    "            self.solver_state['optimization_history'].append({\n",
    "                'solution': None,\n",
    "                'value': float('-inf'),\n",
    "                'search_space_size': 0\n",
    "            })\n",
    "            return None\n",
    "\n",
    "        is_batched = theta.ndim == 2\n",
    "        batch_size = theta.shape[0] if is_batched else 1\n",
    "\n",
    "        if is_batched:\n",
    "            # Stores the best candidate tensor for each item in the batch\n",
    "            best_solution_list: List[Optional[torch.Tensor]] = [None] * batch_size \n",
    "            best_value_tensor = torch.full((batch_size,), float('-inf'), device=theta.device, dtype=theta.dtype)\n",
    "        else:\n",
    "            best_solution_single: Optional[torch.Tensor] = None\n",
    "            best_value_scalar = float('-inf')\n",
    "        \n",
    "        for candidate_state_maybe_none in search_space:\n",
    "            if candidate_state_maybe_none is None:\n",
    "                continue\n",
    "            # Ensure candidate is on the same device as theta and has the same dtype\n",
    "            candidate = candidate_state_maybe_none.to(device=theta.device, dtype=theta.dtype)\n",
    "\n",
    "            current_objective_value: Union[torch.Tensor, float] \n",
    "\n",
    "            if is_batched:\n",
    "                # theta is [B, D'], candidate is [D'] -> objective_value_batch is [B]\n",
    "                current_objective_value = torch.mv(theta, candidate)\n",
    "            else:\n",
    "                # theta is [D'], candidate is [D'] -> objective_value_scalar is scalar\n",
    "                current_objective_value = torch.dot(theta, candidate)\n",
    "\n",
    "            if self.phi_network is not None:\n",
    "                # candidate is [D'], phi_network expects [N, D']\n",
    "                phi_input = candidate.unsqueeze(0) # [1, D']\n",
    "                phi_val = self.phi_network(phi_input) # Output [1, 1] or [1]\n",
    "                \n",
    "                # Squeeze to make it a scalar or 1D tensor if it was [1,1] or [1]\n",
    "                phi_val_squeezed = phi_val.squeeze()\n",
    "                \n",
    "                # Ensure phi_val_squeezed is a scalar tensor before adding\n",
    "                if phi_val_squeezed.ndim > 0 and phi_val_squeezed.numel() == 1:\n",
    "                    phi_val_squeezed = phi_val_squeezed.squeeze()\n",
    "\n",
    "                # Add scalar phi_val to objective_value (scalar or [B] tensor)\n",
    "                # This works due to broadcasting if current_objective_value is [B]\n",
    "                current_objective_value = current_objective_value + phi_val_squeezed # Ensure it's an assignment\n",
    "            \n",
    "            if is_batched:\n",
    "                # current_objective_value is a tensor of shape [B]\n",
    "                # best_value_tensor is a tensor of shape [B]\n",
    "                improved_mask = current_objective_value > best_value_tensor\n",
    "                best_value_tensor[improved_mask] = current_objective_value[improved_mask]\n",
    "                for i in range(batch_size):\n",
    "                    if improved_mask[i]:\n",
    "                        best_solution_list[i] = candidate.clone()\n",
    "            else: # not batched, current_objective_value is a scalar float or 0-dim tensor\n",
    "                obj_val_float: float\n",
    "                if isinstance(current_objective_value, torch.Tensor): # Ensure it's a Python float for comparison\n",
    "                    obj_val_float = current_objective_value.item()\n",
    "                else:\n",
    "                    # This case should ideally not happen if operations are tensor-based\n",
    "                    obj_val_float = float(current_objective_value) \n",
    "                \n",
    "                if obj_val_float > best_value_scalar:\n",
    "                    best_value_scalar = obj_val_float\n",
    "                    best_solution_single = candidate.clone()\n",
    "            \n",
    "            self.solver_state['num_evaluations'] += 1\n",
    "\n",
    "        # Update solver_state and determine return value\n",
    "        final_return_solution: Optional[torch.Tensor] = None\n",
    "\n",
    "        if is_batched:\n",
    "            # For solver_state, use the first item of the batch as a compromise\n",
    "            actual_best_solution_for_state = best_solution_list[0] if best_solution_list and best_solution_list[0] is not None else None\n",
    "            actual_best_value_for_state = float(best_value_tensor[0].item()) if best_value_tensor.numel() > 0 else float('-inf')\n",
    "\n",
    "            self.solver_state['last_solution'] = actual_best_solution_for_state.clone() if actual_best_solution_for_state is not None else None\n",
    "            self.solver_state['last_objective_value'] = actual_best_value_for_state\n",
    "            self.solver_state['optimization_history'].append({\n",
    "                'solution': actual_best_solution_for_state.clone().cpu().numpy() if actual_best_solution_for_state is not None else None,\n",
    "                'value': actual_best_value_for_state,\n",
    "                'search_space_size': len(search_space)\n",
    "            })\n",
    "\n",
    "            # For return value: if any item in batch failed to find a solution, return None. Otherwise, stack.\n",
    "            if any(s is None for s in best_solution_list):\n",
    "                final_return_solution = None\n",
    "            else:\n",
    "                # All solutions are tensors, safe to stack.\n",
    "                # Need to cast best_solution_list to List[torch.Tensor] for stack\n",
    "                final_return_solution = torch.stack([s for s in best_solution_list if s is not None])\n",
    "        \n",
    "        else: # not batched\n",
    "            self.solver_state['last_solution'] = best_solution_single.clone() if best_solution_single is not None else None\n",
    "            self.solver_state['last_objective_value'] = float(best_value_scalar)\n",
    "            self.solver_state['optimization_history'].append({\n",
    "                'solution': best_solution_single.clone().cpu().numpy() if best_solution_single is not None else None,\n",
    "                'value': float(best_value_scalar),\n",
    "                'search_space_size': len(search_space)\n",
    "            })\n",
    "            final_return_solution = best_solution_single\n",
    "            \n",
    "        return final_return_solution\n",
    "\n",
    "    def get_solver_state(self) -> Dict[str, Any]:\n",
    "        return self.solver_state.copy()\n",
    "\n",
    "    def set_solver_parameters(self, params: Dict[str, Any]) -> None:\n",
    "        if 'reset_history' in params and params['reset_history']:\n",
    "            self.solver_state['optimization_history'] = []\n",
    "            self.solver_state['num_evaluations'] = 0\n",
    "\n",
    "\n",
    "class CorrectionRatioMCMC(nn.Module):\n",
    "    def __init__(self,\n",
    "                 output_space: DiscreteOutputSpace,\n",
    "                 config: MCMCConfig,\n",
    "                 phi_network: Optional[nn.Module] = None,\n",
    "                 exact_oracle: Optional[ExactOptimizationOracle] = None):\n",
    "        super().__init__()\n",
    "        self.output_space = output_space\n",
    "        self.config = config\n",
    "        self.phi_network = phi_network\n",
    "        self.exact_oracle = exact_oracle\n",
    "        self.temp_scheduler = self._create_temperature_scheduler()\n",
    "        self.persistent_states: Optional[List[Optional[torch.Tensor]]] = None\n",
    "        self.correction_ratios_log: List[float] = []\n",
    "        self.solver_diagnostics_log: List[Dict[str, Any]] = []\n",
    "        self.step_count = 0\n",
    "\n",
    "    def _create_temperature_scheduler(self) -> Callable[[int], float]:\n",
    "        if self.config.temperature_schedule == \"geometric\":\n",
    "            return TemperatureScheduler.geometric(self.config.initial_temp, self.config.decay_rate, self.config.final_temp)\n",
    "        elif self.config.temperature_schedule == \"linear\":\n",
    "            return TemperatureScheduler.linear(self.config.initial_temp, self.config.final_temp, self.config.chain_length)\n",
    "        else:\n",
    "            return TemperatureScheduler.constant(self.config.final_temp)\n",
    "\n",
    "    def phi_function(self, state: torch.Tensor) -> torch.Tensor:\n",
    "        if self.phi_network is not None:\n",
    "            state_for_phi = state.unsqueeze(0) if state.dim() == self.output_space.dimension.bit_length() else state\n",
    "            if state_for_phi.dim() == 1:\n",
    "                state_for_phi = state_for_phi.unsqueeze(0)\n",
    "\n",
    "            return self.phi_network(state_for_phi).squeeze()\n",
    "        return torch.tensor(0.0, device=state.device)\n",
    "\n",
    "    def compute_correction_ratio(self, current: torch.Tensor, proposal: torch.Tensor, theta: torch.Tensor,\n",
    "                                 strategy_name: str, strategy_params: Dict[str, Any]) -> float:\n",
    "        if strategy_name == \"LNS\":\n",
    "            # For LNS, the proposal mechanism is different and typically involves an oracle.\n",
    "            # A common simplification is to assume the correction ratio is 1.0,\n",
    "            # effectively treating the LNS proposal as symmetric for the correction term.\n",
    "            # This means acceptance relies primarily on the energy difference.\n",
    "            # A more rigorous treatment would require defining q_LNS(y'|y) and q_LNS(y|y')\n",
    "            # based on the LNS oracle's behavior.\n",
    "            return 1.0\n",
    "\n",
    "        q_proposal_given_current = self.output_space.get_proposal_prob(current, proposal, strategy_name, **strategy_params)\n",
    "        q_current_given_proposal = self.output_space.get_proposal_prob(proposal, current, strategy_name, **strategy_params)\n",
    "\n",
    "        if q_proposal_given_current == 0: # Cannot propose this move\n",
    "            return 0.0 \n",
    "        if q_current_given_proposal == 0: # Cannot reverse this move via proposal\n",
    "            # If q(y|y') is 0, the detailed balance implies acceptance should be 0 \n",
    "            # unless E(y') is drastically lower than E(y) and the exp term dominates.\n",
    "            # Setting correction to 0 ensures this.\n",
    "            return 0.0\n",
    "        \n",
    "        correction = q_current_given_proposal / q_proposal_given_current\n",
    "        return correction\n",
    "\n",
    "\n",
    "    def enhanced_acceptance_ratio(self, current: torch.Tensor, proposal: torch.Tensor, theta: torch.Tensor,\n",
    "                                temperature: float, strategy_name: str, strategy_params: Dict[str, Any]) -> float:\n",
    "        current_energy = torch.dot(theta, current.squeeze()) + self.phi_function(current.squeeze())\n",
    "        proposal_energy = torch.dot(theta, proposal.squeeze()) + self.phi_function(proposal.squeeze())\n",
    "        energy_diff = proposal_energy - current_energy\n",
    "        \n",
    "        correction_factor = self.compute_correction_ratio(current, proposal, theta, strategy_name, strategy_params)\n",
    "        self.correction_ratios_log.append(correction_factor)\n",
    "\n",
    "        if correction_factor < 0:\n",
    "            correction_factor = 0.0\n",
    "        if temperature <= 1e-9:\n",
    "            return float('inf') if energy_diff <= 0 and correction_factor > 1e-9 else 0.0\n",
    "        \n",
    "        exp_term = torch.exp(energy_diff / temperature)\n",
    "        acceptance_term_pk = float(correction_factor * exp_term)\n",
    "        return max(0.0, acceptance_term_pk)\n",
    "\n",
    "    def large_neighborhood_search_step(self, current_state: torch.Tensor, theta: torch.Tensor,\n",
    "                                     neighborhood_size: int = 5) -> Optional[torch.Tensor]:\n",
    "        if self.exact_oracle is None:\n",
    "            available_strategies = self.output_space.get_available_neighborhood_strategies(current_state)\n",
    "            if not available_strategies:\n",
    "                return current_state\n",
    "            chosen_strategy = random.choice(available_strategies)\n",
    "            s_params = {'radius': 1} if 'radius' in chosen_strategy else {'num_flips':1} if 'flip' in chosen_strategy else {}\n",
    "\n",
    "            neighbors = self.output_space.get_neighbors(current_state, chosen_strategy, **s_params)\n",
    "            return random.choice(neighbors) if neighbors else current_state\n",
    "\n",
    "        large_neighborhood: List[torch.Tensor] = []\n",
    "        strat_params = {'num_flips': neighborhood_size // 2, 'num_swaps': neighborhood_size // 2}\n",
    "        for strat_name in self.output_space.get_available_neighborhood_strategies(current_state):\n",
    "            large_neighborhood.extend(self.output_space.get_neighbors(current_state, strat_name, **strat_params))\n",
    "            if len(large_neighborhood) >= neighborhood_size:\n",
    "                break\n",
    "        \n",
    "        while len(large_neighborhood) < neighborhood_size:\n",
    "            random_s = self.output_space.random_state()\n",
    "            if not any(torch.allclose(random_s, existing) for existing in large_neighborhood):\n",
    "                large_neighborhood.append(random_s)\n",
    "        \n",
    "        large_neighborhood = large_neighborhood[:min(len(large_neighborhood), neighborhood_size * 2)]\n",
    "\n",
    "        best_solution = self.exact_oracle.solve(theta, large_neighborhood)\n",
    "        if self.exact_oracle.solver_state:\n",
    "             self.solver_diagnostics_log.append(self.exact_oracle.get_solver_state())\n",
    "        return best_solution if best_solution is not None else current_state\n",
    "\n",
    "    def sample_chain_corrected(self, theta: torch.Tensor, chain_id: int = 0,\n",
    "                               target_y: Optional[torch.Tensor] = None, #target_state was changed to target_y to avoid errors. \n",
    "                               use_large_neighborhood_step_flag: bool = False\n",
    "                               ) -> Tuple[List[torch.Tensor], Dict[str, float]]:\n",
    "        # theta is now expected to be a 1D tensor for the current chain/batch item.\n",
    "        if self.config.initialization_method == \"persistent\" and self.persistent_states is not None and \\\n",
    "           chain_id < len(self.persistent_states) and self.persistent_states[chain_id] is not None:\n",
    "            current_state = self.persistent_states[chain_id].clone().to(theta.device)\n",
    "        elif self.config.initialization_method == \"data_based\" and target_y is not None:\n",
    "            current_state = target_y.clone().to(theta.device) # target_state is also 1D here\n",
    "        else:\n",
    "            current_state = self.output_space.random_state().to(theta.device)\n",
    "\n",
    "        samples = []\n",
    "        acceptances = 0\n",
    "        total_steps_for_chain = self.config.chain_length + self.config.burn_in\n",
    "        \n",
    "        temperature = self.config.initial_temp # Initialize temperature for the loop\n",
    "\n",
    "        for step_idx in range(total_steps_for_chain):\n",
    "            temperature = self.temp_scheduler(step_idx)\n",
    "            proposal = None\n",
    "            chosen_strategy_name = \"unknown\"\n",
    "            strategy_params: Dict[str, Any] = {}\n",
    "\n",
    "            perform_lns_this_iteration = False\n",
    "            if use_large_neighborhood_step_flag and isinstance(self, LargeNeighborhoodSearchMCMC) and self.exact_oracle:\n",
    "                lns_freq = getattr(self, 'lns_frequency', 10) \n",
    "                if lns_freq > 0 and (step_idx + 1) % lns_freq == 0:\n",
    "                    perform_lns_this_iteration = True\n",
    "            \n",
    "            if perform_lns_this_iteration and isinstance(self, LargeNeighborhoodSearchMCMC):\n",
    "                lns_hood_size = getattr(self, 'lns_neighborhood_size', 5)\n",
    "                proposal = self.large_neighborhood_search_step(current_state, theta, lns_hood_size) # Pass 1D theta\n",
    "                chosen_strategy_name = \"LNS\"\n",
    "                strategy_params = {'lns_generated': True}\n",
    "            else:\n",
    "                available_strategies = self.output_space.get_available_neighborhood_strategies(current_state)\n",
    "                if not available_strategies:\n",
    "                    if step_idx >= self.config.burn_in:\n",
    "                        samples.append(current_state.clone())\n",
    "                    continue\n",
    "                chosen_strategy_name = random.choice(available_strategies)\n",
    "                \n",
    "                if \"radius\" in chosen_strategy_name:\n",
    "                    strategy_params['radius'] = self.config.neighborhood_radius\n",
    "                elif \"flip\" in chosen_strategy_name:\n",
    "                    strategy_params['num_flips'] = 1\n",
    "                elif \"swap\" in chosen_strategy_name:\n",
    "                    strategy_params['num_swaps'] = 1\n",
    "                else:\n",
    "                    strategy_params['radius'] = self.config.neighborhood_radius\n",
    "\n",
    "                neighbors = self.output_space.get_neighbors(current_state, chosen_strategy_name, **strategy_params)\n",
    "                if not neighbors:\n",
    "                    if step_idx >= self.config.burn_in:\n",
    "                        samples.append(current_state.clone())\n",
    "                    continue\n",
    "                proposal = random.choice(neighbors)\n",
    "            \n",
    "            if proposal is None:\n",
    "                if step_idx >= self.config.burn_in:\n",
    "                    samples.append(current_state.clone())\n",
    "                continue\n",
    "            \n",
    "            proposal = proposal.to(theta.device)\n",
    "\n",
    "            # MODIFIED LINE: Pass 1D theta directly\n",
    "            acceptance_term_pk = self.enhanced_acceptance_ratio(current_state, proposal, theta, temperature, chosen_strategy_name, strategy_params)\n",
    "            \n",
    "            if random.random() < min(1.0, acceptance_term_pk):\n",
    "                current_state = proposal\n",
    "                acceptances += 1\n",
    "            \n",
    "            if step_idx >= self.config.burn_in:\n",
    "                samples.append(current_state.clone())\n",
    "        \n",
    "        if self.persistent_states is None or len(self.persistent_states) != self.config.num_chains:\n",
    "             self.persistent_states = [None for _ in range(self.config.num_chains)] # Should match num_chains for indexing\n",
    "        if chain_id < len(self.persistent_states): # Ensure chain_id is a valid index\n",
    "            self.persistent_states[chain_id] = current_state.clone()\n",
    "        else:\n",
    "            # This case should ideally not be reached if chain_id is always < self.config.num_chains\n",
    "            warnings.warn(f\"chain_id {chain_id} is out of bounds for persistent_states (len {len(self.persistent_states)}). Skipping persistence update for this chain.\")\n",
    "\n",
    "        stats = {\n",
    "            'acceptance_rate': acceptances / total_steps_for_chain if total_steps_for_chain > 0 else 0.0,\n",
    "            'final_temperature': temperature,\n",
    "            'chain_length_collected': len(samples)\n",
    "        }\n",
    "        return samples, stats\n",
    "\n",
    "    def estimate_expectation_with_corrections(self, theta_batch: torch.Tensor, target_state_batch: Optional[torch.Tensor] = None,\n",
    "                                              use_large_neighborhood: bool = False\n",
    "                                              ) -> Tuple[torch.Tensor, Dict[str, Any]]:\n",
    "        # theta_batch is expected to be (batch_size, feature_dim)\n",
    "        # target_state_batch is expected to be (batch_size, feature_dim) or None\n",
    "        \n",
    "        if theta_batch.ndim == 1: # If a single theta is passed, unsqueeze to make it a batch of 1\n",
    "            theta_batch = theta_batch.unsqueeze(0)\n",
    "            if target_state_batch is not None and target_state_batch.ndim == 1:\n",
    "                target_state_batch = target_state_batch.unsqueeze(0)\n",
    "\n",
    "        batch_size = theta_batch.shape[0]\n",
    "        all_batch_expectations: List[torch.Tensor] = []\n",
    "        all_batch_stats_collector: List[Dict[str, Any]] = []\n",
    "\n",
    "        # Initialize persistent_states if needed. It's a list of length self.config.num_chains.\n",
    "        # Each call to sample_chain_corrected for a given chain_id will use/update the corresponding persistent state.\n",
    "        if self.persistent_states is None or len(self.persistent_states) != self.config.num_chains:\n",
    "            self.persistent_states = [None for _ in range(self.config.num_chains)]\n",
    "\n",
    "        for i in range(batch_size):\n",
    "            current_theta_item = theta_batch[i]  # This is 1D: (feature_dim,)\n",
    "            current_target_state_item = None\n",
    "            if target_state_batch is not None:\n",
    "                if target_state_batch.shape[0] == batch_size: # Ensure target_state_batch matches batch size\n",
    "                    current_target_state_item = target_state_batch[i] # Also 1D\n",
    "                elif batch_size == 1 and target_state_batch.ndim == 1: # Special case: single item batch, target might be 1D\n",
    "                     current_target_state_item = target_state_batch\n",
    "            # Else, if target_state_batch is not None but dimensions mismatch, current_target_state_item remains None\n",
    "            # or you could raise an error or warning. For now, it defaults to None if not perfectly aligned.\n",
    "\n",
    "            item_all_samples: List[torch.Tensor] = []\n",
    "            item_all_chain_stats: List[Dict[str, float]] = []\n",
    "\n",
    "            for chain_id in range(self.config.num_chains):\n",
    "                is_lns_sampler = isinstance(self, LargeNeighborhoodSearchMCMC)\n",
    "                # sample_chain_corrected now receives 1D theta (current_theta_item)\n",
    "                # and 1D target_state (current_target_state_item)\n",
    "                samples, chain_stats_for_chain = self.sample_chain_corrected(\n",
    "                    current_theta_item,\n",
    "                    chain_id,\n",
    "                    current_target_state_item,\n",
    "                    use_large_neighborhood_step_flag=(use_large_neighborhood and is_lns_sampler)\n",
    "                )\n",
    "                item_all_samples.extend(samples)\n",
    "                item_all_chain_stats.append(chain_stats_for_chain)\n",
    "\n",
    "            if not item_all_samples:\n",
    "                # Fallback for this specific batch item\n",
    "                # Ensure output_space and its dimension attribute are correctly defined\n",
    "                item_expectation = torch.zeros(self.output_space.dimension, device=current_theta_item.device, dtype=current_theta_item.dtype)\n",
    "                warnings.warn(f\"No MCMC samples collected for batch item {i}. Returning zeros for this item.\")\n",
    "                item_stats_dict = {'error': f'No samples collected for batch item {i}', 'num_samples': 0, 'avg_acceptance_rate': 0.0, 'sample_entropy': 0.0, 'chain_stats': []}\n",
    "            else:\n",
    "                item_expectation = torch.mean(torch.stack(item_all_samples).float(), dim=0)\n",
    "                avg_acceptance_item = np.mean([s['acceptance_rate'] for s in item_all_chain_stats if 'acceptance_rate' in s]) if item_all_chain_stats else 0.0\n",
    "                sample_entropy_item = compute_normalized_entropy(torch.stack(item_all_samples).detach().cpu()) if item_all_samples else 0.0\n",
    "                item_stats_dict = {\n",
    "                    'num_samples': len(item_all_samples),\n",
    "                    'avg_acceptance_rate': float(avg_acceptance_item),\n",
    "                    'sample_entropy': sample_entropy_item.tolist() if isinstance(sample_entropy_item, torch.Tensor) else float(sample_entropy_item),\n",
    "                    'chain_stats': item_all_chain_stats\n",
    "                }\n",
    "            \n",
    "            all_batch_expectations.append(item_expectation)\n",
    "            all_batch_stats_collector.append(item_stats_dict)\n",
    "\n",
    "        if not all_batch_expectations: # Handles batch_size = 0\n",
    "            fallback_dim = theta_batch.shape[1] if theta_batch.ndim == 2 and theta_batch.shape[0] == 0 else self.output_space.dimension\n",
    "            final_expectation = torch.empty(0, fallback_dim, device=theta_batch.device, dtype=theta_batch.dtype)\n",
    "            combined_summary_stats = {'error': 'No batch items processed or all failed', 'batch_item_stats': [], 'overall_avg_acceptance_rate': 0.0, 'total_samples_collected': 0}\n",
    "            warnings.warn(f\"No expectations computed for any batch item. Returning empty tensor.\")\n",
    "            return final_expectation, combined_summary_stats\n",
    "\n",
    "        final_expectation = torch.stack(all_batch_expectations) # Stack to get (B, D)\n",
    "\n",
    "        # Aggregate statistics\n",
    "        overall_avg_acceptance = 0.0\n",
    "        total_samples = 0\n",
    "        if all_batch_stats_collector:\n",
    "            rates = [s['avg_acceptance_rate'] for s in all_batch_stats_collector if s.get('num_samples', 0) > 0]\n",
    "            if rates:\n",
    "                overall_avg_acceptance = np.mean(rates)\n",
    "            total_samples = sum(s.get('num_samples', 0) for s in all_batch_stats_collector)\n",
    "    \n",
    "        combined_summary_stats = {\n",
    "            'batch_item_stats': all_batch_stats_collector, # Detailed stats per item\n",
    "            'overall_avg_acceptance_rate': float(overall_avg_acceptance),\n",
    "            'total_samples_collected': total_samples\n",
    "        }\n",
    "        return final_expectation, combined_summary_stats\n",
    "    \n",
    "    def forward(self, theta: torch.Tensor, target: torch.Tensor, use_large_neighborhood: bool = False\n",
    "               ) -> Tuple[torch.Tensor, Dict[str, Any]]:\n",
    "        expectation, stats = self.estimate_expectation_with_corrections(theta, target_y=target, use_large_neighborhood=use_large_neighborhood)\n",
    "        return expectation, stats\n",
    "\n",
    "\n",
    "class LargeNeighborhoodSearchMCMC(CorrectionRatioMCMC):\n",
    "    def __init__(self,\n",
    "                 output_space: DiscreteOutputSpace,\n",
    "                 config: MCMCConfig,\n",
    "                 phi_network: Optional[nn.Module] = None,\n",
    "                 lns_frequency: int = 10,\n",
    "                 lns_neighborhood_size: int = 20):\n",
    "        exact_oracle = ExactOptimizationOracle(output_space, phi_network)\n",
    "        super().__init__(output_space, config, phi_network, exact_oracle)\n",
    "        self.lns_frequency = lns_frequency\n",
    "        self.lns_neighborhood_size = lns_neighborhood_size\n",
    "\n",
    "\n",
    "class EnhancedCTMFenchelYoungIntegration(nn.Module):\n",
    "    def __init__(self,\n",
    "                 input_dim: int,\n",
    "                 output_space: DiscreteOutputSpace,\n",
    "                 mcmc_config: MCMCConfig,\n",
    "                 hidden_dim: int = 256,\n",
    "                 num_thought_steps: int = 5,\n",
    "                 use_large_neighborhood_search: bool = True,\n",
    "                 lns_frequency: int = 10,\n",
    "                 lns_neighborhood_size: int = 20):\n",
    "        super().__init__()\n",
    "        self.output_space_dim = output_space.dimension\n",
    "        \n",
    "        self.thought_network = nn.Sequential(\n",
    "            nn.Linear(input_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, self.output_space_dim)\n",
    "        )\n",
    "        \n",
    "        self.phi_network = nn.Sequential(\n",
    "            nn.Linear(self.output_space_dim, hidden_dim // 2),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim // 2, 1)\n",
    "        )\n",
    "        \n",
    "        if use_large_neighborhood_search:\n",
    "            self.mcmc_sampler: Union[LargeNeighborhoodSearchMCMC, CorrectionRatioMCMC] = LargeNeighborhoodSearchMCMC(\n",
    "                output_space=output_space, config=mcmc_config, phi_network=self.phi_network,\n",
    "                lns_frequency=lns_frequency, lns_neighborhood_size=lns_neighborhood_size\n",
    "            )\n",
    "        else:\n",
    "            self.mcmc_sampler = CorrectionRatioMCMC(\n",
    "                output_space=output_space, config=mcmc_config, phi_network=self.phi_network\n",
    "            )\n",
    "        self.num_thought_steps = num_thought_steps\n",
    "\n",
    "    def forward(self, x: torch.Tensor, target_y: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor, Dict[str, Any]]:\n",
    "        theta = self.thought_network(x)\n",
    "        \n",
    "        expectation_y, mcmc_stats = self.mcmc_sampler.estimate_expectation_with_corrections(\n",
    "            theta, target_y,\n",
    "            use_large_neighborhood=isinstance(self.mcmc_sampler, LargeNeighborhoodSearchMCMC)\n",
    "        )\n",
    "        \n",
    "        # The Fenchel-Young loss is typically <theta, E[y]> - <theta, y_target>\n",
    "        # The gradient w.r.t. theta is simply E[y] - y_target\n",
    "        loss = torch.sum(theta * (expectation_y.detach() - target_y))\n",
    "        \n",
    "        return loss, expectation_y, mcmc_stats"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "070937a0",
   "metadata": {},
   "source": [
    "## 8. Instantiation and Configuration\n",
    "This section defines necessary global configuration variables and then instantiates the core components."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "0937a070",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using MAX_GRID_SIZE: (30, 30)\n",
      "Using NUM_ARC_SYMBOLS: 10\n",
      "Using ARC_INPUT_FLAT_DIM: 900\n",
      "Using MCMC_CONFIG_ARC: chains=3, length=20\n"
     ]
    }
   ],
   "source": [
    "# --- Configuration Variables ---\n",
    "# These variables define the ARC environment and MCMC behavior.\n",
    "# NOTE: You must provide the paths to your ARC dataset directories.\n",
    "ARC_TRAIN_DIR = \"../data/training\" # <<< IMPORTANT: SET THIS PATH\n",
    "\n",
    "import os\n",
    "\n",
    "# Get the notebook's current working directory\n",
    "notebook_cwd = os.getcwd()\n",
    "eval_dir = os.path.abspath(os.path.join(notebook_cwd, '..', 'data', 'evaluation')) # <<< IMPORTANT: SET THIS PATH\n",
    "\n",
    "MAX_GRID_SIZE = (30, 30)\n",
    "NUM_ARC_SYMBOLS = 10\n",
    "PADDING_VALUE = -1 # A value not in 0-9 to be ignored by the loss function\n",
    "MAX_DEMO_PAIRS = 5 # Max number of demonstration pairs to consider for context\n",
    "\n",
    "# Configuration for ARC-AGI-2 Training (shared constants)\n",
    "ARC_INPUT_FLAT_DIM = MAX_GRID_SIZE[0] * MAX_GRID_SIZE[1]\n",
    "\n",
    "# MCMC Configuration for ARC\n",
    "MCMC_OUTPUT_SPACE_DIM = ARC_INPUT_FLAT_DIM\n",
    "MCMC_CONFIG_ARC = MCMCConfig(\n",
    "    num_chains=3, \n",
    "    chain_length=20,\n",
    "    burn_in=5,\n",
    "    initial_temp=5.0,\n",
    "    final_temp=1.0,\n",
    "    temperature_schedule=\"geometric\",\n",
    "    decay_rate=0.95,\n",
    "    neighborhood_radius=1\n",
    ")\n",
    "ENABLE_CTM_MCMC_INTEGRATION_FOR_ARC = True\n",
    "\n",
    "print(f\"Using MAX_GRID_SIZE: {MAX_GRID_SIZE}\")\n",
    "print(f\"Using NUM_ARC_SYMBOLS: {NUM_ARC_SYMBOLS}\")\n",
    "print(f\"Using ARC_INPUT_FLAT_DIM: {ARC_INPUT_FLAT_DIM}\")\n",
    "print(f\"Using MCMC_CONFIG_ARC: chains={MCMC_CONFIG_ARC.num_chains}, length={MCMC_CONFIG_ARC.chain_length}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8467d801",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "EnhancedCTMFenchelYoungIntegration module initialized.\n",
      "  Output space dimension: 900\n",
      "  MCMC sampler type: LargeNeighborhoodSearchMCMC\n",
      "    LNS Frequency: 5\n",
      "    LNS Neighborhood Size: 10\n",
      "\n",
      "ENHANCED_MCMC_AVAILABLE set to: True\n"
     ]
    }
   ],
   "source": [
    "# --- Instantiation ---\n",
    "arc_grid_output_space = ARCGridOutputSpace(\n",
    "    dimension=ARC_INPUT_FLAT_DIM,\n",
    "    grid_shape=MAX_GRID_SIZE,\n",
    "    num_symbols=NUM_ARC_SYMBOLS\n",
    ")\n",
    "\n",
    "ctm_encoder_output_dim = ARC_INPUT_FLAT_DIM \n",
    "\n",
    "enhanced_ctm_mcmc = None\n",
    "if ENABLE_CTM_MCMC_INTEGRATION_FOR_ARC:\n",
    "    enhanced_ctm_mcmc = EnhancedCTMFenchelYoungIntegration(\n",
    "        input_dim=ctm_encoder_output_dim, \n",
    "        output_space=arc_grid_output_space,\n",
    "        mcmc_config=MCMC_CONFIG_ARC,\n",
    "        use_large_neighborhood_search=True,\n",
    "        lns_frequency=5,\n",
    "        lns_neighborhood_size=10\n",
    "    )\n",
    "\n",
    "    print(f\"\\nEnhancedCTMFenchelYoungIntegration module initialized.\")\n",
    "    print(f\"  Output space dimension: {enhanced_ctm_mcmc.output_space_dim}\")\n",
    "    if isinstance(enhanced_ctm_mcmc.mcmc_sampler, LargeNeighborhoodSearchMCMC):\n",
    "        print(f\"  MCMC sampler type: LargeNeighborhoodSearchMCMC\")\n",
    "        print(f\"    LNS Frequency: {enhanced_ctm_mcmc.mcmc_sampler.lns_frequency}\")\n",
    "        print(f\"    LNS Neighborhood Size: {enhanced_ctm_mcmc.mcmc_sampler.lns_neighborhood_size}\")\n",
    "    else:\n",
    "        print(\"  MCMC sampler type: CorrectionRatioMCMC\")\n",
    "    ENHANCED_MCMC_AVAILABLE = True\n",
    "    print(f\"\\nENHANCED_MCMC_AVAILABLE set to: {ENHANCED_MCMC_AVAILABLE}\")\n",
    "else:\n",
    "    print(\"\\nMCMC Integration is disabled for ARC.\")\n",
    "    ENHANCED_MCMC_AVAILABLE = False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "073178b6",
   "metadata": {},
   "source": [
    "# --- ARC Dataset and Dataloader Logic --- #All Module Paths should now be defined since the modules that are not on path are automatically added to path. \n",
    "\n",
    "# Note: The function `pad_grid` is called but not defined in the original source.\n",
    "# It is required for the NewCustomARCGridDataset to function correctly.\n",
    "# You must provide its definition. A placeholder is provided below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b678317",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ARC Output Head Dim: 9000\n",
      "\n",
      "-----------------------------------------------------------------------------\n",
      "Initializing Configuration and Model for ARC with EnhancedCTMDiffusion\n",
      "-----------------------------------------------------------------------------\n",
      "Warning: inferred_task_latent_dim not found or is None in config, defaulting to 64.\n",
      "✓ EnhancedCTMConfig for ARC (config_arc_diffusion) created.\n",
      "Using enhanced neuron select type: bio_multi_objective\n",
      "Using enhanced neuron select type: bio_multi_objective\n",
      "Using enhanced neuron select type: bio_multi_objective\n",
      "✓ Integration Flow + Task-Aware HiPA sampler initialized\n",
      "  - Model type: VE\n",
      "  - Intelligent modality detection enabled\n",
      "  - HiPA will be applied only to appropriate data types\n",
      "  - Text/discrete tasks protected from frequency corruption\n",
      "✓ EnhancedCTMDiffusion model for ARC (ctm_model_arc) initialized.\n",
      "✓ ARC Output Head initialized (input_dim: 64, output_dim: 9000).\n",
      "Re-initializing external enhanced_ctm_mcmc for new input_dim 64\n",
      "✓ External MCMC Integration for ARC is enabled.\n",
      "✓ ARC models (EnhancedCTMDiffusion) and optimizer prepared with Accelerate.\n",
      "ARC Checkpoints will be saved to: checkpoints/ctm_arc_agi_2_enhanced_diffusion\n",
      "NewCustomARCGridDataset: Looking for tasks in: ../data/training\n",
      "NewCustomARCGridDataset: Loaded 1000 ARC tasks from ../data/training.\n",
      "NewCustomARCGridDataset: Looking for tasks in: ../data/evaluation\n",
      "NewCustomARCGridDataset: Loaded 120 ARC tasks from ../data/evaluation.\n",
      "✓ ARC Training DataLoader initialized with 1000 tasks.\n",
      "✓ ARC Evaluation DataLoader initialized with 120 tasks.\n",
      "\n",
      "✓ ARC-AGI-2 Setup Complete.\n"
     ]
    }
   ],
   "source": [
    "ARC_EVAL_DIR = \"/workspace/Arc-AGI-2/contineous-thought-machines/data/evaluation\"  #Evaluation Dataset Directory.\n",
    "\n",
    "# --- Context: 2D Grid Padding (from original code) ---\n",
    "# This function handles padding at the 2D grid level, before serialization.\n",
    "def pad_grid(grid_list, max_dims, pad_value):\n",
    "    \"\"\"Pads a 2D grid to specified maximum dimensions.\"\"\"\n",
    "    grid_np = np.array(grid_list, dtype=np.int32)\n",
    "    padded_grid = np.full(max_dims, pad_value, dtype=np.int32)\n",
    "    h, w = grid_np.shape\n",
    "    padded_grid[:h, :w] = grid_np\n",
    "    return padded_grid\n",
    "\n",
    "# --- Fix: Byte Sequence Padding for the Model --- #\n",
    "# According to the model explanation, the key step is to pad the *serialized byte sequence*\n",
    "# to `config.max_sequence_length`. The function below implements this logic.\n",
    "\n",
    "# Define the model's expected input dimension from the configuration.\n",
    "MAX_SEQUENCE_LENGTH = 8192\n",
    "PADDING_BYTE_VALUE = 0\n",
    "\n",
    "def serialize_and_pad_grid(grid, max_len=MAX_SEQUENCE_LENGTH, pad_value=PADDING_BYTE_VALUE):\n",
    "    \"\"\"\n",
    "    Serializes a grid into a byte sequence and pads it to a fixed length.\n",
    "\n",
    "    This function implements the required padding logic for the LearnedBytePatcherEncoder.\n",
    "    It takes a grid, converts it to a flat byte sequence, and then pads or truncates\n",
    "    it to `max_sequence_length` (8192 bytes), ensuring a fixed-size input for the model.\n",
    "    \n",
    "    Args:\n",
    "        grid (list or np.ndarray): The input ARC grid.\n",
    "        max_len (int): The target length for the byte sequence, corresponding to\n",
    "                       `config.max_sequence_length`.\n",
    "        pad_value (int): The byte value to use for padding (0-255).\n",
    "\n",
    "    Returns:\n",
    "        bytes: The padded byte sequence of length `max_len`.\n",
    "    \"\"\"\n",
    "    # Convert the grid to a NumPy array of single bytes (uint8) and flatten it.\n",
    "    # ARC values (0-9) fit perfectly within a single byte.\n",
    "    flat_array = np.array(grid, dtype=np.uint8).flatten()\n",
    "\n",
    "    # Serialize the flattened array into a raw byte sequence.\n",
    "    byte_sequence = flat_array.tobytes()\n",
    "\n",
    "    # Calculate the number of padding bytes needed.\n",
    "    padding_len = max_len - len(byte_sequence)\n",
    "\n",
    "    if padding_len < 0:\n",
    "        # If the original sequence is too long, truncate it.\n",
    "        padded_sequence = byte_sequence[:max_len]\n",
    "    else:\n",
    "        # If the sequence is shorter, create padding and append it.\n",
    "        padding = bytes([pad_value] * padding_len)\n",
    "        padded_sequence = byte_sequence + padding\n",
    "        \n",
    "    return padded_sequence\n",
    "\n",
    "class NewCustomARCGridDataset(Dataset):\n",
    "    def __init__(self, data_dir, max_grid_size=MAX_GRID_SIZE, padding_value=PADDING_VALUE):\n",
    "        self.data_dir = data_dir\n",
    "        self.task_files = glob.glob(os.path.join(data_dir, \"*.json\"))\n",
    "        self.max_grid_size = max_grid_size\n",
    "        self.padding_value = padding_value\n",
    "        self.tasks = []\n",
    "        print(f\"NewCustomARCGridDataset: Looking for tasks in: {data_dir}\")\n",
    "        if not self.task_files:\n",
    "            print(f\"NewCustomARCGridDataset Warning: No JSON files found in {data_dir}. Dataset will be empty.\")\n",
    "        for task_file in self.task_files:\n",
    "            try:\n",
    "                with open(task_file, 'r') as f:\n",
    "                    self.tasks.append(json.load(f))\n",
    "            except Exception as e:\n",
    "                print(f\"NewCustomARCGridDataset Warning: Could not load or parse {task_file}: {e}\")\n",
    "        if not self.tasks:\n",
    "            print(f\"NewCustomARCGridDataset Warning: No tasks successfully loaded from {data_dir}.\")\n",
    "        else:\n",
    "            print(f\"NewCustomARCGridDataset: Loaded {len(self.tasks)} ARC tasks from {data_dir}.\")\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.tasks)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        task_data = self.tasks[idx]\n",
    "        processed_task = {'train': [], 'test': [], 'id': os.path.basename(self.task_files[idx]) if idx < len(self.task_files) else 'unknown_task'}\n",
    "\n",
    "        for pair_type in ['train', 'test']:\n",
    "            for item in task_data.get(pair_type, []):\n",
    "                input_grid_list = item.get('input', [])\n",
    "                output_grid_list = item.get('output', [])\n",
    "                \n",
    "                original_input_dims = (len(input_grid_list), len(input_grid_list[0]) if input_grid_list and input_grid_list[0] else (0,0))\n",
    "                original_output_dims = (len(output_grid_list), len(output_grid_list[0]) if output_grid_list and output_grid_list[0] else (0,0))\n",
    "\n",
    "                padded_input_np = pad_grid(input_grid_list, self.max_grid_size, self.padding_value)\n",
    "                padded_output_np = pad_grid(output_grid_list, self.max_grid_size, self.padding_value)\n",
    "                \n",
    "                processed_task[pair_type].append({\n",
    "                    'input': torch.from_numpy(padded_input_np).long(),\n",
    "                    'output': torch.from_numpy(padded_output_np).long(),\n",
    "                    'original_input_dims': original_input_dims,\n",
    "                    'original_output_dims': original_output_dims\n",
    "                })\n",
    "        return processed_task\n",
    "\n",
    "def collate_fn_new_custom_arc(batch_of_tasks):\n",
    "    input_byte_sequences_list = []\n",
    "    target_byte_sequences_for_diffusion_list = []\n",
    "    original_target_grids_for_ce_loss_list = []\n",
    "\n",
    "    for task in batch_of_tasks:\n",
    "        if not isinstance(task, dict):\n",
    "            continue\n",
    "\n",
    "        # Process 'train' pairs from the task\n",
    "        for train_pair in task.get('train', []):\n",
    "            if not isinstance(train_pair, dict) or 'input' not in train_pair or 'output' not in train_pair:\n",
    "                continue\n",
    "\n",
    "            # train_pair['input'] and train_pair['output'] are already padded 2D LongTensors from NewCustomARCGridDataset\n",
    "            input_grid_np = train_pair['input'].numpy() # Convert to numpy for serialize_and_pad_grid\n",
    "            target_grid_np = train_pair['output'].numpy()\n",
    "\n",
    "            # 1. Create input_byte_sequences (uint8)\n",
    "            input_bytes = serialize_and_pad_grid(input_grid_np, max_len=MAX_SEQUENCE_LENGTH, pad_value=PADDING_BYTE_VALUE)\n",
    "            input_byte_sequences_list.append(torch.tensor(list(input_bytes), dtype=torch.uint8))\n",
    "\n",
    "            # 2. Create target_byte_sequences_for_diffusion (uint8)\n",
    "            target_bytes_for_diffusion = serialize_and_pad_grid(target_grid_np, max_len=MAX_SEQUENCE_LENGTH, pad_value=PADDING_BYTE_VALUE)\n",
    "            target_byte_sequences_for_diffusion_list.append(torch.tensor(list(target_bytes_for_diffusion), dtype=torch.uint8))\n",
    "\n",
    "            # 3. Keep original_target_grids_for_ce_loss (long tensor, flattened)\n",
    "            original_target_grids_for_ce_loss_list.append(train_pair['output'].view(-1)) # Flattened LongTensor\n",
    "            \n",
    "    if not input_byte_sequences_list:\n",
    "        return {\n",
    "            'input_byte_sequences': torch.empty(0, MAX_SEQUENCE_LENGTH, dtype=torch.uint8),\n",
    "            'target_byte_sequences_for_diffusion': torch.empty(0, MAX_SEQUENCE_LENGTH, dtype=torch.uint8),\n",
    "            'original_target_grids_for_ce_loss': torch.empty(0, ARC_INPUT_FLAT_DIM, dtype=torch.long),\n",
    "        }\n",
    "\n",
    "    # Stack all collected tensors\n",
    "    final_input_byte_sequences = torch.stack(input_byte_sequences_list)\n",
    "    final_target_byte_sequences_for_diffusion = torch.stack(target_byte_sequences_for_diffusion_list)\n",
    "    final_original_target_grids_for_ce_loss = torch.stack(original_target_grids_for_ce_loss_list)\n",
    "    \n",
    "    return {\n",
    "        'input_byte_sequences': final_input_byte_sequences,\n",
    "        'target_byte_sequences_for_diffusion': final_target_byte_sequences_for_diffusion,\n",
    "        'original_target_grids_for_ce_loss': final_original_target_grids_for_ce_loss,\n",
    "    }\n",
    "\n",
    "# --- ARC Training Setup ---\n",
    "ARC_OUTPUT_HEAD_DIM = ARC_INPUT_FLAT_DIM * NUM_ARC_SYMBOLS\n",
    "ARC_TASK_ID = 3\n",
    "print(f\"ARC Output Head Dim: {ARC_OUTPUT_HEAD_DIM}\")\n",
    "\n",
    "ctm_model_arc, arc_output_head, optimizer_arc, ctm_mcmc_integration_arc, accelerator_arc = None, None, None, None, None\n",
    "\n",
    "print(\"\\n-----------------------------------------------------------------------------\")\n",
    "print(\"Initializing Configuration and Model for ARC with EnhancedCTMDiffusion\")\n",
    "print(\"-----------------------------------------------------------------------------\")\n",
    "\n",
    "'''\n",
    "You do not need to add any of the variables again from the ctm_Diffusion_NEWNEW.py file to your config_arc_diffusion in the Arc_AGI_2_Final.ipynb file. All the parameters you listed are already explicitly defined when config_arc_diffusion is created (between lines 2200 and 2378 approximately).\n",
    "\n",
    "The EnhancedCTMConfig class in ctm_Diffusion_NEWNEW.py provides default values for its fields. When you create an instance like config_arc_diffusion, any parameters you explicitly set will override these defaults. Since all the parameters in your list are already set in your notebook, those are the values that will be used for training.\n",
    "\n",
    "For example:\n",
    "\n",
    "attention_type is set to \"subquadratic\" on line 2260.\n",
    "positional_embedding_type is set to 'multi-learnable-fourier' on line 2276.\n",
    "enable_pipeline_parallelism is set to True on line 2288.\n",
    "And so on for all the other parameters you mentioned.\n",
    "If you wish to change any of these settings, you should modify their values directly in the existing config_arc_diffusion definition within your Arc_AGI_2_Final.ipynb file.\n",
    "'''\n",
    "\n",
    "# Define EnhancedCTMConfig for ARC with EnhancedCTMDiffusion\n",
    "# Assuming EnhancedCTMConfig is a defined class and MAX_SEQUENCE_LENGTH is a defined variable\n",
    "# For example:\n",
    "# from your_model_library import EnhancedCTMConfig\n",
    "# MAX_SEQUENCE_LENGTH = 8192\n",
    "\n",
    "# From contineous-thought-machines/models/constants.py\n",
    "VALID_NEURON_SELECT_TYPES = [\n",
    "    'first-last', 'random', 'random-pairing',  # Legacy\n",
    "    # Biologically-inspired types\n",
    "    'bio_hebbian', 'bio_plasticity', 'bio_competitive', 'bio_homeostatic',\n",
    "    'bio_evolutionary', 'bio_stdp', 'bio_criticality', 'bio_multi_objective',\n",
    "    # Hybrid approaches\n",
    "    'adaptive_random', 'performance_guided', 'task_aware'\n",
    "]\n",
    "\n",
    "VALID_POSITIONAL_EMBEDDING_TYPES = [\n",
    "    'learnable-fourier', 'multi-learnable-fourier',\n",
    "    'custom-rotational', 'custom-rotational-1d'\n",
    "]\n",
    "\n",
    "# From contineous-thought-machines/models/ctm_Diffusion_NEWNEW.py\n",
    "from dataclasses import dataclass, field\n",
    "from typing import Dict, Optional, Tuple, Union, Any, List\n",
    "\n",
    "@dataclass\n",
    "class EnhancedCTMConfig: # Renamed from ContinualLearningConfig for consistency in the target file\n",
    "    \"\"\"Enhanced configuration for continual learning CTM-diffusion model,\n",
    "    incorporating binary processing, multi-task learning, and advanced CTM features.\"\"\"\n",
    "    \n",
    "    # Model architecture (General Transformer/Diffusion settings)\n",
    "    d_model: int = 512  # Main model dimensionality\n",
    "    n_heads: int = 8\n",
    "    n_layers: int = 24\n",
    "    max_sequence_length: int = 8192 # Max input sequence length in terms of bytes or patches\n",
    "    dropout: float = 0.1\n",
    "    \n",
    "    # --- Byte Processing Options ---\n",
    "    patch_embedding_dim: int = 256         # <<< NEW: Output embedding dimension per patch from patcher\n",
    "    patch_encoder_cnn_channels: int = 64   # <<< NEW: Intermediate channels for CNN patch encoder\n",
    "\n",
    "    # --- Dynamic Entropy Patching Options (Inspired by BLT paper) ---\n",
    "    use_dynamic_entropy_patcher: bool = True # Flag to enable dynamic entropy-based patching\n",
    "    entropy_patcher_threshold_type: str = \"global\"  # 'global' or 'relative_monotonic'\n",
    "    entropy_patcher_global_threshold: float = 0.75 # Entropy threshold for 'global' type\n",
    "    entropy_patcher_relative_threshold: float = 0.1 # Entropy diff threshold for 'relative_monotonic'\n",
    "    entropy_patcher_min_patch_size: int = 4      # Minimum number of bytes in a dynamic patch\n",
    "    entropy_patcher_max_patch_size: int = 128    # Maximum number of bytes in a dynamic patch (for CNN encoder)\n",
    "    \n",
    "    # --- Learnable Entropy Model Parameters (for _EntropyProxyModel) ---\n",
    "    entropy_model_byte_vocab_size: int = 256\n",
    "    entropy_model_embedding_dim: int = 64\n",
    "    entropy_model_hidden_dim: int = 128\n",
    "    entropy_model_num_layers: int = 1\n",
    "    entropy_model_dropout: float = 0.1\n",
    "    entropy_model_loss_weight: float = 0.1 # Weight for its auxiliary loss contribution\n",
    "    # Note: These parameters are used if use_dynamic_entropy_patcher is True,\n",
    "    # as LearnedBytePatcherEncoder now instantiates the learnable _EntropyProxyModel.\n",
    "    \n",
    "    # Fallback if not using learned_patch_encoder or dynamic_entropy_patcher\n",
    "    byte_embedding_dim: int = 256\n",
    "    multi_granularity: bool = False # Default to False if patcher is preferred\n",
    "    # multi_granularity_output_dim is complex to predefine, MGP should expose its output dim.\n",
    "    # For now, if multi_granularity is True AND use_learned_patch_encoder is False, this would be used.\n",
    "    multi_granularity_output_dim: int = 256 # Placeholder if MGP is used.\n",
    "    \n",
    "    hierarchical_processing: bool = True # General flag, could apply to patcher or MGP\n",
    "    \n",
    "    # CTM Core Parameters (Specific to the OriginalCTMCore module)\n",
    "    # These are prefixed with 'ctm_' to distinguish from general model params\n",
    "    ctm_iterations: int = 5  # Original 'iterations'\n",
    "    ctm_d_model: int = 512   # Original 'd_model' for CTM's internal latent space\n",
    "    ctm_input_dim: int = 256 # Dimensionality of inputs to CTM (e.g., from byte embeddings or other features)\n",
    "                             # This was 'd_input' in OriginalCTMCore if it took external features.\n",
    "                             # If CTM processes outputs of byte_embedding, this might be byte_embedding_dim.\n",
    "    ctm_heads: int = 8       # Attention heads within CTM\n",
    "    ctm_n_synch_out: int = 64\n",
    "    ctm_n_synch_action: int = 64\n",
    "    ctm_synapse_depth: int = 3\n",
    "    ctm_memory_length: int = 10\n",
    "    ctm_deep_nlms: bool = True\n",
    "    ctm_memory_hidden_dims: int = 2048\n",
    "    ctm_do_layernorm_nlm: bool = False\n",
    "    ctm_out_dims: int = 512  # Output dimension of CTM's own projector\n",
    "    ctm_prediction_reshaper: list = field(default_factory=lambda: [-1])\n",
    "    ctm_dropout: float = 0.1\n",
    "    ctm_dropout_nlm: Optional[float] = None\n",
    "    # Neuron selection strategy. Available options:\n",
    "    # Legacy: 'first-last', 'random', 'random-pairing'\n",
    "    # Biologically-inspired: 'bio_hebbian', 'bio_plasticity', 'bio_competitive',\n",
    "    #                        'bio_homeostatic', 'bio_evolutionary', 'bio_stdp',\n",
    "    #                        'bio_criticality', 'bio_multi_objective'\n",
    "    # Hybrid: 'adaptive_random', 'performance_guided', 'task_aware'\n",
    "    ctm_neuron_select_type: str = 'bio_multi_objective'\n",
    "    ctm_n_random_pairing_self: int = 0\n",
    "    \n",
    "    # Diffusion Parameters\n",
    "    diffusion_steps: int = 1000\n",
    "    noise_schedule: str = \"cosine\" # e.g., \"linear\", \"cosine\"\n",
    "    diffusion_beta_start: float = 0.0001\n",
    "    diffusion_beta_end: float = 0.02\n",
    "    diffusion_timesteps: int = 1000 # Number of timesteps for the diffusion process\n",
    "    ctm_diffusion_coupling_strength: float = 0.8 # How CTM influences diffusion\n",
    "    adaptive_scheduling: bool = True  # CTM-adaptive diffusion timestep scheduling\n",
    "    iterative_refinement: bool = True # Iterative CTM-diffusion refinement for sampling\n",
    "    \n",
    "\n",
    "    \n",
    "    # Training Efficiency\n",
    "    mixed_precision: bool = True\n",
    "    gradient_checkpointing: bool = True\n",
    "    sparse_attention: bool = True  # Now implemented with BinarySparseAttention\n",
    "    adaptive_depth: bool = False   # Defaulting to False, can be enabled if implemented\n",
    "    \n",
    "    # Sparse Attention Parameters\n",
    "    sparse_attention_ratio: float = 0.1  # Keep only 10% of attention connections\n",
    "    binary_pattern_size: int = 8  # Size of binary patterns to detect\n",
    "\n",
    "    # Attention Mechanism Type\n",
    "    attention_type: str = \"subquadratic\"  # Options: \"standard\", \"binary_sparse\", \"subquadratic\"\n",
    "    \n",
    "    # Subquadratic Attention Parameters (if attention_type is \"subquadratic\")\n",
    "    subquadratic_attn_epsilon: float = 1e-6\n",
    "    subquadratic_attn_poly_degree: int = 5\n",
    "    attention_qkv_bias: bool = True # General QKV bias for attention mechanisms like Subquadratic or standard MHA\n",
    "    # attn_drop and proj_drop for subquadratic_attn will be mapped from ctm_dropout\n",
    "\n",
    "    # Positional Embedding Parameters\n",
    "    positional_embedding_type: Optional[str] = 'multi-learnable-fourier' # e.g., 'custom-rotational-1d', 'learnable-fourier', multi-learnable-fourier' #Can set the value here. \n",
    "    positional_embedding_dim: Optional[int] = None  # Dimension of the positional embedding, defaults to ctm_input_dim if None\n",
    "    reshape_patch_sequence_to_grid: bool = True # If True, reshape patch sequence to a 2D grid for 2D PEs. Must set to true if using 2D Grid for Positional Embeddings.\n",
    "    patch_grid_width: Optional[int] = None       # Desired width of the patch grid if reshaping\n",
    "\n",
    "    # Pipeline Parallelism Parameters\n",
    "    enable_pipeline_parallelism: bool = True\n",
    "    pipeline_stages: int = 4  # CTM, MCMC, Diffusion prep, Diffusion exec\n",
    "    pipeline_overlap_ratio: float = 0.7  # Target overlap ratio\n",
    "    \n",
    "    # Adaptive Batch Sizing Parameters\n",
    "    enable_adaptive_batching: bool = True\n",
    "    initial_batch_size: int = 32\n",
    "    min_batch_size: int = 8\n",
    "    max_batch_size: int = 256\n",
    "    batch_adaptation_frequency: int = 100\n",
    "    memory_threshold_high: float = 0.85\n",
    "    memory_threshold_low: float = 0.6\n",
    "    \n",
    "    # Smart Data Sampling Parameters\n",
    "    enable_smart_sampling: bool = True\n",
    "    sample_importance_weight: float = 0.6\n",
    "    sample_diversity_weight: float = 0.4\n",
    "    initial_sample_ratio: float = 0.3\n",
    "    complexity_analysis_enabled: bool = True\n",
    "    \n",
    "    # Multi-input/output parameters\n",
    "    num_inputs: int = 1  # Number of input streams\n",
    "    num_outputs: int = 1  # Number of output heads\n",
    "    output_dims: List[int] = field(default_factory=lambda: [64])  # Dimensions for each output head\n",
    "    \n",
    "    # Self-supervised learning\n",
    "    ssl_dim: int = 128  # Dimension for self-supervised projection\n",
    "    ssl_weight: float = 0.1  # Weight for self-supervised loss\n",
    "    ssl_temperature: float = 0.07  # Temperature for contrastive loss\n",
    "    ssl_noise_std: float = 0.1  # Noise standard deviation for contrastive augmentation\n",
    "    \n",
    "    # Spatiotemporal Processing\n",
    "    use_spatial: bool = True  # Enable spatial processing for image/video data\n",
    "    \n",
    "    # WINA Attention\n",
    "    use_wina_attention: bool = True  # Enable WINA sparse attention\n",
    "    \n",
    "    # Multi-task Learning Parameters\n",
    "    max_tasks: int = 50  # Maximum number of tasks for continual learning\n",
    "    # Added to resolve TypeError for unexpected keyword arguments\n",
    "    vocab_size: Optional[int] = None\n",
    "    output_audio_bytes: bool = False\n",
    "    inferred_task_latent_dim: Optional[int] = None # Default to None, __post_init__ handles it\n",
    "    use_hipa_attention: bool = False # Default to False\n",
    "    hipa_num_heads: Optional[int] = None # Default to None\n",
    "    audio_output_dtype_str: Optional[str] = \"float32\" # Default as per __post_init__ logic\n",
    "    unet_input_feature_dim: Optional[int] = None # Default to None, __post_init__ calculates it\n",
    "\n",
    "    # --- JEPA Training Parameters (Integrated with LearnedBytePatcherEncoder) ---\n",
    "    use_jepa_training: bool = False\n",
    "    # jepa_embed_dim will be derived from patch_embedding_dim if dynamic_entropy_patcher is used\n",
    "    jepa_predictor_hidden_dim: int = 512 # Hidden dimension of JEPA predictor MLP\n",
    "    jepa_mask_ratio_min: float = 0.15 # Min proportion of patch sequence to mask for target\n",
    "    jepa_mask_ratio_max: float = 0.75 # Max proportion of patch sequence to mask for target\n",
    "    jepa_context_scale_min: float = 0.3 # Min proportion of patches for context\n",
    "    jepa_context_scale_max: float = 0.7 # Max proportion of patches for context\n",
    "    jepa_momentum_beta: float = 0.996 # Momentum for target encoder update\n",
    "    jepa_loss_weight: float = 0.1 # Weight for the JEPA loss component\n",
    "    jepa_num_target_blocks: int = 1 # Number of target blocks to predict\n",
    "\n",
    "    # --- Knowledge Store Parameters ---\n",
    "\n",
    "    def __post_init__(self):\n",
    "        # Validate output dimensions\n",
    "        if len(self.output_dims) != self.num_outputs:\n",
    "            raise ValueError(f\"output_dims length ({len(self.output_dims)}) must match num_outputs ({self.num_outputs})\")\n",
    "\n",
    "        # Merged content from the second __post_init__\n",
    "        if hasattr(self, 'ctm_prediction_reshaper') and self.ctm_prediction_reshaper == [-1] and self.vocab_size is not None:\n",
    "            pass\n",
    "        if hasattr(self, 'ctm_dropout_nlm') and self.ctm_dropout_nlm is None and hasattr(self, 'ctm_dropout'):\n",
    "            self.ctm_dropout_nlm = self.ctm_dropout\n",
    "        if hasattr(self, 'mcmc_output_space_dim') and self.mcmc_output_space_dim is None and hasattr(self, 'ctm_out_dims'):\n",
    "            self.mcmc_output_space_dim = self.ctm_out_dims\n",
    "        \n",
    "        if hasattr(self, 'ctm_neuron_select_type') and \\\n",
    "           VALID_NEURON_SELECT_TYPES is not None and self.ctm_neuron_select_type not in VALID_NEURON_SELECT_TYPES:\n",
    "            print(f\"Warning: ctm_neuron_select_type '{self.ctm_neuron_select_type}' is not in VALID_NEURON_SELECT_TYPES ({VALID_NEURON_SELECT_TYPES}).\")\n",
    "\n",
    "        if hasattr(self, 'positional_embedding_type') and self.positional_embedding_type is not None:\n",
    "            if VALID_POSITIONAL_EMBEDDING_TYPES is None: # Fallback if import failed\n",
    "                print(f\"Warning: VALID_POSITIONAL_EMBEDDING_TYPES not available for validation.\")\n",
    "            elif self.positional_embedding_type not in VALID_POSITIONAL_EMBEDDING_TYPES:\n",
    "                print(f\"Warning: positional_embedding_type '{self.positional_embedding_type}' is not in VALID_POSITIONAL_EMBEDDING_TYPES ({VALID_POSITIONAL_EMBEDDING_TYPES}).\")\n",
    "            if self.positional_embedding_dim is not None and self.positional_embedding_dim <= 0:\n",
    "                raise ValueError(\"positional_embedding_dim must be positive if set.\")\n",
    "            \n",
    "            if self.reshape_patch_sequence_to_grid:\n",
    "                if self.patch_grid_width is None or self.patch_grid_width <= 0:\n",
    "                    raise ValueError(\"patch_grid_width must be a positive integer if reshape_patch_sequence_to_grid is True.\")\n",
    "                if self.positional_embedding_type not in ['learnable-fourier', 'multi-learnable-fourier', 'custom-rotational']:\n",
    "                    print(f\"Warning: reshape_patch_sequence_to_grid is True, but positional_embedding_type ('{self.positional_embedding_type}') is not a typical 2D PE. Ensure compatibility.\")\n",
    "\n",
    "        # Validations for new patch encoder\n",
    "        if self.use_dynamic_entropy_patcher:\n",
    "            if self.patch_embedding_dim <= 0:\n",
    "                raise ValueError(\"patch_embedding_dim must be positive if use_dynamic_entropy_patcher is True.\")\n",
    "            if self.entropy_patcher_min_patch_size <= 0:\n",
    "                raise ValueError(\"entropy_patcher_min_patch_size must be positive.\")\n",
    "            if self.entropy_patcher_max_patch_size < self.entropy_patcher_min_patch_size:\n",
    "                raise ValueError(\"entropy_patcher_max_patch_size must be >= entropy_patcher_min_patch_size.\")\n",
    "            if self.entropy_patcher_threshold_type not in [\"global\", \"relative_monotonic\"]:\n",
    "                raise ValueError(\"entropy_patcher_threshold_type must be 'global' or 'relative_monotonic'.\")\n",
    "        elif self.multi_granularity and self.multi_granularity_output_dim <= 0:\n",
    "            print(\"Warning: multi_granularity_output_dim might not be correctly set for validation if not using a patcher and MGP is active.\")\n",
    "        \n",
    "        if not hasattr(self, 'inferred_task_latent_dim') or self.inferred_task_latent_dim is None:\n",
    "            print(\"Warning: inferred_task_latent_dim not found or is None in config, defaulting to 64.\")\n",
    "            self.inferred_task_latent_dim = 512\n",
    "        elif self.inferred_task_latent_dim <= 0: # This check is now safe\n",
    "            raise ValueError(\"inferred_task_latent_dim must be positive.\")\n",
    " \n",
    "        if hasattr(self, 'use_hipa_attention') and self.use_hipa_attention and \\\n",
    "            (not hasattr(self, 'hipa_num_heads') or self.hipa_num_heads <= 0):\n",
    "             raise ValueError(\"hipa_num_heads must be positive if use_hipa_attention is True.\")\n",
    " \n",
    "        if hasattr(self, 'audio_output_dtype_str'):\n",
    "            if self.audio_output_dtype_str == \"float32\":\n",
    "                self.audio_output_item_size = 4\n",
    "            elif self.audio_output_dtype_str == \"int16\":\n",
    "                self.audio_output_item_size = 2\n",
    "            else:\n",
    "                if hasattr(self, 'output_audio_bytes') and self.output_audio_bytes:\n",
    "                    raise ValueError(f\"Unsupported audio_output_dtype_str: {self.audio_output_dtype_str} when output_audio_bytes is True.\")\n",
    "                else:\n",
    "                    self.audio_output_item_size = 4\n",
    "        elif hasattr(self, 'output_audio_bytes') and self.output_audio_bytes:\n",
    "            if not hasattr(self, 'audio_output_dtype_str') or self.audio_output_dtype_str is None:\n",
    "                raise ValueError(\"audio_output_dtype_str must be defined in config if output_audio_bytes is True.\")\n",
    "        else:\n",
    "            self.audio_output_item_size = 4\n",
    "\n",
    "        # Calculate unet_input_feature_dim if not set\n",
    "        if self.unet_input_feature_dim is None:\n",
    "            if self.max_sequence_length <= 0 or self.audio_output_item_size <= 0:\n",
    "                raise ValueError(\"max_sequence_length and audio_output_item_size must be positive to calculate unet_input_feature_dim.\")\n",
    "            self.unet_input_feature_dim = self.max_sequence_length // self.audio_output_item_size\n",
    "            if self.unet_input_feature_dim <= 0:\n",
    "                raise ValueError(f\"Calculated unet_input_feature_dim ({self.unet_input_feature_dim}) must be positive. Check max_sequence_length and audio_output_item_size.\")\n",
    "        elif self.unet_input_feature_dim <= 0:\n",
    "            raise ValueError(\"unet_input_feature_dim, if set, must be positive.\")\n",
    "\n",
    "        if self.use_jepa_training:\n",
    "            if not (0 < self.jepa_mask_ratio_min < 1 and 0 < self.jepa_mask_ratio_max < 1 and self.jepa_mask_ratio_min <= self.jepa_mask_ratio_max):\n",
    "                raise ValueError(\"JEPA mask ratios must be between 0 and 1, with min <= max.\")\n",
    "            if not (0 < self.jepa_context_scale_min < 1 and 0 < self.jepa_context_scale_max < 1 and self.jepa_context_scale_min <= self.jepa_context_scale_max):\n",
    "                raise ValueError(\"JEPA context scales must be between 0 and 1, with min <= max.\")\n",
    "            if not (0 <= self.jepa_momentum_beta < 1):\n",
    "                raise ValueError(\"jepa_momentum_beta must be between 0 and 1.\")\n",
    "            if self.jepa_num_target_blocks <= 0:\n",
    "                raise ValueError(\"jepa_num_target_blocks must be positive.\")\n",
    "            if not self.use_dynamic_entropy_patcher:\n",
    "                print(\"Warning: JEPA training is enabled but use_dynamic_entropy_patcher is False. JEPA relies on the patch embeddings from LearnedBytePatcherEncoder.\")\n",
    "\n",
    "# Define EnhancedCTMConfig for ARC with EnhancedCTMDiffusion\n",
    "config_arc_diffusion = EnhancedCTMConfig(\n",
    "   \n",
    "from contineous_thought_machines.models.ctm_Diffusion_NEWNEW import EnhancedCTMConfig\n",
    "\n",
    "# --- Model Configuration ---\n",
    "config_arc_diffusion = EnhancedCTMConfig(\n",
    "    d_model=512,\n",
    "    n_heads=8,\n",
    "    n_layers=24,\n",
    "    max_sequence_length=MAX_SEQUENCE_LENGTH,\n",
    "    dropout=0.1,\n",
    "    use_dynamic_entropy_patcher=True,\n",
    "    patch_embedding_dim=256,\n",
    "    patch_grid_width=16,\n",
    "    patch_encoder_cnn_channels=64,\n",
    "    entropy_patcher_threshold_type=\"global\",\n",
    "    entropy_patcher_global_threshold=0.75,\n",
    "    entropy_patcher_relative_threshold=0.1,\n",
    "    entropy_patcher_min_patch_size=4,\n",
    "    entropy_patcher_max_patch_size=128,\n",
    "    entropy_model_byte_vocab_size=256,\n",
    "    entropy_model_embedding_dim=64,\n",
    "    entropy_model_hidden_dim=128,\n",
    "    entropy_model_num_layers=1,\n",
    "    entropy_model_dropout=0.1,\n",
    "    entropy_model_loss_weight=0.1,\n",
    "    ctm_input_dim=256,\n",
    "    ctm_d_model=512,\n",
    "    ctm_iterations=5,\n",
    "    ctm_heads=8,\n",
    "    ctm_out_dims=512,\n",
    "    ctm_neuron_select_type='bio_multi_objective',\n",
    "    attention_type=\"subquadratic\",\n",
    "    subquadratic_attn_epsilon=1e-6,\n",
    "    subquadratic_attn_poly_degree=5,\n",
    "    attention_qkv_bias=True,\n",
    "    positional_embedding_type='multi-learnable-fourier',\n",
    "    positional_embedding_dim=None,\n",
    "    reshape_patch_sequence_to_grid=True,\n",
    "    enable_pipeline_parallelism=True,\n",
    "    pipeline_stages=4,\n",
    "    pipeline_overlap_ratio=0.7,\n",
    "    enable_adaptive_batching=True,\n",
    "    initial_batch_size=32,\n",
    "    min_batch_size=8,\n",
    "    max_batch_size=256,\n",
    "    batch_adaptation_frequency=100,\n",
    "    memory_threshold_high=0.85,\n",
    "    memory_threshold_low=0.6,\n",
    "    enable_smart_sampling=True,\n",
    "    sample_importance_weight=0.6,\n",
    "    sample_diversity_weight=0.4,\n",
    "    initial_sample_ratio=0.3,\n",
    "    complexity_analysis_enabled=True,\n",
    "    num_inputs=1,\n",
    "    num_outputs=1,\n",
    "    output_dims=[64],\n",
    "    ssl_dim=128,\n",
    "    ssl_weight=0.1,\n",
    "    ssl_temperature=0.07,\n",
    "    ssl_noise_std=0.1,\n",
    "    use_spatial=True,\n",
    "    use_wina_attention=True,\n",
    "    max_tasks=50,\n",
    "    diffusion_steps=1000,\n",
    "    ctm_diffusion_coupling_strength=0.8,\n",
    "    vocab_size=None,\n",
    "    output_audio_bytes=False,\n",
    "    unet_input_feature_dim=8192,\n",
    "\n",
    "     # --- Global Plasticity Loss Parameters ---\n",
    "    use_global_plasticity_loss= True,\n",
    "    global_plasticity_loss_weight = 0.05,\n",
    "    local_neuron_selector_loss_weight = 0.1,\n",
    "    target_hebbian_pattern = 0.0 # Target for the aggregated Hebbian signal\n",
    ")\n",
    "print(\"✓ EnhancedCTMConfig for ARC (config_arc_diffusion) created.\")\n",
    "\n",
    "if 'enhanced_ctm_mcmc' not in globals():\n",
    "    print(\"Warning: 'enhanced_ctm_mcmc' not found in globals. Defaulting to None. Ensure the cell defining it (approx. lines 1820-1866) was run successfully.\")\n",
    "    enhanced_ctm_mcmc = None\n",
    "    \n",
    "if 'EnhancedCTMDiffusion' in globals() and EnhancedCTMDiffusion is not None:\n",
    "    ctm_model_arc = EnhancedCTMDiffusion(config=config_arc_diffusion).to(device)\n",
    "    print(\"✓ EnhancedCTMDiffusion model for ARC (ctm_model_arc) initialized.\")\n",
    "\n",
    "    # The external ARC output head will take features from the CTM core part of EnhancedCTMDiffusion\n",
    "    arc_output_head_input_dim = config_arc_diffusion.ctm_out_dims\n",
    "    arc_output_head = nn.Linear(arc_output_head_input_dim, ARC_OUTPUT_HEAD_DIM).to(device)\n",
    "    print(f\"✓ ARC Output Head initialized (input_dim: {arc_output_head_input_dim}, output_dim: {ARC_OUTPUT_HEAD_DIM}).\")\n",
    "\n",
    "    # Handle external MCMC integration if enabled\n",
    "    if ENABLE_CTM_MCMC_INTEGRATION_FOR_ARC and enhanced_ctm_mcmc:\n",
    "        # Ensure the external MCMC module's input_dim matches the new CTM's output\n",
    "        if enhanced_ctm_mcmc.thought_network[0].in_features != config_arc_diffusion.ctm_out_dims:\n",
    "            print(f\"Re-initializing external enhanced_ctm_mcmc for new input_dim {config_arc_diffusion.ctm_out_dims}\")\n",
    "            enhanced_ctm_mcmc = EnhancedCTMFenchelYoungIntegration(\n",
    "                input_dim=config_arc_diffusion.ctm_out_dims, # Use output dim of CTM core\n",
    "                output_space=arc_grid_output_space,\n",
    "                mcmc_config=MCMC_CONFIG_ARC,\n",
    "                use_large_neighborhood_search=True,\n",
    "                lns_frequency=5,\n",
    "                lns_neighborhood_size=10\n",
    "            )\n",
    "        ctm_mcmc_integration_arc = enhanced_ctm_mcmc.to(device) if enhanced_ctm_mcmc else None\n",
    "        print(f\"✓ External MCMC Integration for ARC is {'enabled' if ctm_mcmc_integration_arc else 'FAILED to enable'}.\")\n",
    "    \n",
    "    arc_trainable_params = list(ctm_model_arc.parameters()) # EnhancedCTMDiffusion parameters\n",
    "    if arc_output_head: arc_trainable_params.extend(list(arc_output_head.parameters()))\n",
    "    if ctm_mcmc_integration_arc:\n",
    "        arc_trainable_params.extend(list(ctm_mcmc_integration_arc.parameters()))\n",
    "\n",
    "    optimizer_arc = optim.AdamW([p for p in arc_trainable_params if p.requires_grad], lr=LEARNING_RATE, weight_decay=1e-4)\n",
    "    \n",
    "    if ACCELERATE_AVAILABLE:\n",
    "        accelerator_arc = Accelerator()\n",
    "        models_to_prepare = [ctm_model_arc] # Start with the main model\n",
    "        if arc_output_head: models_to_prepare.append(arc_output_head)\n",
    "        if ctm_mcmc_integration_arc: models_to_prepare.append(ctm_mcmc_integration_arc)\n",
    "        \n",
    "        prepared_components = accelerator_arc.prepare(*models_to_prepare, optimizer_arc)\n",
    "        \n",
    "        optimizer_arc = prepared_components[-1] # Last element is the optimizer\n",
    "        prepared_models_tuple = prepared_components[:-1] # All other elements are models\n",
    "\n",
    "        ctm_model_arc = prepared_models_tuple[0]\n",
    "        model_idx = 1\n",
    "        if arc_output_head:\n",
    "            arc_output_head = prepared_models_tuple[model_idx]\n",
    "            model_idx +=1\n",
    "        if ctm_mcmc_integration_arc:\n",
    "            ctm_mcmc_integration_arc = prepared_models_tuple[model_idx]\n",
    "        print(\"✓ ARC models (EnhancedCTMDiffusion) and optimizer prepared with Accelerate.\")\n",
    "else:\n",
    "    print(\"⚠️ EnhancedCTMDiffusion model or its config for ARC-AGI-2 could not be initialized. Check imports.\")\n",
    "\n",
    "CHECKPOINT_DIR_ARC = os.path.join(CHECKPOINT_DIR, \"ctm_arc_agi_2_enhanced_diffusion\") # New checkpoint dir\n",
    "os.makedirs(CHECKPOINT_DIR_ARC, exist_ok=True)\n",
    "print(f\"ARC Checkpoints will be saved to: {CHECKPOINT_DIR_ARC}\")\n",
    "\n",
    "NUM_EPOCHS_ARC = 20\n",
    "ARC_BATCH_SIZE = 8\n",
    "\n",
    "arc_train_dataset = NewCustomARCGridDataset(ARC_TRAIN_DIR)\n",
    "arc_eval_dataset = NewCustomARCGridDataset(ARC_EVAL_DIR)\n",
    "\n",
    "arc_train_loader, arc_eval_loader = None, None\n",
    "if arc_train_dataset and len(arc_train_dataset) > 0:\n",
    "    arc_train_loader = DataLoader(\n",
    "        arc_train_dataset, batch_size=ARC_BATCH_SIZE, shuffle=True,\n",
    "        collate_fn=collate_fn_new_custom_arc, **OPTIMIZED_DATALOADER_CONFIG\n",
    "    )\n",
    "    if accelerator_arc: arc_train_loader = accelerator_arc.prepare(arc_train_loader)\n",
    "    print(f\"✓ ARC Training DataLoader initialized with {len(arc_train_dataset)} tasks.\")\n",
    "else:\n",
    "    print(\"⚠️ ARC Training DataLoader could not be initialized.\")\n",
    "\n",
    "if arc_eval_dataset and len(arc_eval_dataset) > 0:\n",
    "    arc_eval_loader = DataLoader(\n",
    "        arc_eval_dataset, batch_size=1, shuffle=False,\n",
    "        collate_fn=collate_fn_new_custom_arc, **OPTIMIZED_DATALOADER_CONFIG\n",
    "    )\n",
    "    if accelerator_arc: arc_eval_loader = accelerator_arc.prepare(arc_eval_loader)\n",
    "    print(f\"✓ ARC Evaluation DataLoader initialized with {len(arc_eval_dataset)} tasks.\")\n",
    "else:\n",
    "    print(\"⚠️ ARC Evaluation DataLoader could not be initialized.\")\n",
    "\n",
    "arc_criterion = nn.CrossEntropyLoss(ignore_index=PADDING_VALUE)\n",
    "print(\"\\n✓ ARC-AGI-2 Setup Complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d887b26b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "🚀 STARTING PHASE 4: ARC-AGI-2 Training\n",
      "   Epochs: 20, Batch Size: 8, Task ID: 3\n",
      "   Device: cuda\n",
      "============================================================\n",
      "\n",
      "Reshaped patch sequence to grid (128x16) and applied 2D PE. New sequence length: 2048\n",
      "Reshaped patch sequence to grid (128x16) and applied 2D PE. New sequence length: 2048\n",
      "Reshaped patch sequence to grid (128x16) and applied 2D PE. New sequence length: 2048\n",
      "Reshaped patch sequence to grid (128x16) and applied 2D PE. New sequence length: 2048\n",
      "Reshaped patch sequence to grid (128x16) and applied 2D PE. New sequence length: 2048\n",
      "Reshaped patch sequence to grid (128x16) and applied 2D PE. New sequence length: 2048\n",
      "Reshaped patch sequence to grid (128x16) and applied 2D PE. New sequence length: 2048\n",
      "Reshaped patch sequence to grid (128x16) and applied 2D PE. New sequence length: 2048\n",
      "Reshaped patch sequence to grid (128x16) and applied 2D PE. New sequence length: 2048\n",
      "Reshaped patch sequence to grid (128x16) and applied 2D PE. New sequence length: 2048\n",
      "Reshaped patch sequence to grid (128x16) and applied 2D PE. New sequence length: 2048\n",
      "Reshaped patch sequence to grid (128x16) and applied 2D PE. New sequence length: 2048\n",
      "Reshaped patch sequence to grid (128x16) and applied 2D PE. New sequence length: 2048\n",
      "Reshaped patch sequence to grid (128x16) and applied 2D PE. New sequence length: 2048\n",
      "Reshaped patch sequence to grid (128x16) and applied 2D PE. New sequence length: 2048\n",
      "Reshaped patch sequence to grid (128x16) and applied 2D PE. New sequence length: 2048\n",
      "Reshaped patch sequence to grid (128x16) and applied 2D PE. New sequence length: 2048\n",
      "Reshaped patch sequence to grid (128x16) and applied 2D PE. New sequence length: 2048\n",
      "Reshaped patch sequence to grid (128x16) and applied 2D PE. New sequence length: 2048\n",
      "Reshaped patch sequence to grid (128x16) and applied 2D PE. New sequence length: 2048\n",
      "Reshaped patch sequence to grid (128x16) and applied 2D PE. New sequence length: 2048\n",
      "Reshaped patch sequence to grid (128x16) and applied 2D PE. New sequence length: 2048\n",
      "Reshaped patch sequence to grid (128x16) and applied 2D PE. New sequence length: 2048\n",
      "Reshaped patch sequence to grid (128x16) and applied 2D PE. New sequence length: 2048\n",
      "Reshaped patch sequence to grid (128x16) and applied 2D PE. New sequence length: 2048\n",
      "Reshaped patch sequence to grid (128x16) and applied 2D PE. New sequence length: 2048\n",
      "Reshaped patch sequence to grid (128x16) and applied 2D PE. New sequence length: 2048\n",
      "Reshaped patch sequence to grid (128x16) and applied 2D PE. New sequence length: 2048\n",
      "Reshaped patch sequence to grid (128x16) and applied 2D PE. New sequence length: 2048\n",
      "Reshaped patch sequence to grid (128x16) and applied 2D PE. New sequence length: 2048\n",
      "Reshaped patch sequence to grid (128x16) and applied 2D PE. New sequence length: 2048\n",
      "Reshaped patch sequence to grid (128x16) and applied 2D PE. New sequence length: 2048\n",
      "Reshaped patch sequence to grid (128x16) and applied 2D PE. New sequence length: 2048\n",
      "Reshaped patch sequence to grid (128x16) and applied 2D PE. New sequence length: 2048\n",
      "Reshaped patch sequence to grid (128x16) and applied 2D PE. New sequence length: 2048\n",
      "Reshaped patch sequence to grid (128x16) and applied 2D PE. New sequence length: 2048\n",
      "Reshaped patch sequence to grid (128x16) and applied 2D PE. New sequence length: 2048\n",
      "Reshaped patch sequence to grid (128x16) and applied 2D PE. New sequence length: 2048\n",
      "Reshaped patch sequence to grid (128x16) and applied 2D PE. New sequence length: 2048\n",
      "Reshaped patch sequence to grid (128x16) and applied 2D PE. New sequence length: 2048\n",
      "Reshaped patch sequence to grid (128x16) and applied 2D PE. New sequence length: 2048\n",
      "Reshaped patch sequence to grid (128x16) and applied 2D PE. New sequence length: 2048\n",
      "Reshaped patch sequence to grid (128x16) and applied 2D PE. New sequence length: 2048\n",
      "Reshaped patch sequence to grid (128x16) and applied 2D PE. New sequence length: 2048\n",
      "Reshaped patch sequence to grid (128x16) and applied 2D PE. New sequence length: 2048\n",
      "Reshaped patch sequence to grid (128x16) and applied 2D PE. New sequence length: 2048\n",
      "Reshaped patch sequence to grid (128x16) and applied 2D PE. New sequence length: 2048\n",
      "Reshaped patch sequence to grid (128x16) and applied 2D PE. New sequence length: 2048\n",
      "Reshaped patch sequence to grid (128x16) and applied 2D PE. New sequence length: 2048\n",
      "Reshaped patch sequence to grid (128x16) and applied 2D PE. New sequence length: 2048\n",
      "  Epoch [1/20], Batch [50/125], Loss: 2531.8359\n",
      "Reshaped patch sequence to grid (128x16) and applied 2D PE. New sequence length: 2048\n",
      "Reshaped patch sequence to grid (128x16) and applied 2D PE. New sequence length: 2048\n",
      "Reshaped patch sequence to grid (128x16) and applied 2D PE. New sequence length: 2048\n",
      "Reshaped patch sequence to grid (128x16) and applied 2D PE. New sequence length: 2048\n",
      "Reshaped patch sequence to grid (128x16) and applied 2D PE. New sequence length: 2048\n",
      "Reshaped patch sequence to grid (128x16) and applied 2D PE. New sequence length: 2048\n",
      "Reshaped patch sequence to grid (128x16) and applied 2D PE. New sequence length: 2048\n",
      "Reshaped patch sequence to grid (128x16) and applied 2D PE. New sequence length: 2048\n",
      "Reshaped patch sequence to grid (128x16) and applied 2D PE. New sequence length: 2048\n",
      "Reshaped patch sequence to grid (128x16) and applied 2D PE. New sequence length: 2048\n",
      "Reshaped patch sequence to grid (128x16) and applied 2D PE. New sequence length: 2048\n",
      "Reshaped patch sequence to grid (128x16) and applied 2D PE. New sequence length: 2048\n",
      "Reshaped patch sequence to grid (128x16) and applied 2D PE. New sequence length: 2048\n",
      "Reshaped patch sequence to grid (128x16) and applied 2D PE. New sequence length: 2048\n",
      "Reshaped patch sequence to grid (128x16) and applied 2D PE. New sequence length: 2048\n",
      "Reshaped patch sequence to grid (128x16) and applied 2D PE. New sequence length: 2048\n",
      "Reshaped patch sequence to grid (128x16) and applied 2D PE. New sequence length: 2048\n",
      "Reshaped patch sequence to grid (128x16) and applied 2D PE. New sequence length: 2048\n",
      "Reshaped patch sequence to grid (128x16) and applied 2D PE. New sequence length: 2048\n",
      "Reshaped patch sequence to grid (128x16) and applied 2D PE. New sequence length: 2048\n",
      "Reshaped patch sequence to grid (128x16) and applied 2D PE. New sequence length: 2048\n",
      "Reshaped patch sequence to grid (128x16) and applied 2D PE. New sequence length: 2048\n",
      "Reshaped patch sequence to grid (128x16) and applied 2D PE. New sequence length: 2048\n",
      "Reshaped patch sequence to grid (128x16) and applied 2D PE. New sequence length: 2048\n",
      "Reshaped patch sequence to grid (128x16) and applied 2D PE. New sequence length: 2048\n",
      "Reshaped patch sequence to grid (128x16) and applied 2D PE. New sequence length: 2048\n",
      "Reshaped patch sequence to grid (128x16) and applied 2D PE. New sequence length: 2048\n",
      "Reshaped patch sequence to grid (128x16) and applied 2D PE. New sequence length: 2048\n",
      "Reshaped patch sequence to grid (128x16) and applied 2D PE. New sequence length: 2048\n",
      "Reshaped patch sequence to grid (128x16) and applied 2D PE. New sequence length: 2048\n",
      "Reshaped patch sequence to grid (128x16) and applied 2D PE. New sequence length: 2048\n",
      "Reshaped patch sequence to grid (128x16) and applied 2D PE. New sequence length: 2048\n",
      "Reshaped patch sequence to grid (128x16) and applied 2D PE. New sequence length: 2048\n",
      "Reshaped patch sequence to grid (128x16) and applied 2D PE. New sequence length: 2048\n",
      "Reshaped patch sequence to grid (128x16) and applied 2D PE. New sequence length: 2048\n",
      "Reshaped patch sequence to grid (128x16) and applied 2D PE. New sequence length: 2048\n",
      "Reshaped patch sequence to grid (128x16) and applied 2D PE. New sequence length: 2048\n",
      "Reshaped patch sequence to grid (128x16) and applied 2D PE. New sequence length: 2048\n",
      "Reshaped patch sequence to grid (128x16) and applied 2D PE. New sequence length: 2048\n",
      "Reshaped patch sequence to grid (128x16) and applied 2D PE. New sequence length: 2048\n",
      "Reshaped patch sequence to grid (128x16) and applied 2D PE. New sequence length: 2048\n",
      "Reshaped patch sequence to grid (128x16) and applied 2D PE. New sequence length: 2048\n",
      "Reshaped patch sequence to grid (128x16) and applied 2D PE. New sequence length: 2048\n",
      "Reshaped patch sequence to grid (128x16) and applied 2D PE. New sequence length: 2048\n",
      "Reshaped patch sequence to grid (128x16) and applied 2D PE. New sequence length: 2048\n",
      "Reshaped patch sequence to grid (128x16) and applied 2D PE. New sequence length: 2048\n",
      "Reshaped patch sequence to grid (128x16) and applied 2D PE. New sequence length: 2048\n",
      "Reshaped patch sequence to grid (128x16) and applied 2D PE. New sequence length: 2048\n",
      "Reshaped patch sequence to grid (128x16) and applied 2D PE. New sequence length: 2048\n",
      "Reshaped patch sequence to grid (128x16) and applied 2D PE. New sequence length: 2048\n",
      "  Epoch [1/20], Batch [100/125], Loss: 2136.4333\n",
      "Reshaped patch sequence to grid (128x16) and applied 2D PE. New sequence length: 2048\n",
      "Reshaped patch sequence to grid (128x16) and applied 2D PE. New sequence length: 2048\n",
      "Reshaped patch sequence to grid (128x16) and applied 2D PE. New sequence length: 2048\n",
      "Reshaped patch sequence to grid (128x16) and applied 2D PE. New sequence length: 2048\n",
      "Reshaped patch sequence to grid (128x16) and applied 2D PE. New sequence length: 2048\n",
      "Reshaped patch sequence to grid (128x16) and applied 2D PE. New sequence length: 2048\n",
      "Reshaped patch sequence to grid (128x16) and applied 2D PE. New sequence length: 2048\n",
      "Reshaped patch sequence to grid (128x16) and applied 2D PE. New sequence length: 2048\n",
      "Reshaped patch sequence to grid (128x16) and applied 2D PE. New sequence length: 2048\n",
      "Reshaped patch sequence to grid (128x16) and applied 2D PE. New sequence length: 2048\n",
      "Reshaped patch sequence to grid (128x16) and applied 2D PE. New sequence length: 2048\n",
      "Reshaped patch sequence to grid (128x16) and applied 2D PE. New sequence length: 2048\n",
      "Reshaped patch sequence to grid (128x16) and applied 2D PE. New sequence length: 2048\n",
      "Reshaped patch sequence to grid (128x16) and applied 2D PE. New sequence length: 2048\n",
      "Reshaped patch sequence to grid (128x16) and applied 2D PE. New sequence length: 2048\n",
      "Reshaped patch sequence to grid (128x16) and applied 2D PE. New sequence length: 2048\n",
      "Reshaped patch sequence to grid (128x16) and applied 2D PE. New sequence length: 2048\n",
      "Reshaped patch sequence to grid (128x16) and applied 2D PE. New sequence length: 2048\n",
      "Reshaped patch sequence to grid (128x16) and applied 2D PE. New sequence length: 2048\n",
      "Reshaped patch sequence to grid (128x16) and applied 2D PE. New sequence length: 2048\n",
      "Reshaped patch sequence to grid (128x16) and applied 2D PE. New sequence length: 2048\n",
      "Reshaped patch sequence to grid (128x16) and applied 2D PE. New sequence length: 2048\n",
      "Reshaped patch sequence to grid (128x16) and applied 2D PE. New sequence length: 2048\n",
      "Reshaped patch sequence to grid (128x16) and applied 2D PE. New sequence length: 2048\n",
      "Reshaped patch sequence to grid (128x16) and applied 2D PE. New sequence length: 2048\n",
      "Epoch [1/20] completed. Average Loss: 2319.5913\n",
      "[DEBUG] Rank 0 out of 1 total ranks\n",
      "[2025-06-17 16:32:58,249] [INFO] [real_accelerator.py:254:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/bin/ld: cannot find -laio: No such file or directory\n",
      "collect2: error: ld returned 1 exit status\n",
      "/usr/bin/ld: cannot find -laio: No such file or directory\n",
      "collect2: error: ld returned 1 exit status\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-06-17 16:32:59,538] [INFO] [logging.py:107:log_dist] [Rank -1] [TorchCheckpointEngine] Initialized with serialization = False\n",
      "✓ Checkpoint saved for epoch 1 on rank 0 to checkpoints/ctm_arc_agi_2_enhanced_diffusion\n",
      "Reshaped patch sequence to grid (128x16) and applied 2D PE. New sequence length: 2048\n",
      "Reshaped patch sequence to grid (128x16) and applied 2D PE. New sequence length: 2048\n",
      "Reshaped patch sequence to grid (128x16) and applied 2D PE. New sequence length: 2048\n",
      "Reshaped patch sequence to grid (128x16) and applied 2D PE. New sequence length: 2048\n",
      "Reshaped patch sequence to grid (128x16) and applied 2D PE. New sequence length: 2048\n",
      "Reshaped patch sequence to grid (128x16) and applied 2D PE. New sequence length: 2048\n",
      "Reshaped patch sequence to grid (128x16) and applied 2D PE. New sequence length: 2048\n",
      "Reshaped patch sequence to grid (128x16) and applied 2D PE. New sequence length: 2048\n",
      "Reshaped patch sequence to grid (128x16) and applied 2D PE. New sequence length: 2048\n",
      "Reshaped patch sequence to grid (128x16) and applied 2D PE. New sequence length: 2048\n",
      "Reshaped patch sequence to grid (128x16) and applied 2D PE. New sequence length: 2048\n",
      "Reshaped patch sequence to grid (128x16) and applied 2D PE. New sequence length: 2048\n",
      "Reshaped patch sequence to grid (128x16) and applied 2D PE. New sequence length: 2048\n",
      "Reshaped patch sequence to grid (128x16) and applied 2D PE. New sequence length: 2048\n",
      "Reshaped patch sequence to grid (128x16) and applied 2D PE. New sequence length: 2048\n",
      "Reshaped patch sequence to grid (128x16) and applied 2D PE. New sequence length: 2048\n",
      "Reshaped patch sequence to grid (128x16) and applied 2D PE. New sequence length: 2048\n",
      "Reshaped patch sequence to grid (128x16) and applied 2D PE. New sequence length: 2048\n",
      "Reshaped patch sequence to grid (128x16) and applied 2D PE. New sequence length: 2048\n",
      "Reshaped patch sequence to grid (128x16) and applied 2D PE. New sequence length: 2048\n",
      "Reshaped patch sequence to grid (128x16) and applied 2D PE. New sequence length: 2048\n",
      "Reshaped patch sequence to grid (128x16) and applied 2D PE. New sequence length: 2048\n",
      "Reshaped patch sequence to grid (128x16) and applied 2D PE. New sequence length: 2048\n",
      "Reshaped patch sequence to grid (128x16) and applied 2D PE. New sequence length: 2048\n",
      "Reshaped patch sequence to grid (128x16) and applied 2D PE. New sequence length: 2048\n",
      "Reshaped patch sequence to grid (128x16) and applied 2D PE. New sequence length: 2048\n",
      "Reshaped patch sequence to grid (128x16) and applied 2D PE. New sequence length: 2048\n",
      "Reshaped patch sequence to grid (128x16) and applied 2D PE. New sequence length: 2048\n",
      "Reshaped patch sequence to grid (128x16) and applied 2D PE. New sequence length: 2048\n",
      "Reshaped patch sequence to grid (128x16) and applied 2D PE. New sequence length: 2048\n",
      "Reshaped patch sequence to grid (128x16) and applied 2D PE. New sequence length: 2048\n",
      "Reshaped patch sequence to grid (128x16) and applied 2D PE. New sequence length: 2048\n",
      "Reshaped patch sequence to grid (128x16) and applied 2D PE. New sequence length: 2048\n",
      "Reshaped patch sequence to grid (128x16) and applied 2D PE. New sequence length: 2048\n",
      "Reshaped patch sequence to grid (128x16) and applied 2D PE. New sequence length: 2048\n",
      "Reshaped patch sequence to grid (128x16) and applied 2D PE. New sequence length: 2048\n",
      "Reshaped patch sequence to grid (128x16) and applied 2D PE. New sequence length: 2048\n",
      "Reshaped patch sequence to grid (128x16) and applied 2D PE. New sequence length: 2048\n",
      "Reshaped patch sequence to grid (128x16) and applied 2D PE. New sequence length: 2048\n",
      "Reshaped patch sequence to grid (128x16) and applied 2D PE. New sequence length: 2048\n",
      "Reshaped patch sequence to grid (128x16) and applied 2D PE. New sequence length: 2048\n",
      "Reshaped patch sequence to grid (128x16) and applied 2D PE. New sequence length: 2048\n",
      "Reshaped patch sequence to grid (128x16) and applied 2D PE. New sequence length: 2048\n",
      "Reshaped patch sequence to grid (128x16) and applied 2D PE. New sequence length: 2048\n",
      "Reshaped patch sequence to grid (128x16) and applied 2D PE. New sequence length: 2048\n",
      "Reshaped patch sequence to grid (128x16) and applied 2D PE. New sequence length: 2048\n",
      "Reshaped patch sequence to grid (128x16) and applied 2D PE. New sequence length: 2048\n",
      "Reshaped patch sequence to grid (128x16) and applied 2D PE. New sequence length: 2048\n",
      "Reshaped patch sequence to grid (128x16) and applied 2D PE. New sequence length: 2048\n",
      "Reshaped patch sequence to grid (128x16) and applied 2D PE. New sequence length: 2048\n",
      "  Epoch [2/20], Batch [50/125], Loss: 696.9127\n",
      "Reshaped patch sequence to grid (128x16) and applied 2D PE. New sequence length: 2048\n",
      "Reshaped patch sequence to grid (128x16) and applied 2D PE. New sequence length: 2048\n",
      "Reshaped patch sequence to grid (128x16) and applied 2D PE. New sequence length: 2048\n",
      "Reshaped patch sequence to grid (128x16) and applied 2D PE. New sequence length: 2048\n",
      "Reshaped patch sequence to grid (128x16) and applied 2D PE. New sequence length: 2048\n",
      "Reshaped patch sequence to grid (128x16) and applied 2D PE. New sequence length: 2048\n",
      "Reshaped patch sequence to grid (128x16) and applied 2D PE. New sequence length: 2048\n",
      "Reshaped patch sequence to grid (128x16) and applied 2D PE. New sequence length: 2048\n",
      "Reshaped patch sequence to grid (128x16) and applied 2D PE. New sequence length: 2048\n",
      "Reshaped patch sequence to grid (128x16) and applied 2D PE. New sequence length: 2048\n",
      "Reshaped patch sequence to grid (128x16) and applied 2D PE. New sequence length: 2048\n",
      "Reshaped patch sequence to grid (128x16) and applied 2D PE. New sequence length: 2048\n",
      "Reshaped patch sequence to grid (128x16) and applied 2D PE. New sequence length: 2048\n",
      "Reshaped patch sequence to grid (128x16) and applied 2D PE. New sequence length: 2048\n",
      "Reshaped patch sequence to grid (128x16) and applied 2D PE. New sequence length: 2048\n",
      "Reshaped patch sequence to grid (128x16) and applied 2D PE. New sequence length: 2048\n",
      "Reshaped patch sequence to grid (128x16) and applied 2D PE. New sequence length: 2048\n",
      "Reshaped patch sequence to grid (128x16) and applied 2D PE. New sequence length: 2048\n",
      "Reshaped patch sequence to grid (128x16) and applied 2D PE. New sequence length: 2048\n",
      "Reshaped patch sequence to grid (128x16) and applied 2D PE. New sequence length: 2048\n",
      "Reshaped patch sequence to grid (128x16) and applied 2D PE. New sequence length: 2048\n",
      "Reshaped patch sequence to grid (128x16) and applied 2D PE. New sequence length: 2048\n",
      "Reshaped patch sequence to grid (128x16) and applied 2D PE. New sequence length: 2048\n",
      "Reshaped patch sequence to grid (128x16) and applied 2D PE. New sequence length: 2048\n",
      "Reshaped patch sequence to grid (128x16) and applied 2D PE. New sequence length: 2048\n",
      "Reshaped patch sequence to grid (128x16) and applied 2D PE. New sequence length: 2048\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[15], line 37\u001b[0m\n\u001b[1;32m     32\u001b[0m optimizer_arc\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m     34\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m autocast(enabled\u001b[38;5;241m=\u001b[39mUSE_MIXED_PRECISION, dtype\u001b[38;5;241m=\u001b[39mautocast_dtype) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m accelerator_arc \u001b[38;5;28;01melse\u001b[39;00m accelerator_arc\u001b[38;5;241m.\u001b[39mautocast():\n\u001b[1;32m     35\u001b[0m     \u001b[38;5;66;03m# Forward pass through EnhancedCTMDiffusion\u001b[39;00m\n\u001b[1;32m     36\u001b[0m     \u001b[38;5;66;03m# The model internally handles patching, CTM core, diffusion (if target provided), and entropy aux loss.\u001b[39;00m\n\u001b[0;32m---> 37\u001b[0m     model_output_dict \u001b[38;5;241m=\u001b[39m \u001b[43mctm_model_arc\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     38\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbyte_sequence\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_bytes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     39\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtarget_diffusion_output\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtarget_bytes_for_diffusion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;66;43;03m# Provide target for diffusion loss component\u001b[39;49;00m\n\u001b[1;32m     40\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mctm_controlled_diffusion\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;66;43;03m# Ensure diffusion part is active for loss calculation\u001b[39;49;00m\n\u001b[1;32m     41\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtimestep\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrandint\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig_arc_diffusion\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdiffusion_steps\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mcurrent_batch_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_bytes\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlong\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;66;43;03m# Random timesteps for diffusion training\u001b[39;49;00m\n\u001b[1;32m     42\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtarget_mcmc_output\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;66;43;03m# Internal MCMC is disabled in config_arc_diffusion\u001b[39;49;00m\n\u001b[1;32m     43\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtask_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mARC_AGI_2\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;66;43;03m# Optional task name\u001b[39;49;00m\n\u001b[1;32m     44\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcurrent_epoch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mepoch\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;66;43;03m# Pass current epoch\u001b[39;49;00m\n\u001b[1;32m     45\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     47\u001b[0m     \u001b[38;5;66;03m# Loss from EnhancedCTMDiffusion (includes entropy aux loss, diffusion loss, etc.)\u001b[39;00m\n\u001b[1;32m     48\u001b[0m     enhanced_ctm_loss \u001b[38;5;241m=\u001b[39m model_output_dict\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtotal_loss\u001b[39m\u001b[38;5;124m'\u001b[39m, torch\u001b[38;5;241m.\u001b[39mtensor(\u001b[38;5;241m0.0\u001b[39m, device\u001b[38;5;241m=\u001b[39minput_bytes\u001b[38;5;241m.\u001b[39mdevice))\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1751\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1749\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1751\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1762\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1757\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1758\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1759\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1760\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1761\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1762\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1764\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1765\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m/workspace/Arc-AGI-2/contineous-thought-machines/models/ctm_Diffusion_NEWNEW.py:4428\u001b[0m, in \u001b[0;36mEnhancedCTMDiffusion.forward\u001b[0;34m(self, byte_sequence, target_diffusion_output, mode, timestep, target_mcmc_output, current_epoch, current_batch, task_name)\u001b[0m\n\u001b[1;32m   4420\u001b[0m losses[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mjepa_loss\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor(\u001b[38;5;241m0.0\u001b[39m, device\u001b[38;5;241m=\u001b[39mdevice)\n\u001b[1;32m   4421\u001b[0m \u001b[38;5;66;03m# The aux loss from dynamic_entropy_patcher (online JEPA encoder) is handled by _prepare_input_features\u001b[39;00m\n\u001b[1;32m   4422\u001b[0m \u001b[38;5;66;03m# No separate jepa_context_aux_loss or jepa_target_aux_loss needed here for now.\u001b[39;00m\n\u001b[1;32m   4423\u001b[0m \n\u001b[1;32m   4424\u001b[0m \u001b[38;5;66;03m# Prepare input features using the online encoder (dynamic_entropy_patcher)\u001b[39;00m\n\u001b[1;32m   4425\u001b[0m \u001b[38;5;66;03m# This also gives us the online_patch_embeddings (kv_features_for_ctm) and their original byte indices.\u001b[39;00m\n\u001b[1;32m   4426\u001b[0m \u001b[38;5;66;03m# kv_features_for_ctm, current_inferred_latent, current_hipa_signal, entropy_aux_loss are computed once\u001b[39;00m\n\u001b[1;32m   4427\u001b[0m kv_features_for_ctm, current_inferred_latent, current_hipa_signal, entropy_aux_loss \u001b[38;5;241m=\u001b[39m \\\n\u001b[0;32m-> 4428\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_prepare_input_features\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbyte_sequence\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   4430\u001b[0m losses[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mentropy_model_aux_loss\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m entropy_aux_loss \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mentropy_model_loss_weight\n\u001b[1;32m   4431\u001b[0m online_patch_embeddings \u001b[38;5;241m=\u001b[39m kv_features_for_ctm \u001b[38;5;66;03m# Shape: (B, S_patches, D_embed)\u001b[39;00m\n",
      "File \u001b[0;32m/workspace/Arc-AGI-2/contineous-thought-machines/models/ctm_Diffusion_NEWNEW.py:4050\u001b[0m, in \u001b[0;36mEnhancedCTMDiffusion._prepare_input_features\u001b[0;34m(self, byte_sequence)\u001b[0m\n\u001b[1;32m   4046\u001b[0m patch_indices \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   4048\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdynamic_entropy_patcher:\n\u001b[1;32m   4049\u001b[0m     \u001b[38;5;66;03m# LearnedBytePatcherEncoder (when it's the dynamic one) now returns (encoded_patches, patch_indices, aux_loss)\u001b[39;00m\n\u001b[0;32m-> 4050\u001b[0m     raw_features, patch_indices, current_entropy_aux_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdynamic_entropy_patcher\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbyte_sequence\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   4051\u001b[0m     entropy_aux_loss \u001b[38;5;241m=\u001b[39m current_entropy_aux_loss\n\u001b[1;32m   4052\u001b[0m     \u001b[38;5;66;03m# raw_features shape: (batch_size, num_dynamic_patches, embedding_dim)\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1751\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1749\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1751\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1762\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1757\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1758\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1759\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1760\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1761\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1762\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1764\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1765\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m/workspace/Arc-AGI-2/contineous-thought-machines/models/ctm_Diffusion_NEWNEW.py:1809\u001b[0m, in \u001b[0;36mDynamicEntropyPatcher.forward\u001b[0;34m(self, byte_sequence)\u001b[0m\n\u001b[1;32m   1806\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1807\u001b[0m     patch_end \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmin\u001b[39m(current_start \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmax_patch_size \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m, seq_len \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m) \u001b[38;5;66;03m# No boundary, take max size\u001b[39;00m\n\u001b[0;32m-> 1809\u001b[0m patch_end \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mmax\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mpatch_end\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcurrent_start\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;66;03m# Ensure patch has at least one byte\u001b[39;00m\n\u001b[1;32m   1811\u001b[0m all_patches_data\u001b[38;5;241m.\u001b[39mappend(byte_sequence[i, current_start : patch_end \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m])\n\u001b[1;32m   1812\u001b[0m patch_indices_for_sample\u001b[38;5;241m.\u001b[39mappend((current_start, patch_end))\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# --- ARC-AGI-2 Meta-Learning Training Loop ---\n",
    "import os\n",
    "import torch\n",
    "import torch.distributed as dist\n",
    "from safetensors.torch import save_file\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import glob\n",
    "import json\n",
    "\n",
    "CUDA_LAUNCH_BLOCKING=1 #Diagnose cuda errors. \n",
    "# --- FIX: Define NUM_ARC_SYMBOLS globally for DataLoader workers ---\n",
    "# The standard ARC task has 10 symbols (0-9).\n",
    "NUM_ARC_SYMBOLS = 10\n",
    "import numpy as np\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(f\"🚀 STARTING PHASE 4: ARC-AGI-2 Meta-Learning Training\")\n",
    "print(f\"   Epochs: {NUM_EPOCHS_ARC}, Batch Size: {ARC_BATCH_SIZE}, Task ID: {ARC_TASK_ID}\")\n",
    "print(f\"   Device: {device if not accelerator_arc else accelerator_arc.device}\")\n",
    "print(\"=\"*60 + \"\\n\")\n",
    "\n",
    "# --- Context: 2D Grid Padding (from original code) ---\n",
    "def pad_grid(grid_list, max_dims, pad_value):\n",
    "    \"\"\"Pads a 2D grid to specified maximum dimensions.\"\"\"\n",
    "    grid_np = np.array(grid_list, dtype=np.int32)\n",
    "    padded_grid = np.full(max_dims, pad_value, dtype=np.int32)\n",
    "    h, w = grid_np.shape\n",
    "    padded_grid[:h, :w] = grid_np\n",
    "    return padded_grid\n",
    "\n",
    "# --- Fix: Byte Sequence Padding for the Model --- #\n",
    "MAX_SEQUENCE_LENGTH = 8192\n",
    "PADDING_BYTE_VALUE = 0\n",
    "\n",
    "def serialize_and_pad_grid(grid, max_len=MAX_SEQUENCE_LENGTH, pad_value=PADDING_BYTE_VALUE):\n",
    "    \"\"\"\n",
    "    Serializes a grid into a byte sequence and pads it to a fixed length.\n",
    "    \"\"\"\n",
    "    flat_array = np.array(grid, dtype=np.uint8).flatten()\n",
    "    byte_sequence = flat_array.tobytes()\n",
    "    padding_len = max_len - len(byte_sequence)\n",
    "\n",
    "    if padding_len < 0:\n",
    "        padded_sequence = byte_sequence[:max_len]\n",
    "    else:\n",
    "        padding = bytes([pad_value] * padding_len)\n",
    "        padded_sequence = byte_sequence + padding\n",
    "        \n",
    "    return padded_sequence\n",
    "\n",
    "class NewCustomARCGridDataset(Dataset):\n",
    "    def __init__(self, data_dir, max_grid_size=MAX_GRID_SIZE, padding_value=PADDING_VALUE):\n",
    "        self.data_dir = data_dir\n",
    "        self.task_files = glob.glob(os.path.join(data_dir, \"*.json\"))\n",
    "        self.max_grid_size = max_grid_size\n",
    "        self.padding_value = padding_value\n",
    "        self.tasks = []\n",
    "        print(f\"NewCustomARCGridDataset: Looking for tasks in: {data_dir}\")\n",
    "        if not self.task_files:\n",
    "            print(f\"NewCustomARCGridDataset Warning: No JSON files found in {data_dir}. Dataset will be empty.\")\n",
    "        for task_file in self.task_files:\n",
    "            try:\n",
    "                with open(task_file, 'r') as f:\n",
    "                    self.tasks.append(json.load(f))\n",
    "            except Exception as e:\n",
    "                print(f\"NewCustomARCGridDataset Warning: Could not load or parse {task_file}: {e}\")\n",
    "        if not self.tasks:\n",
    "            print(f\"NewCustomARCGridDataset Warning: No tasks successfully loaded from {data_dir}.\")\n",
    "        else:\n",
    "            print(f\"NewCustomARCGridDataset: Loaded {len(self.tasks)} ARC tasks from {data_dir}.\")\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.tasks)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        task_data = self.tasks[idx]\n",
    "        processed_task = {'train': [], 'test': [], 'id': os.path.basename(self.task_files[idx]) if idx < len(self.task_files) else 'unknown_task'}\n",
    "\n",
    "        for pair_type in ['train', 'test']:\n",
    "            for item in task_data.get(pair_type, []):\n",
    "                input_grid_list = item.get('input', [])\n",
    "                output_grid_list = item.get('output', [])\n",
    "                \n",
    "                original_input_dims = (len(input_grid_list), len(input_grid_list[0]) if input_grid_list and input_grid_list[0] else (0,0))\n",
    "                original_output_dims = (len(output_grid_list), len(output_grid_list[0]) if output_grid_list and output_grid_list[0] else (0,0))\n",
    "\n",
    "                padded_input_np = pad_grid(input_grid_list, self.max_grid_size, self.padding_value)\n",
    "                padded_output_np = pad_grid(output_grid_list, self.max_grid_size, self.padding_value)\n",
    "                \n",
    "                processed_task[pair_type].append({\n",
    "                    'input': torch.from_numpy(padded_input_np).long(),\n",
    "                    'output': torch.from_numpy(padded_output_np).long(),\n",
    "                    'original_input_dims': original_input_dims,\n",
    "                    'original_output_dims': original_output_dims\n",
    "                })\n",
    "        return processed_task\n",
    "\n",
    "def collate_fn_new_custom_arc(batch_of_tasks):\n",
    "    input_byte_sequences_list = []\n",
    "    target_byte_sequences_for_diffusion_list = []\n",
    "    original_target_grids_for_ce_loss_list = []\n",
    "\n",
    "    for task in batch_of_tasks:\n",
    "        if not isinstance(task, dict):\n",
    "            continue\n",
    "\n",
    "        for train_pair in task.get('train', []):\n",
    "            if not isinstance(train_pair, dict) or 'input' not in train_pair or 'output' not in train_pair:\n",
    "                continue\n",
    "\n",
    "            input_grid_np = train_pair['input'].numpy()\n",
    "            target_grid_np = train_pair['output'].numpy()\n",
    "\n",
    "            input_bytes = serialize_and_pad_grid(input_grid_np, max_len=MAX_SEQUENCE_LENGTH, pad_value=PADDING_BYTE_VALUE)\n",
    "            input_byte_sequences_list.append(torch.tensor(list(input_bytes), dtype=torch.uint8))\n",
    "\n",
    "            target_bytes_for_diffusion = serialize_and_pad_grid(target_grid_np, max_len=MAX_SEQUENCE_LENGTH, pad_value=PADDING_BYTE_VALUE)\n",
    "            target_byte_sequences_for_diffusion_list.append(torch.tensor(list(target_bytes_for_diffusion), dtype=torch.uint8))\n",
    "\n",
    "            original_target_grids_for_ce_loss_list.append(train_pair['output'].view(-1))\n",
    "            \n",
    "    if not input_byte_sequences_list:\n",
    "        return {\n",
    "            'input_byte_sequences': torch.empty(0, MAX_SEQUENCE_LENGTH, dtype=torch.uint8),\n",
    "            'target_byte_sequences_for_diffusion': torch.empty(0, MAX_SEQUENCE_LENGTH, dtype=torch.uint8),\n",
    "            'original_target_grids_for_ce_loss': torch.empty(0, ARC_INPUT_FLAT_DIM, dtype=torch.long),\n",
    "        }\n",
    "\n",
    "    final_input_byte_sequences = torch.stack(input_byte_sequences_list)\n",
    "    final_target_byte_sequences_for_diffusion = torch.stack(target_byte_sequences_for_diffusion_list)\n",
    "    final_original_target_grids_for_ce_loss = torch.stack(original_target_grids_for_ce_loss_list)\n",
    "    \n",
    "    # --- Fix for potential out-of-bounds padding values ---\n",
    "    # The CrossEntropyLoss criterion expects class indices to be in [0, C-1].\n",
    "    # If the padding value is negative or >= C, it can cause a CUDA 'device-side assert' error.\n",
    "    # We defensively clamp the target tensor to the valid range [0, NUM_ARC_SYMBOLS - 1].\n",
    "    final_original_target_grids_for_ce_loss.clamp_(min=0, max=NUM_ARC_SYMBOLS - 1)\n",
    "    \n",
    "    return {\n",
    "        'input_byte_sequences': final_input_byte_sequences,\n",
    "        'target_byte_sequences_for_diffusion': final_target_byte_sequences_for_diffusion,\n",
    "        'original_target_grids_for_ce_loss': final_original_target_grids_for_ce_loss,\n",
    "    }\n",
    "\n",
    "arc_train_dataset = NewCustomARCGridDataset(ARC_TRAIN_DIR)\n",
    "arc_eval_dataset = NewCustomARCGridDataset(ARC_EVAL_DIR)\n",
    "\n",
    "arc_train_loader, arc_eval_loader = None, None\n",
    "if arc_train_dataset and len(arc_train_dataset) > 0:\n",
    "    arc_train_loader = DataLoader(\n",
    "        arc_train_dataset, batch_size=ARC_BATCH_SIZE, shuffle=True,\n",
    "        collate_fn=collate_fn_new_custom_arc, **OPTIMIZED_DATALOADER_CONFIG\n",
    "    )\n",
    "    if accelerator_arc: arc_train_loader = accelerator_arc.prepare(arc_train_loader)\n",
    "    print(f\"✓ ARC Training DataLoader initialized with {len(arc_train_dataset)} tasks.\")\n",
    "else:\n",
    "    print(\"⚠️ ARC Training DataLoader could not be initialized.\")\n",
    "\n",
    "if arc_eval_dataset and len(arc_eval_dataset) > 0:\n",
    "    arc_eval_loader = DataLoader(\n",
    "        arc_eval_dataset, batch_size=1, shuffle=False,\n",
    "        collate_fn=collate_fn_new_custom_arc, **OPTIMIZED_DATALOADER_CONFIG\n",
    "    )\n",
    "    if accelerator_arc: arc_eval_loader = accelerator_arc.prepare(arc_eval_loader)\n",
    "    print(f\"✓ ARC Evaluation DataLoader initialized with {len(arc_eval_dataset)} tasks.\")\n",
    "else:\n",
    "    print(\"⚠️ ARC Evaluation DataLoader could not be initialized.\")\n",
    "\n",
    "\n",
    "# === DEBUG + RANK CHECK ===\n",
    "def get_rank_debug():\n",
    "    if dist.is_available() and dist.is_initialized():\n",
    "        rank = dist.get_rank()\n",
    "        world_size = dist.get_world_size()\n",
    "    else:\n",
    "        rank = 0\n",
    "        world_size = 1\n",
    "\n",
    "    print(f\"[DEBUG] Rank {rank} out of {world_size} total ranks\")\n",
    "    return rank, world_size\n",
    "\n",
    "# --- MCMC Plasticity Loss Normalization Factor ---\n",
    "MCMC_LOSS_GAMMA = 0.01\n",
    "\n",
    "if not all([ctm_model_arc, arc_output_head, optimizer_arc, arc_train_loader, arc_criterion]):\n",
    "    print(\"⚠️ Skipping ARC-AGI-2 training due to missing components.\")\n",
    "else:\n",
    "    for epoch in range(NUM_EPOCHS_ARC):\n",
    "        ctm_model_arc.train()\n",
    "        arc_output_head.train()\n",
    "        if ctm_mcmc_integration_arc: ctm_mcmc_integration_arc.train()\n",
    "\n",
    "        total_arc_loss = 0\n",
    "        processed_batches = 0\n",
    "\n",
    "        for batch_idx, batch_data in enumerate(arc_train_loader):\n",
    "            if not batch_data or batch_data['input_byte_sequences'].numel() == 0:\n",
    "                print(f\"Skipping empty batch {batch_idx}\")\n",
    "                continue\n",
    "\n",
    "            # Get data from the updated collate_fn\n",
    "            input_bytes = batch_data['input_byte_sequences'].to(device if not accelerator_arc else accelerator_arc.device)\n",
    "            target_bytes_for_diffusion = batch_data['target_byte_sequences_for_diffusion'].to(device if not accelerator_arc else accelerator_arc.device)\n",
    "            original_target_grids_for_ce = batch_data['original_target_grids_for_ce_loss'].to(device if not accelerator_arc else accelerator_arc.device)\n",
    "\n",
    "            current_batch_size = input_bytes.size(0)\n",
    "\n",
    "            optimizer_arc.zero_grad()\n",
    "\n",
    "            with autocast(enabled=USE_MIXED_PRECISION, dtype=autocast_dtype) if not accelerator_arc else accelerator_arc.autocast():\n",
    "                # Forward pass through EnhancedCTMDiffusion\n",
    "                model_output_dict = ctm_model_arc(\n",
    "                    byte_sequence=input_bytes,\n",
    "                    target_diffusion_output=target_bytes_for_diffusion,\n",
    "                    mode='ctm_controlled_diffusion',\n",
    "                    timestep=torch.randint(0, config_arc_diffusion.diffusion_steps, (current_batch_size,), device=input_bytes.device).long(),\n",
    "                    target_mcmc_output=None,\n",
    "                    task_name=\"ARC_AGI_2\",\n",
    "                    current_epoch=epoch\n",
    "                )\n",
    "\n",
    "                # The 'total_loss' from the model output already includes the predictive coding loss,\n",
    "                # so we use it directly. This resolves the NameError for PC_LOSS_WEIGHT.\n",
    "                diffusion_loss = model_output_dict.get('diffusion_loss', torch.tensor(0.0, device=input_bytes.device))\n",
    "\n",
    "                # Initialize total loss for the optimizer with the diffusion loss\n",
    "                total_loss = diffusion_loss\n",
    "                \n",
    "                # --- Get CTM core output for auxiliary heads ---\n",
    "                ctm_backbone_output = None\n",
    "                if model_output_dict and 'predictions' in model_output_dict:\n",
    "                    ctm_backbone_output = model_output_dict['predictions'][:, :, -1]\n",
    "                elif model_output_dict and 'final_sync_out' in model_output_dict:\n",
    "                    ctm_backbone_output = model_output_dict['final_sync_out']\n",
    "                else:\n",
    "                    print(\"Warning: CTM core output not found. Using zeros for auxiliary head inputs.\")\n",
    "                    ctm_backbone_output = torch.zeros(current_batch_size, config_arc_diffusion.ctm_out_dims, device=input_bytes.device)\n",
    "                \n",
    "                # --- Calculate and add auxiliary losses to the total_loss for the optimizer ---\n",
    "                ce_loss = torch.tensor(0.0, device=input_bytes.device)\n",
    "                if arc_output_head and ctm_backbone_output is not None:\n",
    "                    if ctm_backbone_output.ndim > 2:\n",
    "                        ctm_features_for_head = ctm_backbone_output.mean(dim=1)\n",
    "                    else:\n",
    "                        ctm_features_for_head = ctm_backbone_output\n",
    "                    \n",
    "                    predicted_logits = arc_output_head(torch.tanh(ctm_features_for_head))\n",
    "                    predicted_logits_reshaped = predicted_logits.view(current_batch_size * ARC_INPUT_FLAT_DIM, NUM_ARC_SYMBOLS)\n",
    "                    target_grids_reshaped = original_target_grids_for_ce.view(current_batch_size * ARC_INPUT_FLAT_DIM)\n",
    "                    ce_loss = arc_criterion(predicted_logits_reshaped, target_grids_reshaped)\n",
    "                    total_loss = total_loss + ce_loss\n",
    "\n",
    "                mcmc_loss_val = torch.tensor(0.0, device=input_bytes.device)\n",
    "                norm_mcmc_loss_for_plasticity = torch.tensor(0.0, device=input_bytes.device)\n",
    "                if ctm_mcmc_integration_arc and ctm_backbone_output is not None:\n",
    "                    target_grids_for_mcmc = (original_target_grids_for_ce > 0).float()\n",
    "                    # Apply y-normalization for MCMC target\n",
    "                    y_mean = target_grids_for_mcmc.mean()\n",
    "                    y_std = target_grids_for_mcmc.std()\n",
    "                    normalized_target_y = (target_grids_for_mcmc - y_mean) / (y_std + 1e-8)\n",
    "\n",
    "                    mcmc_input_features = ctm_backbone_output.detach()\n",
    "                    if mcmc_input_features.ndim > 2:\n",
    "                        mcmc_input_features = mcmc_input_features.mean(dim=1)\n",
    "\n",
    "                    mcmc_loss_val, _, _ = ctm_mcmc_integration_arc(x=mcmc_input_features, target_y=normalized_target_y)\n",
    "                    total_loss = total_loss + mcmc_loss_val\n",
    "                    \n",
    "                    # Per goal instructions, create a normalized MCMC loss for the plasticity update\n",
    "                    norm_mcmc_loss_for_plasticity = MCMC_LOSS_GAMMA * torch.log(1 + mcmc_loss_val.detach())\n",
    "\n",
    "            # --- NaN Check and Loss Debugging ---\n",
    "            if torch.isnan(total_loss) or torch.isinf(total_loss):\n",
    "                print(f\"[NaN or Inf Loss Detected] at Epoch {epoch+1}, Batch {batch_idx+1}. Skipping backward pass.\")\n",
    "                print(f\"  - Diffusion Loss: {diffusion_loss.item() if not torch.isnan(diffusion_loss) else 'NaN'}\")\n",
    "                print(f\"  - CE Loss: {ce_loss.item() if not torch.isnan(ce_loss) else 'NaN'}\")\n",
    "                print(f\"  - MCMC Loss: {mcmc_loss_val.item() if not torch.isnan(mcmc_loss_val) else 'NaN'}\")\n",
    "                continue # Skip to the next batch\n",
    "\n",
    "            # --- DEBUGGING: Print all loss components ---\n",
    "            print(f\"[Losses] Diff: {diffusion_loss.item():.4f}, CE: {ce_loss.item():.4f}, MCMC: {mcmc_loss_val.item():.4f}, Total: {total_loss.item():.4f}\")\n",
    "\n",
    "            if scaler:\n",
    "                scaler.scale(total_loss).backward()\n",
    "                if (batch_idx + 1) % GRADIENT_ACCUMULATION_STEPS == 0:\n",
    "                    scaler.unscale_(optimizer_arc)\n",
    "                    torch.nn.utils.clip_grad_norm_(ctm_model_arc.parameters(), MAX_GRAD_NORM)\n",
    "                    # --- Activity-Dependent Plasticity uses ONLY diffusion_loss ---\n",
    "                    unwrapped_model = ctm_model_arc\n",
    "                    # Pass a modified loss to plasticity to ensure the learning signal can be positive.\n",
    "                    # By subtracting the large CE loss, we create a metric that is negative when\n",
    "                    # the model performs well, which in turn creates a positive learning signal.\n",
    "                    unwrapped_model.ctm_core.apply_activity_plasticity(diffusion_loss, ce_loss, norm_mcmc_loss_for_plasticity)\n",
    "                    scaler.step(optimizer_arc)\n",
    "                    scaler.update()\n",
    "                    optimizer_arc.zero_grad(set_to_none=True)\n",
    "            elif accelerator_arc:\n",
    "                 accelerator_arc.backward(total_loss)\n",
    "                 if (batch_idx + 1) % GRADIENT_ACCUMULATION_STEPS == 0:\n",
    "                    # --- Activity-Dependent Plasticity uses ONLY diffusion_loss ---\n",
    "                    unwrapped_model = accelerator_arc.unwrap_model(ctm_model_arc)\n",
    "                    unwrapped_model.ctm_core.apply_activity_plasticity(diffusion_loss, ce_loss, norm_mcmc_loss_for_plasticity)\n",
    "                    optimizer_arc.step()\n",
    "                    optimizer_arc.zero_grad()\n",
    "            else:\n",
    "                total_loss.backward()\n",
    "                if (batch_idx + 1) % GRADIENT_ACCUMULATION_STEPS == 0:\n",
    "                    torch.nn.utils.clip_grad_norm_(ctm_model_arc.parameters(), MAX_GRAD_NORM)\n",
    "                    # --- Activity-Dependent Plasticity uses ONLY diffusion_loss ---\n",
    "                    ctm_model_arc.ctm_core.apply_activity_plasticity(diffusion_loss, ce_loss, norm_mcmc_loss_for_plasticity)\n",
    "                    optimizer_arc.step()\n",
    "                    optimizer_arc.zero_grad()\n",
    "            \n",
    "            total_arc_loss += total_loss.item()\n",
    "            processed_batches += 1\n",
    "\n",
    "            if (batch_idx + 1) % 50 == 0:\n",
    "                print(f\"  Epoch [{epoch+1}/{NUM_EPOCHS_ARC}], Batch [{batch_idx+1}/{len(arc_train_loader)}], Loss: {total_loss.item():.4f}\")\n",
    "        \n",
    "        avg_epoch_loss = total_arc_loss / processed_batches if processed_batches > 0 else 0\n",
    "        print(f\"Epoch [{epoch+1}/{NUM_EPOCHS_ARC}] completed. Average Loss: {avg_epoch_loss:.4f}\")\n",
    "\n",
    "        # === SAVE ONLY ON RANK 0 ===\n",
    "        rank, world_size = get_rank_debug()\n",
    "        if rank == 0 and CHECKPOINT_DIR_ARC:\n",
    "            model_to_save_ctm = accelerator_arc.unwrap_model(ctm_model_arc) if accelerator_arc else ctm_model_arc\n",
    "            model_to_save_head = accelerator_arc.unwrap_model(arc_output_head) if accelerator_arc else arc_output_head\n",
    "\n",
    "            # Check if DeepSpeed is used and the model is wrapped\n",
    "            if hasattr(model_to_save_ctm, 'zero_optimization') and hasattr(model_to_save_ctm, 'module'):\n",
    "                print(\"✓ Using DeepSpeed consolidated state_dict for CTM model\")\n",
    "                ctm_state_dict = model_to_save_ctm._zero3_consolidated_16bit_state_dict()\n",
    "            else:\n",
    "                ctm_state_dict = model_to_save_ctm.state_dict()\n",
    "\n",
    "            if hasattr(model_to_save_head, 'zero_optimization') and hasattr(model_to_save_head, 'module'):\n",
    "                print(\"✓ Using DeepSpeed consolidated state_dict for ARC head\")\n",
    "                head_state_dict = model_to_save_head._zero3_consolidated_16bit_state_dict()\n",
    "            else:\n",
    "                head_state_dict = model_to_save_head.state_dict()\n",
    "\n",
    "            # Save model weights with safetensors\n",
    "            save_file(ctm_state_dict, os.path.join(CHECKPOINT_DIR_ARC, f\"ctm_model_arc_epoch_{epoch+1}.safetensors\"))\n",
    "            save_file(head_state_dict, os.path.join(CHECKPOINT_DIR_ARC, f\"arc_output_head_epoch_{epoch+1}.safetensors\"))\n",
    "\n",
    "            # Save optimizer (use torch.save, not supported by safetensors)\n",
    "            torch.save(optimizer_arc.state_dict(), os.path.join(CHECKPOINT_DIR_ARC, f\"optimizer_arc_epoch_{epoch+1}.pt\"))\n",
    "\n",
    "            print(f\"✓ Checkpoint saved for epoch {epoch+1} on rank {rank} to {CHECKPOINT_DIR_ARC}\")\n",
    "\n",
    "    print(\"\\n🎉 ARC-AGI-2 Meta-Learning Training Phase Completed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a9ed5119-b7db-4a29-96c0-15d04f546f3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ARC_EVAL_DIR: /workspace/Arc-AGI-2/contineous-thought-machines/examples/contineous-thought-machines/data/evaluation\n",
      "Exists? False\n"
     ]
    }
   ],
   "source": [
    "print(f\"ARC_EVAL_DIR: {ARC_EVAL_DIR}\")\n",
    "print(\"Exists?\", os.path.exists(ARC_EVAL_DIR))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "752a08e7-f218-4c0f-87d7-1b718d508fdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "ARC_EVAL_DIR = \"/workspace/Arc-AGI-2/contineous-thought-machines/data/evaluation\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "61acb935-a668-4957-9f37-f6bc6c7ac4ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ARC_EVAL_DIR: /workspace/Arc-AGI-2/contineous-thought-machines/data/evaluation\n",
      "Exists? True\n"
     ]
    }
   ],
   "source": [
    "print(f\"ARC_EVAL_DIR: {ARC_EVAL_DIR}\")\n",
    "print(\"Exists?\", os.path.exists(ARC_EVAL_DIR))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "228a61ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Setting up module paths ---\n",
      "\n",
      "--- Statically importing EnhancedCTMDiffusion model ---\n",
      " -> Successfully imported EnhancedCTMDiffusion from models package.\n",
      "Warning: inferred_task_latent_dim not found or is None in config, defaulting to 64.\n",
      "✓ EnhancedCTMConfig for ARC (config_arc_diffusion) created.\n",
      "Using enhanced neuron select type: bio_multi_objective\n",
      "Using enhanced neuron select type: bio_multi_objective\n",
      "Using enhanced neuron select type: bio_multi_objective\n",
      "✓ Integration Flow + Task-Aware HiPA sampler initialized\n",
      "  - Model type: VE\n",
      "  - Intelligent modality detection enabled\n",
      "  - HiPA will be applied only to appropriate data types\n",
      "  - Text/discrete tasks protected from frequency corruption\n",
      "✓ EnhancedCTMDiffusion model for ARC (ctm_model_arc) initialized.\n",
      "✓ ARC Output Head initialized (input_dim: 64, output_dim: 9000).\n",
      "✓ External MCMC Integration for ARC is enabled.\n",
      " -> Preparing components with Hugging Face Accelerate...\n",
      "✓ ARC models and optimizer prepared with Accelerate.\n",
      "\n",
      "--- Searching for evaluation and checkpoint directories ---\n",
      "Found 'ctm_arc_agi_2_enhanced_diffusion' directory at: ./checkpoints/ctm_arc_agi_2_enhanced_diffusion\n",
      "\n",
      "--- Initializing Evaluation Dataloader ---\n",
      "  > Using NewCustomARCGridDataset for evaluation from path: /workspace/Arc-AGI-2/contineous-thought-machines/data/evaluation\n",
      "Loaded 120 tasks from /workspace/Arc-AGI-2/contineous-thought-machines/data/evaluation.\n",
      "✓ Evaluation DataLoader initialized with 120 tasks.\n",
      "\n",
      "============================================================\n",
      "🔬 STARTING ARC-AGI-2 Evaluation on device 'cuda'\n",
      "============================================================\n",
      "\n",
      "  > Loading CTM checkpoint from ./checkpoints/ctm_arc_agi_2_enhanced_diffusion/ctm_model_arc_epoch_20.safetensors...\n",
      "✓ Loaded CTM checkpoint from epoch 20.\n",
      "  > Loading ARC Output Head checkpoint from ./checkpoints/ctm_arc_agi_2_enhanced_diffusion/arc_output_head_epoch_20.safetensors...\n",
      "✓ Loaded ARC Output Head checkpoint from epoch 20.\n",
      "❌ Error during ARC-AGI-2 evaluation: expected np.ndarray (got bytes)\n",
      "\n",
      "🔬 ARC-AGI-2 Evaluation Phase Completed.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/tmp/ipykernel_1811/861996040.py\", line 693, in <module>\n",
      "    input_bytes_eval = torch.from_numpy(input_bytes_eval_single).to(torch.uint8).unsqueeze(0).to(device)\n",
      "TypeError: expected np.ndarray (got bytes)\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import numpy as np\n",
    "import glob\n",
    "import json\n",
    "import traceback\n",
    "from dataclasses import dataclass, field\n",
    "from typing import List, Optional\n",
    "\n",
    "# Setup module paths based on user-provided successful import logic\n",
    "print(\"--- Setting up module paths ---\")\n",
    "# Get the absolute path to the project root\n",
    "project_root = '/workspaces/Arc-AGI-2'\n",
    "# Define the path to the 'contineous-thought-machines' directory\n",
    "module_path = os.path.join(project_root, 'contineous-thought-machines')\n",
    "\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)\n",
    "    print(f\"Added to sys.path: {module_path}\")\n",
    "\n",
    "try:\n",
    "    from safetensors.torch import load_file\n",
    "except ImportError:\n",
    "    print(\"Warning: safetensors not found. Loading .safetensors will fail.\")\n",
    "    def load_file(path, device=\"cpu\"):\n",
    "        raise ImportError(f\"safetensors is not installed, cannot load {path}\")\n",
    "\n",
    "import importlib.util\n",
    "\n",
    "# --- Statically Importing EnhancedCTMDiffusion model ---\n",
    "print(\"\\n--- Statically importing EnhancedCTMDiffusion model ---\")\n",
    "EnhancedCTMDiffusion = None\n",
    "try:\n",
    "    from models.ctm_Diffusion_NEWNEW import EnhancedCTMDiffusion\n",
    "    print(\" -> Successfully imported EnhancedCTMDiffusion from models package.\")\n",
    "except ImportError as e_direct:\n",
    "    print(f\"FATAL: Import from models package failed. Last error: {e_direct}\")\n",
    "    EnhancedCTMDiffusion = None # Ensure it's None on failure\n",
    "\n",
    "try:\n",
    "    from accelerate import Accelerator\n",
    "    ACCELERATE_AVAILABLE = True\n",
    "except ImportError:\n",
    "    print(\"Warning: Hugging Face Accelerate not found. Will run on a single device.\")\n",
    "    ACCELERATE_AVAILABLE = False\n",
    "    Accelerator = None\n",
    "\n",
    "# --- Constants and Configuration ---\n",
    "# These are gathered from your setup script to make this file runnable\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "MAX_GRID_SIZE = (30, 30)\n",
    "PADDING_VALUE = -1 # A common padding value for ARC\n",
    "ARC_INPUT_FLAT_DIM = MAX_GRID_SIZE[0] * MAX_GRID_SIZE[1]\n",
    "MAX_SEQUENCE_LENGTH = 8192\n",
    "PADDING_BYTE_VALUE = 0\n",
    "NUM_ARC_SYMBOLS = 10 # 0-9\n",
    "ARC_OUTPUT_HEAD_DIM = ARC_INPUT_FLAT_DIM * NUM_ARC_SYMBOLS\n",
    "LEARNING_RATE = 1e-4\n",
    "\n",
    "# --- Your Provided Setup Code ---\n",
    "\n",
    "# ## Data Handling ##\n",
    "def pad_grid(grid_list, max_dims, pad_value):\n",
    "    grid_np = np.array(grid_list, dtype=np.int32)\n",
    "    padded_grid = np.full(max_dims, pad_value, dtype=np.int32)\n",
    "    h, w = grid_np.shape\n",
    "    padded_grid[:h, :w] = grid_np\n",
    "    return padded_grid\n",
    "\n",
    "def serialize_and_pad_grid(grid, max_len=MAX_SEQUENCE_LENGTH, pad_value=PADDING_BYTE_VALUE):\n",
    "    flat_array = np.array(grid, dtype=np.uint8).flatten()\n",
    "    byte_sequence = flat_array.tobytes()\n",
    "    padding_len = max_len - len(byte_sequence)\n",
    "    if padding_len < 0:\n",
    "        return byte_sequence[:max_len]\n",
    "    return byte_sequence + bytes([pad_value] * padding_len)\n",
    "\n",
    "# Define EnhancedCTMConfig for ARC with EnhancedCTMDiffusion\n",
    "# Assuming EnhancedCTMConfig is a defined class and MAX_SEQUENCE_LENGTH is a defined variable\n",
    "# For example:\n",
    "# from your_model_library import EnhancedCTMConfig\n",
    "# MAX_SEQUENCE_LENGTH = 8192\n",
    "\n",
    "# From contineous-thought-machines/models/constants.py\n",
    "VALID_NEURON_SELECT_TYPES = [\n",
    "    'first-last', 'random', 'random-pairing',  # Legacy\n",
    "    # Biologically-inspired types\n",
    "    'bio_hebbian', 'bio_plasticity', 'bio_competitive', 'bio_homeostatic',\n",
    "    'bio_evolutionary', 'bio_stdp', 'bio_criticality', 'bio_multi_objective',\n",
    "    # Hybrid approaches\n",
    "    'adaptive_random', 'performance_guided', 'task_aware'\n",
    "]\n",
    "\n",
    "VALID_POSITIONAL_EMBEDDING_TYPES = [\n",
    "    'learnable-fourier', 'multi-learnable-fourier',\n",
    "    'custom-rotational', 'custom-rotational-1d'\n",
    "]\n",
    "\n",
    "# From contineous-thought-machines/models/ctm_Diffusion_NEWNEW.py\n",
    "from dataclasses import dataclass, field\n",
    "from typing import Dict, Optional, Tuple, Union, Any, List\n",
    "\n",
    "@dataclass\n",
    "class EnhancedCTMConfig: # Renamed from ContinualLearningConfig for consistency in the target file\n",
    "    \"\"\"Enhanced configuration for continual learning CTM-diffusion model,\n",
    "    incorporating binary processing, multi-task learning, and advanced CTM features.\"\"\"\n",
    "    \n",
    "    # Model architecture (General Transformer/Diffusion settings)\n",
    "    d_model: int = 512  # Main model dimensionality\n",
    "    n_heads: int = 8\n",
    "    n_layers: int = 24\n",
    "    max_sequence_length: int = 8192 # Max input sequence length in terms of bytes or patches\n",
    "    dropout: float = 0.1\n",
    "    \n",
    "    # --- Byte Processing Options ---\n",
    "    patch_embedding_dim: int = 256         # <<< NEW: Output embedding dimension per patch from patcher\n",
    "    patch_encoder_cnn_channels: int = 64   # <<< NEW: Intermediate channels for CNN patch encoder\n",
    "\n",
    "    # --- Dynamic Entropy Patching Options (Inspired by BLT paper) ---\n",
    "    use_dynamic_entropy_patcher: bool = True # Flag to enable dynamic entropy-based patching\n",
    "    entropy_patcher_threshold_type: str = \"global\"  # 'global' or 'relative_monotonic'\n",
    "    entropy_patcher_global_threshold: float = 0.75 # Entropy threshold for 'global' type\n",
    "    entropy_patcher_relative_threshold: float = 0.1 # Entropy diff threshold for 'relative_monotonic'\n",
    "    entropy_patcher_min_patch_size: int = 4      # Minimum number of bytes in a dynamic patch\n",
    "    entropy_patcher_max_patch_size: int = 128    # Maximum number of bytes in a dynamic patch (for CNN encoder)\n",
    "    \n",
    "    # --- Learnable Entropy Model Parameters (for _EntropyProxyModel) ---\n",
    "    entropy_model_byte_vocab_size: int = 256\n",
    "    entropy_model_embedding_dim: int = 64\n",
    "    entropy_model_hidden_dim: int = 128\n",
    "    entropy_model_num_layers: int = 1\n",
    "    entropy_model_dropout: float = 0.1\n",
    "    entropy_model_loss_weight: float = 0.1 # Weight for its auxiliary loss contribution\n",
    "    # Note: These parameters are used if use_dynamic_entropy_patcher is True,\n",
    "    # as LearnedBytePatcherEncoder now instantiates the learnable _EntropyProxyModel.\n",
    "    \n",
    "    # Fallback if not using learned_patch_encoder or dynamic_entropy_patcher\n",
    "    byte_embedding_dim: int = 256\n",
    "    multi_granularity: bool = False # Default to False if patcher is preferred\n",
    "    # multi_granularity_output_dim is complex to predefine, MGP should expose its output dim.\n",
    "    # For now, if multi_granularity is True AND use_learned_patch_encoder is False, this would be used.\n",
    "    multi_granularity_output_dim: int = 256 # Placeholder if MGP is used.\n",
    "    \n",
    "    hierarchical_processing: bool = True # General flag, could apply to patcher or MGP\n",
    "    \n",
    "    # CTM Core Parameters (Specific to the OriginalCTMCore module)\n",
    "    # These are prefixed with 'ctm_' to distinguish from general model params\n",
    "    ctm_iterations: int = 5  # Original 'iterations'\n",
    "    ctm_d_model: int = 512   # Original 'd_model' for CTM's internal latent space\n",
    "    ctm_input_dim: int = 256 # Dimensionality of inputs to CTM (e.g., from byte embeddings or other features)\n",
    "                             # This was 'd_input' in OriginalCTMCore if it took external features.\n",
    "                             # If CTM processes outputs of byte_embedding, this might be byte_embedding_dim.\n",
    "    ctm_heads: int = 8       # Attention heads within CTM\n",
    "    ctm_n_synch_out: int = 64\n",
    "    ctm_n_synch_action: int = 64\n",
    "    ctm_synapse_depth: int = 3\n",
    "    ctm_memory_length: int = 10\n",
    "    ctm_deep_nlms: bool = True\n",
    "    ctm_memory_hidden_dims: int = 2048\n",
    "    ctm_do_layernorm_nlm: bool = False\n",
    "    ctm_out_dims: int = 512  # Output dimension of CTM's own projector\n",
    "    ctm_prediction_reshaper: list = field(default_factory=lambda: [-1])\n",
    "    ctm_dropout: float = 0.1\n",
    "    ctm_dropout_nlm: Optional[float] = None\n",
    "    # Neuron selection strategy. Available options:\n",
    "    # Legacy: 'first-last', 'random', 'random-pairing'\n",
    "    # Biologically-inspired: 'bio_hebbian', 'bio_plasticity', 'bio_competitive',\n",
    "    #                        'bio_homeostatic', 'bio_evolutionary', 'bio_stdp',\n",
    "    #                        'bio_criticality', 'bio_multi_objective'\n",
    "    # Hybrid: 'adaptive_random', 'performance_guided', 'task_aware'\n",
    "    ctm_neuron_select_type: str = 'bio_multi_objective'\n",
    "    ctm_n_random_pairing_self: int = 0\n",
    "    \n",
    "    # Diffusion Parameters\n",
    "    diffusion_steps: int = 1000\n",
    "    noise_schedule: str = \"cosine\" # e.g., \"linear\", \"cosine\"\n",
    "    diffusion_beta_start: float = 0.0001\n",
    "    diffusion_beta_end: float = 0.02\n",
    "    diffusion_timesteps: int = 1000 # Number of timesteps for the diffusion process\n",
    "    ctm_diffusion_coupling_strength: float = 0.8 # How CTM influences diffusion\n",
    "    adaptive_scheduling: bool = True  # CTM-adaptive diffusion timestep scheduling\n",
    "    iterative_refinement: bool = True # Iterative CTM-diffusion refinement for sampling\n",
    "    \n",
    "\n",
    "    \n",
    "    # Training Efficiency\n",
    "    mixed_precision: bool = True\n",
    "    gradient_checkpointing: bool = True\n",
    "    sparse_attention: bool = True  # Now implemented with BinarySparseAttention\n",
    "    adaptive_depth: bool = False   # Defaulting to False, can be enabled if implemented\n",
    "    \n",
    "    # Sparse Attention Parameters\n",
    "    sparse_attention_ratio: float = 0.1  # Keep only 10% of attention connections\n",
    "    binary_pattern_size: int = 8  # Size of binary patterns to detect\n",
    "\n",
    "    # Attention Mechanism Type\n",
    "    attention_type: str = \"subquadratic\"  # Options: \"standard\", \"binary_sparse\", \"subquadratic\"\n",
    "    \n",
    "    # Subquadratic Attention Parameters (if attention_type is \"subquadratic\")\n",
    "    subquadratic_attn_epsilon: float = 1e-6\n",
    "    subquadratic_attn_poly_degree: int = 5\n",
    "    attention_qkv_bias: bool = True # General QKV bias for attention mechanisms like Subquadratic or standard MHA\n",
    "    # attn_drop and proj_drop for subquadratic_attn will be mapped from ctm_dropout\n",
    "\n",
    "    # Positional Embedding Parameters\n",
    "    positional_embedding_type: Optional[str] = 'multi-learnable-fourier' # e.g., 'custom-rotational-1d', 'learnable-fourier', multi-learnable-fourier' #Can set the value here. \n",
    "    positional_embedding_dim: Optional[int] = None  # Dimension of the positional embedding, defaults to ctm_input_dim if None\n",
    "    reshape_patch_sequence_to_grid: bool = True # If True, reshape patch sequence to a 2D grid for 2D PEs. Must set to true if using 2D Grid for Positional Embeddings.\n",
    "    patch_grid_width: Optional[int] = None       # Desired width of the patch grid if reshaping\n",
    "\n",
    "    # Pipeline Parallelism Parameters\n",
    "    enable_pipeline_parallelism: bool = True\n",
    "    pipeline_stages: int = 4  # CTM, MCMC, Diffusion prep, Diffusion exec\n",
    "    pipeline_overlap_ratio: float = 0.7  # Target overlap ratio\n",
    "    \n",
    "    # Adaptive Batch Sizing Parameters\n",
    "    enable_adaptive_batching: bool = True\n",
    "    initial_batch_size: int = 32\n",
    "    min_batch_size: int = 8\n",
    "    max_batch_size: int = 256\n",
    "    batch_adaptation_frequency: int = 100\n",
    "    memory_threshold_high: float = 0.85\n",
    "    memory_threshold_low: float = 0.6\n",
    "    \n",
    "    # Smart Data Sampling Parameters\n",
    "    enable_smart_sampling: bool = True\n",
    "    sample_importance_weight: float = 0.6\n",
    "    sample_diversity_weight: float = 0.4\n",
    "    initial_sample_ratio: float = 0.3\n",
    "    complexity_analysis_enabled: bool = True\n",
    "    \n",
    "    # Multi-input/output parameters\n",
    "    num_inputs: int = 1  # Number of input streams\n",
    "    num_outputs: int = 1  # Number of output heads\n",
    "    output_dims: List[int] = field(default_factory=lambda: [64])  # Dimensions for each output head\n",
    "    \n",
    "    # Self-supervised learning\n",
    "    ssl_dim: int = 128  # Dimension for self-supervised projection\n",
    "    ssl_weight: float = 0.1  # Weight for self-supervised loss\n",
    "    ssl_temperature: float = 0.07  # Temperature for contrastive loss\n",
    "    ssl_noise_std: float = 0.1  # Noise standard deviation for contrastive augmentation\n",
    "    \n",
    "    # Spatiotemporal Processing\n",
    "    use_spatial: bool = True  # Enable spatial processing for image/video data\n",
    "    \n",
    "    # WINA Attention\n",
    "    use_wina_attention: bool = True  # Enable WINA sparse attention\n",
    "    \n",
    "    # Multi-task Learning Parameters\n",
    "    max_tasks: int = 50  # Maximum number of tasks for continual learning\n",
    "    # Added to resolve TypeError for unexpected keyword arguments\n",
    "    vocab_size: Optional[int] = None\n",
    "    output_audio_bytes: bool = False\n",
    "    inferred_task_latent_dim: Optional[int] = None # Default to None, __post_init__ handles it\n",
    "    use_hipa_attention: bool = False # Default to False\n",
    "    hipa_num_heads: Optional[int] = None # Default to None\n",
    "    audio_output_dtype_str: Optional[str] = \"float32\" # Default as per __post_init__ logic\n",
    "    unet_input_feature_dim: Optional[int] = None # Default to None, __post_init__ calculates it\n",
    "\n",
    "    # --- JEPA Training Parameters (Integrated with LearnedBytePatcherEncoder) ---\n",
    "    use_jepa_training: bool = False\n",
    "    # jepa_embed_dim will be derived from patch_embedding_dim if dynamic_entropy_patcher is used\n",
    "    jepa_predictor_hidden_dim: int = 512 # Hidden dimension of JEPA predictor MLP\n",
    "    jepa_mask_ratio_min: float = 0.15 # Min proportion of patch sequence to mask for target\n",
    "    jepa_mask_ratio_max: float = 0.75 # Max proportion of patch sequence to mask for target\n",
    "    jepa_context_scale_min: float = 0.3 # Min proportion of patches for context\n",
    "    jepa_context_scale_max: float = 0.7 # Max proportion of patches for context\n",
    "    jepa_momentum_beta: float = 0.996 # Momentum for target encoder update\n",
    "    jepa_loss_weight: float = 0.1 # Weight for the JEPA loss component\n",
    "    jepa_num_target_blocks: int = 1 # Number of target blocks to predict\n",
    "\n",
    "    # --- Knowledge Store Parameters ---\n",
    "\n",
    "    def __post_init__(self):\n",
    "        # Validate output dimensions\n",
    "        if len(self.output_dims) != self.num_outputs:\n",
    "            raise ValueError(f\"output_dims length ({len(self.output_dims)}) must match num_outputs ({self.num_outputs})\")\n",
    "\n",
    "        # Merged content from the second __post_init__\n",
    "        if hasattr(self, 'ctm_prediction_reshaper') and self.ctm_prediction_reshaper == [-1] and self.vocab_size is not None:\n",
    "            pass\n",
    "        if hasattr(self, 'ctm_dropout_nlm') and self.ctm_dropout_nlm is None and hasattr(self, 'ctm_dropout'):\n",
    "            self.ctm_dropout_nlm = self.ctm_dropout\n",
    "        if hasattr(self, 'mcmc_output_space_dim') and self.mcmc_output_space_dim is None and hasattr(self, 'ctm_out_dims'):\n",
    "            self.mcmc_output_space_dim = self.ctm_out_dims\n",
    "        \n",
    "        if hasattr(self, 'ctm_neuron_select_type') and \\\n",
    "           VALID_NEURON_SELECT_TYPES is not None and self.ctm_neuron_select_type not in VALID_NEURON_SELECT_TYPES:\n",
    "            print(f\"Warning: ctm_neuron_select_type '{self.ctm_neuron_select_type}' is not in VALID_NEURON_SELECT_TYPES ({VALID_NEURON_SELECT_TYPES}).\")\n",
    "\n",
    "        if hasattr(self, 'positional_embedding_type') and self.positional_embedding_type is not None:\n",
    "            if VALID_POSITIONAL_EMBEDDING_TYPES is None: # Fallback if import failed\n",
    "                print(f\"Warning: VALID_POSITIONAL_EMBEDDING_TYPES not available for validation.\")\n",
    "            elif self.positional_embedding_type not in VALID_POSITIONAL_EMBEDDING_TYPES:\n",
    "                print(f\"Warning: positional_embedding_type '{self.positional_embedding_type}' is not in VALID_POSITIONAL_EMBEDDING_TYPES ({VALID_POSITIONAL_EMBEDDING_TYPES}).\")\n",
    "            if self.positional_embedding_dim is not None and self.positional_embedding_dim <= 0:\n",
    "                raise ValueError(\"positional_embedding_dim must be positive if set.\")\n",
    "            \n",
    "            if self.reshape_patch_sequence_to_grid:\n",
    "                if self.patch_grid_width is None or self.patch_grid_width <= 0:\n",
    "                    raise ValueError(\"patch_grid_width must be a positive integer if reshape_patch_sequence_to_grid is True.\")\n",
    "                if self.positional_embedding_type not in ['learnable-fourier', 'multi-learnable-fourier', 'custom-rotational']:\n",
    "                    print(f\"Warning: reshape_patch_sequence_to_grid is True, but positional_embedding_type ('{self.positional_embedding_type}') is not a typical 2D PE. Ensure compatibility.\")\n",
    "\n",
    "        # Validations for new patch encoder\n",
    "        if self.use_dynamic_entropy_patcher:\n",
    "            if self.patch_embedding_dim <= 0:\n",
    "                raise ValueError(\"patch_embedding_dim must be positive if use_dynamic_entropy_patcher is True.\")\n",
    "            if self.entropy_patcher_min_patch_size <= 0:\n",
    "                raise ValueError(\"entropy_patcher_min_patch_size must be positive.\")\n",
    "            if self.entropy_patcher_max_patch_size < self.entropy_patcher_min_patch_size:\n",
    "                raise ValueError(\"entropy_patcher_max_patch_size must be >= entropy_patcher_min_patch_size.\")\n",
    "            if self.entropy_patcher_threshold_type not in [\"global\", \"relative_monotonic\"]:\n",
    "                raise ValueError(\"entropy_patcher_threshold_type must be 'global' or 'relative_monotonic'.\")\n",
    "        elif self.multi_granularity and self.multi_granularity_output_dim <= 0:\n",
    "            print(\"Warning: multi_granularity_output_dim might not be correctly set for validation if not using a patcher and MGP is active.\")\n",
    "        \n",
    "        if not hasattr(self, 'inferred_task_latent_dim') or self.inferred_task_latent_dim is None:\n",
    "            print(\"Warning: inferred_task_latent_dim not found or is None in config, defaulting to 64.\")\n",
    "            self.inferred_task_latent_dim = 512\n",
    "        elif self.inferred_task_latent_dim <= 0: # This check is now safe\n",
    "            raise ValueError(\"inferred_task_latent_dim must be positive.\")\n",
    " \n",
    "        if hasattr(self, 'use_hipa_attention') and self.use_hipa_attention and \\\n",
    "            (not hasattr(self, 'hipa_num_heads') or self.hipa_num_heads <= 0):\n",
    "             raise ValueError(\"hipa_num_heads must be positive if use_hipa_attention is True.\")\n",
    " \n",
    "        if hasattr(self, 'audio_output_dtype_str'):\n",
    "            if self.audio_output_dtype_str == \"float32\":\n",
    "                self.audio_output_item_size = 4\n",
    "            elif self.audio_output_dtype_str == \"int16\":\n",
    "                self.audio_output_item_size = 2\n",
    "            else:\n",
    "                if hasattr(self, 'output_audio_bytes') and self.output_audio_bytes:\n",
    "                    raise ValueError(f\"Unsupported audio_output_dtype_str: {self.audio_output_dtype_str} when output_audio_bytes is True.\")\n",
    "                else:\n",
    "                    self.audio_output_item_size = 4\n",
    "        elif hasattr(self, 'output_audio_bytes') and self.output_audio_bytes:\n",
    "            if not hasattr(self, 'audio_output_dtype_str') or self.audio_output_dtype_str is None:\n",
    "                raise ValueError(\"audio_output_dtype_str must be defined in config if output_audio_bytes is True.\")\n",
    "        else:\n",
    "            self.audio_output_item_size = 4\n",
    "\n",
    "        # Calculate unet_input_feature_dim if not set\n",
    "        if self.unet_input_feature_dim is None:\n",
    "            if self.max_sequence_length <= 0 or self.audio_output_item_size <= 0:\n",
    "                raise ValueError(\"max_sequence_length and audio_output_item_size must be positive to calculate unet_input_feature_dim.\")\n",
    "            self.unet_input_feature_dim = self.max_sequence_length // self.audio_output_item_size\n",
    "            if self.unet_input_feature_dim <= 0:\n",
    "                raise ValueError(f\"Calculated unet_input_feature_dim ({self.unet_input_feature_dim}) must be positive. Check max_sequence_length and audio_output_item_size.\")\n",
    "        elif self.unet_input_feature_dim <= 0:\n",
    "            raise ValueError(\"unet_input_feature_dim, if set, must be positive.\")\n",
    "\n",
    "        if self.use_jepa_training:\n",
    "            if not (0 < self.jepa_mask_ratio_min < 1 and 0 < self.jepa_mask_ratio_max < 1 and self.jepa_mask_ratio_min <= self.jepa_mask_ratio_max):\n",
    "                raise ValueError(\"JEPA mask ratios must be between 0 and 1, with min <= max.\")\n",
    "            if not (0 < self.jepa_context_scale_min < 1 and 0 < self.jepa_context_scale_max < 1 and self.jepa_context_scale_min <= self.jepa_context_scale_max):\n",
    "                raise ValueError(\"JEPA context scales must be between 0 and 1, with min <= max.\")\n",
    "            if not (0 <= self.jepa_momentum_beta < 1):\n",
    "                raise ValueError(\"jepa_momentum_beta must be between 0 and 1.\")\n",
    "            if self.jepa_num_target_blocks <= 0:\n",
    "                raise ValueError(\"jepa_num_target_blocks must be positive.\")\n",
    "            if not self.use_dynamic_entropy_patcher:\n",
    "                print(\"Warning: JEPA training is enabled but use_dynamic_entropy_patcher is False. JEPA relies on the patch embeddings from LearnedBytePatcherEncoder.\")\n",
    "\n",
    "# Define EnhancedCTMConfig for ARC with EnhancedCTMDiffusion\n",
    "config_arc_diffusion = EnhancedCTMConfig(\n",
    "    d_model=512,\n",
    "    #inferred_task_latent_dim=64, # This line remains commented out\n",
    "    n_heads=8,\n",
    "    n_layers=24, \n",
    "    max_sequence_length=MAX_SEQUENCE_LENGTH,\n",
    "    dropout=0.1,\n",
    "    use_dynamic_entropy_patcher=True,\n",
    "    patch_embedding_dim=256,\n",
    "    patch_grid_width=16,\n",
    "    patch_encoder_cnn_channels=64,\n",
    "    entropy_patcher_threshold_type=\"global\",\n",
    "    entropy_patcher_global_threshold=0.75,\n",
    "    entropy_patcher_relative_threshold=0.1,\n",
    "    entropy_patcher_min_patch_size=4,\n",
    "    entropy_patcher_max_patch_size=128,\n",
    "    # Parameters for the learnable entropy model within LearnedBytePatcherEncoder\n",
    "    entropy_model_byte_vocab_size=256,\n",
    "    entropy_model_embedding_dim=64,\n",
    "    entropy_model_hidden_dim=128,\n",
    "    entropy_model_num_layers=1,\n",
    "    entropy_model_dropout=0.1,\n",
    "    entropy_model_loss_weight=0.1,\n",
    "    \n",
    "    ctm_input_dim=256,\n",
    "    ctm_d_model=512,\n",
    "    ctm_iterations=5,\n",
    "    ctm_heads=8,\n",
    "    ctm_out_dims=512,\n",
    "    ctm_neuron_select_type='bio_multi_objective',\n",
    "    \n",
    "    # Attention Mechanism Type\n",
    "    attention_type=\"subquadratic\",  # Options: \"standard\", \"binary_sparse\", \"subquadratic\"\n",
    "    \n",
    "    # Subquadratic Attention Parameters\n",
    "    subquadratic_attn_epsilon=1e-6,\n",
    "    subquadratic_attn_poly_degree=5,\n",
    "    attention_qkv_bias=True, # Corrected capitalization\n",
    "    \n",
    "    # Positional Embedding Parameters\n",
    "    positional_embedding_type='multi-learnable-fourier',\n",
    "    positional_embedding_dim=None,\n",
    "    reshape_patch_sequence_to_grid=True,\n",
    "    #patch_grid_width=None, #Already defined in the byte patch section of this config. \n",
    "\n",
    "    # Pipeline Parallelism Parameters\n",
    "    enable_pipeline_parallelism=True,\n",
    "    pipeline_stages=4,\n",
    "    pipeline_overlap_ratio=0.7,\n",
    "    \n",
    "    # Adaptive Batch Sizing Parameters\n",
    "    enable_adaptive_batching=True,\n",
    "    initial_batch_size=32,\n",
    "    min_batch_size=8,\n",
    "    max_batch_size=256,\n",
    "    batch_adaptation_frequency=100,\n",
    "    memory_threshold_high=0.85,\n",
    "    memory_threshold_low=0.6,\n",
    "    \n",
    "    # Smart Data Sampling Parametersa\n",
    "    enable_smart_sampling=True,\n",
    "    sample_importance_weight=0.6,\n",
    "    sample_diversity_weight=0.4,\n",
    "    initial_sample_ratio=0.3,\n",
    "    complexity_analysis_enabled=True,\n",
    "    \n",
    "    # Multi-input/output parameters\n",
    "    num_inputs=1,\n",
    "    num_outputs=1,\n",
    "    output_dims=[64],  # Directly pass the list value\n",
    "    \n",
    "    # Self-supervised learning\n",
    "    ssl_dim=128,\n",
    "    ssl_weight=0.1,\n",
    "    ssl_temperature=0.07,\n",
    "    ssl_noise_std=0.1,\n",
    "    \n",
    "    # Spatiotemporal Processing\n",
    "    use_spatial=True,\n",
    "    \n",
    "    # WINA Attention\n",
    "    use_wina_attention=True,\n",
    "    \n",
    "    # Multi-task Learning Parameters\n",
    "    max_tasks=50,\n",
    "    diffusion_steps=1000,\n",
    "    ctm_diffusion_coupling_strength=0.8,\n",
    "    vocab_size=None,\n",
    "    #enable_enhanced_mcmc=False, #ONLY USE THE ARC_AGI NOTEBOOK VERSION AND NOT THE ONE IMPORTED FROM THE DIFFUSION_NEWNEW file (This needs to be false). This flie cannot use this variable.\n",
    "    #mcmc_config=MCMC_CONFIG_ARC, #I don't think this is needed. \n",
    "    output_audio_bytes=False\n",
    ")\n",
    "\n",
    "print(\"✓ EnhancedCTMConfig for ARC (config_arc_diffusion) created.\")\n",
    "\n",
    "if 'enhanced_ctm_mcmc' not in globals():\n",
    "    print(\"Warning: 'enhanced_ctm_mcmc' not found in globals. Defaulting to None. Ensure the cell defining it (approx. lines 1820-1866) was run successfully.\")\n",
    "    enhanced_ctm_mcmc = None\n",
    "    \n",
    "if 'EnhancedCTMDiffusion' in globals() and EnhancedCTMDiffusion is not None:\n",
    "    ctm_model_arc = EnhancedCTMDiffusion(config=config_arc_diffusion).to(device)\n",
    "    print(\"✓ EnhancedCTMDiffusion model for ARC (ctm_model_arc) initialized.\")\n",
    "\n",
    "    # The external ARC output head will take features from the CTM core part of EnhancedCTMDiffusion\n",
    "    arc_output_head_input_dim = config_arc_diffusion.ctm_out_dims\n",
    "    arc_output_head = nn.Linear(arc_output_head_input_dim, ARC_OUTPUT_HEAD_DIM).to(device)\n",
    "    print(f\"✓ ARC Output Head initialized (input_dim: {arc_output_head_input_dim}, output_dim: {ARC_OUTPUT_HEAD_DIM}).\")\n",
    "\n",
    "    # Handle external MCMC integration if enabled\n",
    "    if 'enhanced_ctm_mcmc' in globals() and ENABLE_CTM_MCMC_INTEGRATION_FOR_ARC and enhanced_ctm_mcmc:\n",
    "        # Ensure the external MCMC module's input_dim matches the new CTM's output\n",
    "        if enhanced_ctm_mcmc.thought_network[0].in_features != config_arc_diffusion.ctm_out_dims:\n",
    "            print(f\"Re-initializing external enhanced_ctm_mcmc for new input_dim {config_arc_diffusion.ctm_out_dims}\")\n",
    "            # This part of the code assumes 'EnhancedCTMFenchelYoungIntegration' and other related variables are defined.\n",
    "            # If not, this will raise an error, which is expected behavior if setup is wrong.\n",
    "            enhanced_ctm_mcmc = EnhancedCTMFenchelYoungIntegration(\n",
    "                input_dim=config_arc_diffusion.ctm_out_dims,\n",
    "                output_space=arc_grid_output_space,\n",
    "                mcmc_config=MCMC_CONFIG_ARC,\n",
    "                use_large_neighborhood_search=True,\n",
    "                lns_frequency=5,\n",
    "                lns_neighborhood_size=10\n",
    "            )\n",
    "        ctm_mcmc_integration_arc = enhanced_ctm_mcmc.to(device) if enhanced_ctm_mcmc else None\n",
    "        print(f\"✓ External MCMC Integration for ARC is {'enabled' if ctm_mcmc_integration_arc else 'FAILED to enable'}.\")\n",
    "    else:\n",
    "        ctm_mcmc_integration_arc = None\n",
    "\n",
    "    arc_trainable_params = list(ctm_model_arc.parameters())\n",
    "    if arc_output_head:\n",
    "        arc_trainable_params.extend(list(arc_output_head.parameters()))\n",
    "    if ctm_mcmc_integration_arc:\n",
    "        arc_trainable_params.extend(list(ctm_mcmc_integration_arc.parameters()))\n",
    "\n",
    "    optimizer_arc = optim.AdamW([p for p in arc_trainable_params if p.requires_grad], lr=LEARNING_RATE, weight_decay=1e-4)\n",
    "\n",
    "    if ACCELERATE_AVAILABLE:\n",
    "        print(\" -> Preparing components with Hugging Face Accelerate...\")\n",
    "        accelerator_arc = Accelerator()\n",
    "        components_to_prepare = [ctm_model_arc, optimizer_arc]\n",
    "        if arc_output_head:\n",
    "            components_to_prepare.insert(1, arc_output_head)\n",
    "        if ctm_mcmc_integration_arc:\n",
    "            components_to_prepare.insert(2, ctm_mcmc_integration_arc)\n",
    "        \n",
    "        prepared_components = accelerator_arc.prepare(*components_to_prepare)\n",
    "        \n",
    "        # Unpack the prepared components carefully\n",
    "        ctm_model_arc = prepared_components[0]\n",
    "        next_idx = 1\n",
    "        if arc_output_head:\n",
    "            arc_output_head = prepared_components[next_idx]\n",
    "            next_idx += 1\n",
    "        if ctm_mcmc_integration_arc:\n",
    "            ctm_mcmc_integration_arc = prepared_components[next_idx]\n",
    "            next_idx += 1\n",
    "        optimizer_arc = prepared_components[next_idx]\n",
    "\n",
    "        print(\"✓ ARC models and optimizer prepared with Accelerate.\")\n",
    "else:\n",
    "    print(\"⚠️ Hugging Face Accelerate not available. Running on a single device.\")\n",
    "\n",
    "def find_directory(start_path, dir_name):\n",
    "    \"\"\"Recursively finds a directory by name.\"\"\"\n",
    "    for root, dirs, _ in os.walk(start_path):\n",
    "        if dir_name in dirs:\n",
    "            found_path = os.path.join(root, dir_name)\n",
    "            print(f\"Found '{dir_name}' directory at: {found_path}\")\n",
    "            return found_path\n",
    "    return None\n",
    "\n",
    "def find_file_directory(start_path, filename):\n",
    "    \"\"\"Recursively finds a file and returns its directory.\"\"\"\n",
    "    for root, _, files in os.walk(start_path):\n",
    "        if filename in files:\n",
    "            found_dir = os.path.abspath(root)\n",
    "            print(f\"Found '{filename}' in directory: {found_dir}\")\n",
    "            return found_dir\n",
    "    print(f\"Warning: File '{filename}' not found starting from '{start_path}'.\")\n",
    "    \n",
    "print(\"\\n--- Searching for evaluation and checkpoint directories ---\")\n",
    "# Path to ARC evaluation tasks\n",
    "# Path to CTM checkpoints\n",
    "CHECKPOINT_DIR_ARC_SEARCHED = find_directory(\".\", \"ctm_arc_agi_2_enhanced_diffusion\")\n",
    "\n",
    "# --- Search for the specific evaluation file to determine the data directory ---\n",
    "\n",
    "dynamic_eval_dir =  eval_dir \n",
    "CHECKPOINT_DIR_ARC = CHECKPOINT_DIR_ARC_SEARCHED if CHECKPOINT_DIR_ARC_SEARCHED else os.path.join(\"checkpoints\", \"ctm_arc_agi_2_enhanced_diffusion\")\n",
    "\n",
    "if not CHECKPOINT_DIR_ARC_SEARCHED:\n",
    "    print(f\"-> Checkpoint directory not found dynamically, using fallback: '{CHECKPOINT_DIR_ARC}'\")\n",
    "\n",
    "NUM_EPOCHS_ARC = 20\n",
    "\n",
    "\n",
    "\n",
    "class NewCustomARCGridDataset(Dataset):\n",
    "    def __init__(self, data_path, max_grid_size=MAX_GRID_SIZE, padding_value=PADDING_VALUE):\n",
    "        self.data_path = data_path\n",
    "        self.task_files = []\n",
    "        if os.path.isfile(data_path):\n",
    "            if data_path.endswith(\".json\"):\n",
    "                self.task_files.append(data_path)\n",
    "        elif os.path.isdir(data_path):\n",
    "            for root, _, files in os.walk(data_path):\n",
    "                for file in files:\n",
    "                    if file.endswith(\".json\"):\n",
    "                        self.task_files.append(os.path.join(root, file))\n",
    "        else:\n",
    "            print(f\"Error: Provided data path does not exist or is not a file/directory: {data_path}\")\n",
    "            self.tasks = []\n",
    "            return\n",
    "\n",
    "        if not self.task_files:\n",
    "            print(f\"Warning: No .json files found at path: {data_path}\")\n",
    "            self.tasks = []\n",
    "            return\n",
    "        \n",
    "        self.max_grid_size = max_grid_size\n",
    "        self.padding_value = padding_value\n",
    "        self.tasks = [json.load(open(f)) for f in self.task_files]\n",
    "        print(f\"Loaded {len(self.tasks)} tasks from {data_path}.\")\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.tasks)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        task_data = self.tasks[idx]\n",
    "        processed_task = {'train': [], 'test': [], 'id': os.path.basename(self.task_files[idx])}\n",
    "        for pair_type in ['train', 'test']:\n",
    "            for item in task_data.get(pair_type, []):\n",
    "                input_grid = item['input']\n",
    "                output_grid = item['output']\n",
    "                original_input_dims = (len(input_grid), len(input_grid[0]) if input_grid else 0)\n",
    "                original_output_dims = (len(output_grid), len(output_grid[0]) if output_grid else 0)\n",
    "                padded_input = pad_grid(input_grid, self.max_grid_size, self.padding_value)\n",
    "                padded_output = pad_grid(output_grid, self.max_grid_size, self.padding_value)\n",
    "                processed_task[pair_type].append({\n",
    "                    'input': torch.from_numpy(padded_input).long(),\n",
    "                    'output': torch.from_numpy(padded_output).long(),\n",
    "                    'original_input_dims': original_input_dims,\n",
    "                    'original_output_dims': original_output_dims\n",
    "                })\n",
    "        return processed_task\n",
    "\n",
    "# --- Safetensors loading fix ---\n",
    "# The load_file_safely function has been removed.\n",
    "# Direct use of 'load_file' from safetensors.torch is now used in the main evaluation loop.\n",
    "\n",
    "# --- Dataloader Initialization for Evaluation ---\n",
    "print(\"\\n--- Initializing Evaluation Dataloader ---\")\n",
    "arc_eval_loader = None\n",
    "if 'ARC_EVAL_DIR' in globals() and os.path.exists(ARC_EVAL_DIR):\n",
    "    print(f\"  > Using NewCustomARCGridDataset for evaluation from path: {ARC_EVAL_DIR}\")\n",
    "    arc_eval_dataset = NewCustomARCGridDataset(\n",
    "        data_path=ARC_EVAL_DIR,\n",
    "        max_grid_size=MAX_GRID_SIZE,\n",
    "        padding_value=PADDING_VALUE\n",
    "    )\n",
    "    if len(arc_eval_dataset) > 0:\n",
    "        arc_eval_loader = DataLoader(arc_eval_dataset, batch_size=1, shuffle=False)\n",
    "        print(f\"✓ Evaluation DataLoader initialized with {len(arc_eval_dataset)} tasks.\")\n",
    "    else:\n",
    "        print(\"⚠️ Evaluation dataset is empty. Skipping evaluation.\")\n",
    "else:\n",
    "    print(f\"⚠️ Evaluation directory not found or not specified (ARC_EVAL_DIR='{globals().get('ARC_EVAL_DIR', 'Not Set')}'). Cannot create DataLoader.\")\n",
    "\n",
    "# --- Main Evaluation Logic ---\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(f\"🔬 STARTING ARC-AGI-2 Evaluation on device '{device}'\")\n",
    "print(\"=\"*60 + \"\\n\")\n",
    "\n",
    "if not all([ctm_model_arc, arc_output_head, arc_eval_loader]):\n",
    "     print(\"⚠️ Skipping evaluation due to missing components.\")\n",
    "else:\n",
    "    latest_epoch = NUM_EPOCHS_ARC\n",
    "    ctm_checkpoint_path_eval = os.path.join(CHECKPOINT_DIR_ARC, f\"ctm_model_arc_epoch_{latest_epoch}.safetensors\")\n",
    "    head_checkpoint_path_eval = os.path.join(CHECKPOINT_DIR_ARC, f\"arc_output_head_epoch_{latest_epoch}.safetensors\")\n",
    "\n",
    "    try:\n",
    "        # Load CTM Model\n",
    "        if os.path.exists(ctm_checkpoint_path_eval):\n",
    "            print(f\"  > Loading CTM checkpoint from {ctm_checkpoint_path_eval}...\")\n",
    "            # Load state_dict using the safetensors library directly, as per user feedback\n",
    "            state_dict_ctm = load_file(ctm_checkpoint_path_eval, device=\"cpu\")\n",
    "            ctm_model_arc.load_state_dict(state_dict_ctm, strict=False)\n",
    "            print(f\"✓ Loaded CTM checkpoint from epoch {latest_epoch}.\")\n",
    "        else:\n",
    "            print(f\"⚠️ CTM Checkpoint not found at {ctm_checkpoint_path_eval}.\")\n",
    "\n",
    "        # Load ARC Output Head Model\n",
    "        if os.path.exists(head_checkpoint_path_eval):\n",
    "            print(f\"  > Loading ARC Output Head checkpoint from {head_checkpoint_path_eval}...\")\n",
    "            state_dict_head = load_file(head_checkpoint_path_eval, device=\"cpu\")\n",
    "            arc_output_head.load_state_dict(state_dict_head, strict=False)\n",
    "            print(f\"✓ Loaded ARC Output Head checkpoint from epoch {latest_epoch}.\")\n",
    "        else:\n",
    "            print(f\"⚠️ ARC Output Head Checkpoint not found at {head_checkpoint_path_eval}.\")\n",
    "\n",
    "        ctm_model_arc.eval()\n",
    "        arc_output_head.eval()\n",
    "        \n",
    "        total_tasks = 0\n",
    "        solved_tasks = 0\n",
    "        \n",
    "        with torch.inference_mode():\n",
    "            for task_idx, task_batch in enumerate(arc_eval_loader):\n",
    "                if not task_batch: continue\n",
    "                \n",
    "                current_task_data = task_batch\n",
    "                total_tasks += 1\n",
    "                task_solved_overall = True\n",
    "\n",
    "                if 'test' not in current_task_data or not current_task_data['test']:\n",
    "                    print(f\"Task {task_idx + 1} ({current_task_data.get('id', 'N/A')}): No test cases found. Skipping.\")\n",
    "                    task_solved_overall = False\n",
    "                    continue\n",
    "\n",
    "                for test_pair_idx, test_pair in enumerate(current_task_data['test']):\n",
    "                    input_grid_np_eval = test_pair['input'].cpu().numpy()\n",
    "                    input_bytes_eval_single = serialize_and_pad_grid(input_grid_np_eval, max_len=MAX_SEQUENCE_LENGTH, pad_value=PADDING_BYTE_VALUE)\n",
    "                    input_bytes_eval_np = np.frombuffer(input_bytes_eval_single, dtype=np.uint8).copy()\n",
    "                    input_bytes_eval = torch.from_numpy(input_bytes_eval_np).to(torch.uint8).unsqueeze(0).to(device)\n",
    "\n",
    "                    target_grid_np = test_pair['output'].cpu().numpy()\n",
    "                    original_dims = test_pair['original_output_dims']\n",
    "\n",
    "                    test_input_solved = False\n",
    "                    for trial in range(2):\n",
    "                        current_batch_size_eval = input_bytes_eval.size(0)\n",
    "                        eval_timestep = torch.zeros(current_batch_size_eval, device=input_bytes_eval.device).long()\n",
    "\n",
    "                        # This call assumes ctm_model_arc is on the correct device already\n",
    "                        eval_model_output_dict = ctm_model_arc(\n",
    "                            byte_sequence=input_bytes_eval,\n",
    "                            mode='ctm_controlled_diffusion',\n",
    "                            target_diffusion_output=None,\n",
    "                            timestep=eval_timestep,\n",
    "                            task_name=\"ARC_AGI_2_EVAL_DIFFUSION\"\n",
    "                        )\n",
    "                        \n",
    "                        predicted_byte_sequence = eval_model_output_dict.get('diffusion_output')\n",
    "                        \n",
    "                        if predicted_byte_sequence is None:\n",
    "                            print(\"Warning: Key 'diffusion_output' not found. Trying 'final_output'.\")\n",
    "                            predicted_byte_sequence = eval_model_output_dict.get('final_output')\n",
    "                        \n",
    "                        if predicted_byte_sequence is None:\n",
    "                            print(\"Warning: Generated output key not found. Using zeros as prediction.\")\n",
    "                            preds_grid = np.zeros(MAX_GRID_SIZE, dtype=int)\n",
    "                        else:\n",
    "                            if predicted_byte_sequence.ndim == 1 and current_batch_size_eval == 1:\n",
    "                                predicted_byte_sequence = predicted_byte_sequence.unsqueeze(0)\n",
    "\n",
    "                            if predicted_byte_sequence.shape[1] >= ARC_INPUT_FLAT_DIM:\n",
    "                                preds_flat_bytes = predicted_byte_sequence[0, :ARC_INPUT_FLAT_DIM]\n",
    "                                preds_grid = preds_flat_bytes.view(MAX_GRID_SIZE).long().cpu().numpy()\n",
    "                            else:\n",
    "                                print(f\"Warning: Generated byte sequence too short. Using zeros.\")\n",
    "                                preds_grid = np.zeros(MAX_GRID_SIZE, dtype=int)\n",
    "                        \n",
    "                        h, w = original_dims\n",
    "                        final_pred = preds_grid[:h, :w]\n",
    "                        final_target = target_grid_np[:h, :w]\n",
    "\n",
    "                        if np.array_equal(final_pred, final_target):\n",
    "                            test_input_solved = True\n",
    "                            break\n",
    "\n",
    "                    if not test_input_solved:\n",
    "                        task_solved_overall = False\n",
    "                        break\n",
    "                \n",
    "                if task_solved_overall:\n",
    "                    solved_tasks += 1\n",
    "                    print(f\"  Task {task_idx + 1}/{len(arc_eval_loader)} ({current_task_data.get('id', 'N/A')}): SOLVED\")\n",
    "                else:\n",
    "                    print(f\"  Task {task_idx + 1}/{len(arc_eval_loader)} ({current_task_data.get('id', 'N/A')}): FAILED\")\n",
    "        \n",
    "        if total_tasks > 0:\n",
    "            accuracy = (solved_tasks / total_tasks) * 100\n",
    "            summary = f\"ARC-AGI-2 Evaluation Summary:\\n  Total tasks evaluated: {total_tasks}\\n  Tasks solved: {solved_tasks}\\n  Accuracy: {accuracy:.2f}%\"\n",
    "            print(f\"\\n{summary}\")\n",
    "            with open('arc_agi_2_evaluation_summary.txt', 'w') as f:\n",
    "                f.write(summary)\n",
    "        else:\n",
    "            print(\"\\nARC-AGI-2 Evaluation: No tasks were evaluated.\")\n",
    "            \n",
    "    except FileNotFoundError as e:\n",
    "        print(f\"❌ Checkpoint file not found: {e}. Please ensure paths are correct.\")   \n",
    "    except Exception as e:\n",
    "        print(f\"❌ Error during ARC-AGI-2 evaluation: {e}\")\n",
    "        traceback.print_exc()\n",
    "        \n",
    "    print(\"\\n🔬 ARC-AGI-2 Evaluation Phase Completed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0a86e362-591b-4704-8466-4c64f47f67c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0934a4d8.json  332f06d7.json  65b59efc.json  8e5c0c38.json  c7f57c3e.json\n",
      "135a2760.json  35ab12c3.json  67e490f4.json  8f215267.json  cb2d8a2c.json\n",
      "136b0064.json  36a08778.json  6e453dd6.json  8f3a5a89.json  cbebaa4b.json\n",
      "13e47133.json  38007db0.json  6e4f6532.json  9385bd28.json  d35bdbdc.json\n",
      "142ca369.json  3a25b0d8.json  6ffbe589.json  97d7923e.json  d59b0160.json\n",
      "16b78196.json  3dc255db.json  71e489b6.json  981571dc.json  d8e07eb2.json\n",
      "16de56c4.json  3e6067c3.json  7491f3cf.json  9aaea919.json  da515329.json\n",
      "1818057f.json  409aa875.json  7666fa5d.json  9bbf930d.json  db0c5428.json\n",
      "195c6913.json  446ef5d2.json  78332cb0.json  a251c730.json  db695cfb.json\n",
      "1ae2feb7.json  45a5af55.json  7b0280bc.json  a25697e4.json  dbff022c.json\n",
      "20270e3b.json  4a21e3da.json  7b3084d4.json  a32d8b75.json  dd6b8c4b.json\n",
      "20a9e565.json  4c3d4a41.json  7b5033c1.json  a395ee82.json  de809cff.json\n",
      "21897d95.json  4c416de3.json  7b80bb43.json  a47bf94d.json  dfadab01.json\n",
      "221dfab4.json  4c7dc4dd.json  7c66cb00.json  a6f40cea.json  e12f9a14.json\n",
      "247ef758.json  4e34c42c.json  7ed72f31.json  aa4ec2a5.json  e3721c99.json\n",
      "269e22fb.json  53fb4810.json  800d221b.json  abc82100.json  e376de54.json\n",
      "271d71e2.json  5545f144.json  80a900e0.json  b0039139.json  e8686506.json\n",
      "28a6681f.json  581f7754.json  8698868d.json  b10624e5.json  e87109e9.json\n",
      "291dc1e1.json  58490d8a.json  88bcf3b4.json  b5ca7ac4.json  edb79dae.json\n",
      "2b83f449.json  58f5dbd5.json  88e364bc.json  b6f77b65.json  eee78d87.json\n",
      "2ba387bc.json  5961cc34.json  89565ca0.json  b99e7126.json  f560132c.json\n",
      "2c181942.json  5dbc8537.json  898e7135.json  b9e38dc0.json  f931b4a8.json\n",
      "2d0172a1.json  62593bfd.json  8b7bacbf.json  bf45cf4b.json  faa9f03d.json\n",
      "31f7f899.json  64efde09.json  8b9c3697.json  c4d067a0.json  fc7cae8d.json\n"
     ]
    }
   ],
   "source": [
    "ls /workspace/Arc-AGI-2/contineous-thought-machines/data/evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd259053-d85c-4591-bc59-d57bc5495307",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import numpy as np\n",
    "import glob\n",
    "import json\n",
    "import traceback\n",
    "import random\n",
    "import math\n",
    "from dataclasses import dataclass, field\n",
    "from typing import List, Optional, Callable\n",
    "import collections\n",
    "\n",
    "# --- Focal Loss Implementation ---\n",
    "class FocalLoss(nn.Module):\n",
    "    \"\"\"\n",
    "    Focal Loss for dense object detection.\n",
    "    Reduces the relative loss for well-classified examples, putting more\n",
    "    focus on hard, misclassified examples.\n",
    "    \"\"\"\n",
    "    def __init__(self, alpha=0.25, gamma=2.0, reduction='mean', ignore_index=-1):\n",
    "        super(FocalLoss, self).__init__()\n",
    "        self.alpha = alpha\n",
    "        self.gamma = gamma\n",
    "        self.reduction = reduction\n",
    "        self.ignore_index = ignore_index\n",
    "\n",
    "    def forward(self, inputs, targets):\n",
    "        # inputs: [N, C, H, W], targets: [N, H, W]\n",
    "        if targets.dim() == 4 and targets.shape[1] == 1:\n",
    "            targets = targets.squeeze(1)\n",
    "        ce_loss_fn = nn.CrossEntropyLoss(reduction='none', ignore_index=self.ignore_index)\n",
    "        ce_loss = ce_loss_fn(inputs, targets)\n",
    "        \n",
    "        pt = torch.exp(-ce_loss)\n",
    "        focal_loss = self.alpha * (1 - pt)**self.gamma * ce_loss\n",
    "\n",
    "        if self.reduction == 'mean':\n",
    "            return focal_loss.mean()\n",
    "        elif self.reduction == 'sum':\n",
    "            return focal_loss.sum()\n",
    "        return focal_loss\n",
    "\n",
    "# --- MCMC Self-Learning Components (Adapted from SEAL/mcmc_search.py) ---\n",
    "\n",
    "@dataclass\n",
    "class MCMCConfig:\n",
    "    \"\"\"Configuration for MCMC sampling parameters.\"\"\"\n",
    "    num_chains: int = 3000\n",
    "    chain_length: int = 8000\n",
    "    burn_in: int = 200\n",
    "    temperature_schedule: str = \"adaptive\"  # \"geometric\", \"adaptive\"\n",
    "    initial_temp: float = 30.0\n",
    "    final_temp: float = 0.01\n",
    "    decay_rate: float = 0.998\n",
    "    # Adaptive cooling parameters\n",
    "    target_acceptance_rate: float = 0.30\n",
    "    adaptive_adjustment_factor: float = 0.02\n",
    "    # Proposal strategy\n",
    "    proposal_strategy: str = \"hybrid\" # \"random\", \"hybrid\", \"structured\"\n",
    "    structured_mutation_prob: float = 0.5\n",
    "    mutation_region_size: int = 3\n",
    "    # Convergence Monitoring\n",
    "    convergence_window: int = 250\n",
    "    convergence_threshold: float = 1e-6\n",
    "    # --- New: Iterative Local Refinement ---\n",
    "    enable_local_refinement: bool = True\n",
    "    refinement_trigger_similarity: float = 0.95 # Similarity to target to trigger refinement\n",
    "    refinement_steps: int = 500\n",
    "\n",
    "class ARCSelfEditSpace:\n",
    "    \"\"\"Defines the space of ARC grid edits and their neighborhoods with adaptive mutation.\"\"\"\n",
    "    def __init__(self, grid_dims: tuple, initial_mutation_rate: float = 0.1, final_mutation_rate: float = 0.01, decay_steps: int = 8000):\n",
    "        self.grid_dims = grid_dims\n",
    "        self.initial_mutation_rate = initial_mutation_rate\n",
    "        self.final_mutation_rate = final_mutation_rate\n",
    "        self.decay_steps = decay_steps\n",
    "\n",
    "    def _get_mutation_rate(self, step: int, is_refinement: bool = False) -> float:\n",
    "        \"\"\"Anneals mutation rate from high to low for broader exploration followed by exploitation.\"\"\"\n",
    "        if is_refinement:\n",
    "            return self.final_mutation_rate / 5.0 # Much lower rate for fine-tuning\n",
    "        if step >= self.decay_steps:\n",
    "            return self.final_mutation_rate\n",
    "        \n",
    "        # Use a cosine annealing schedule for a smoother transition\n",
    "        progress = step / self.decay_steps\n",
    "        cosine_out = np.cos(np.pi * progress) + 1\n",
    "        return self.final_mutation_rate + 0.5 * (self.initial_mutation_rate - self.final_mutation_rate) * cosine_out\n",
    "\n",
    "    def get_neighbors(self, state: np.ndarray, step: int, n_neighbors: int = 1, error_focus: Optional[np.ndarray] = None, strategy: str = \"hybrid\", region_size: int = 3, is_refinement: bool = False) -> List[np.ndarray]:\n",
    "        \"\"\"\n",
    "        Generates neighbors using adaptive mutation and various proposal strategies.\n",
    "        During refinement, it uses a much smaller mutation rate.\n",
    "        \"\"\"\n",
    "        mutation_rate = self._get_mutation_rate(step, is_refinement=is_refinement)\n",
    "        neighbors = []\n",
    "        h, w = self.grid_dims\n",
    "\n",
    "        for _ in range(n_neighbors):\n",
    "            neighbor = state.copy()\n",
    "            \n",
    "            # Decide between pixel-level and region-level mutation\n",
    "            use_structured_mutation = random.random() < 0.5 # 50% chance for structured\n",
    "\n",
    "            if strategy == 'hybrid' and error_focus is not None and random.random() < 0.75:\n",
    "                error_indices = np.argwhere(error_focus > 0)\n",
    "                if len(error_indices) > 0:\n",
    "                    num_mutations = max(1, int(len(error_indices) * mutation_rate * 2))\n",
    "                    for _ in range(num_mutations):\n",
    "                        idx_to_mutate = random.choice(error_indices)\n",
    "                        neighbor[tuple(idx_to_mutate)] = random.randint(0, 9)\n",
    "                else:\n",
    "                    use_structured_mutation = True # Fallback to structured if no errors\n",
    "            \n",
    "            elif use_structured_mutation:\n",
    "                # Structured perturbation: modify a contiguous region\n",
    "                r_h = min(region_size, h)\n",
    "                r_w = min(region_size, w)\n",
    "                top = random.randint(0, h - r_h)\n",
    "                left = random.randint(0, w - r_w)\n",
    "                for i in range(top, top + r_h):\n",
    "                    for j in range(left, left + r_w):\n",
    "                        if random.random() < mutation_rate * 2: # Higher effective rate in region\n",
    "                            neighbor[i, j] = random.randint(0, 9)\n",
    "            else:\n",
    "                # Global, random pixel-wise mutation\n",
    "                for i in range(h):\n",
    "                    for j in range(w):\n",
    "                        if random.random() < mutation_rate:\n",
    "                            neighbor[i, j] = random.randint(0, 9)\n",
    "            \n",
    "            neighbors.append(neighbor)\n",
    "        return neighbors\n",
    "\n",
    "class CTMSurrogate:\n",
    "    \"\"\"A surrogate that uses the main CTM model and its head to guide MCMC search.\"\"\"\n",
    "    def __init__(self, model, arc_output_head, input_bytes_eval, target_grid, device, feature_extractor=None):\n",
    "        self.model = model\n",
    "        self.arc_output_head = arc_output_head\n",
    "        self.input_bytes_eval = input_bytes_eval\n",
    "        self.target_grid = target_grid # This is the cropped numpy target\n",
    "        self.device = device\n",
    "        # --- New: Placeholder for a learned feature extractor ---\n",
    "        self.feature_extractor = feature_extractor\n",
    "        self.model_prediction_full = self._get_model_prediction()\n",
    "        h, w = self.target_grid.shape\n",
    "        self.model_prediction_cropped = self.model_prediction_full[:h, :w]\n",
    "\n",
    "    def _get_model_prediction(self):\n",
    "        \"\"\"Runs the model and head once to get a baseline grid prediction.\"\"\"\n",
    "        with torch.inference_mode():\n",
    "            current_batch_size_eval = self.input_bytes_eval.size(0)\n",
    "            eval_timestep = torch.zeros(current_batch_size_eval, device=self.device).long()\n",
    "            \n",
    "            # 1. Get features from the backbone model\n",
    "            eval_model_output_dict = self.model(\n",
    "                byte_sequence=self.input_bytes_eval,\n",
    "                mode='ctm_controlled_diffusion',\n",
    "                target_diffusion_output=None,\n",
    "                timestep=eval_timestep,\n",
    "                task_name=\"ARC_AGI_2_EVAL_DIFFUSION\"\n",
    "            )\n",
    "            \n",
    "            # 2. Extract features consistent with training loop logic\n",
    "            ctm_core_output_data = eval_model_output_dict.get('ctm_core_data')\n",
    "            ctm_backbone_output = None\n",
    "            if ctm_core_output_data and 'final_sync_out' in ctm_core_output_data:\n",
    "                ctm_backbone_output = ctm_core_output_data['final_sync_out']\n",
    "            elif ctm_core_output_data and 'ctm_latent_representation' in ctm_core_output_data:\n",
    "                ctm_backbone_output = ctm_core_output_data['ctm_latent_representation']\n",
    "            \n",
    "            if ctm_backbone_output is not None:\n",
    "                # Process features like in training\n",
    "                if ctm_backbone_output.ndim > 2 and ctm_backbone_output.shape[1] > 0:\n",
    "                     ctm_features_for_head = ctm_backbone_output.mean(dim=1)\n",
    "                else:\n",
    "                     ctm_features_for_head = ctm_backbone_output\n",
    "                \n",
    "                # 3. Get logits from the prediction head\n",
    "                logits = self.arc_output_head(ctm_features_for_head)\n",
    "                preds_flat = torch.argmax(logits.view(-1, NUM_ARC_SYMBOLS), dim=-1)\n",
    "                preds_grid = preds_flat.view(MAX_GRID_SIZE).long().cpu().numpy()\n",
    "                return preds_grid\n",
    "\n",
    "        return np.zeros(MAX_GRID_SIZE, dtype=int)\n",
    "\n",
    "    def predict(self, state: np.ndarray, penalty_weight: float = 0.2) -> float:\n",
    "        \"\"\"\n",
    "        Calculates a blended score using a smoother reward function with a penalty\n",
    "        for drastic changes from the original model's prediction.\n",
    "        Includes calibrated rewards and a placeholder for feature-based similarity.\n",
    "        \"\"\"\n",
    "        # --- Fine-Grained Similarity Metric ---\n",
    "        if self.feature_extractor:\n",
    "            # Placeholder for using a learned feature-based similarity\n",
    "            # target_features = self.feature_extractor(self.target_grid)\n",
    "            # state_features = self.feature_extractor(state)\n",
    "            # sim_to_target = cosine_similarity(target_features, state_features)\n",
    "            pass\n",
    "\n",
    "        sim_to_target = np.sum(state == self.target_grid) / self.target_grid.size\n",
    "\n",
    "        # --- Calibrated Reward Function ---\n",
    "        # Sigmoid scaling to heavily penalize anything less than a perfect match\n",
    "        # The steepness (k) makes the reward sharply increase as similarity approaches 1.\n",
    "        k = 20\n",
    "        reward_from_target = 1 / (1 + np.exp(-k * (sim_to_target - 0.95)))\n",
    "\n",
    "        # Similarity to the initial (failed) model prediction\n",
    "        sim_to_model = np.sum(state == self.model_prediction_cropped) / self.model_prediction_cropped.size\n",
    "        \n",
    "        # Penalty for deviating too far from the model's original prediction\n",
    "        deviation_penalty = np.sum(state != self.model_prediction_cropped) / self.model_prediction_cropped.size\n",
    "        \n",
    "        # Dynamically balance rewards based on initial model quality\n",
    "        baseline_similarity = np.sum(self.model_prediction_cropped == self.target_grid) / self.target_grid.size\n",
    "        \n",
    "        # If the baseline is good, trust the model more. If not, trust the target reward more.\n",
    "        alpha = min(0.9, 1.0 - baseline_similarity**2) # Squaring makes it more sensitive\n",
    "\n",
    "        # Blended score\n",
    "        blended_score = (alpha * reward_from_target +\n",
    "                         (1.0 - alpha) * sim_to_model -\n",
    "                         penalty_weight * deviation_penalty)\n",
    "        return blended_score\n",
    "\n",
    "    def adapt_to_new_task(self, train_pairs: List[dict], optimizer):\n",
    "        \"\"\"\n",
    "        Fine-tunes the surrogate model's head on the training examples of a new,\n",
    "        unseen task to improve its initial predictions (meta-learning).\n",
    "        \"\"\"\n",
    "        print(\"  > Meta-learning: Adapting surrogate to new task train examples...\")\n",
    "        \n",
    "        # This is a conceptual implementation. A real one would involve\n",
    "        # a few quick gradient steps on a small, separate optimizer for the head.\n",
    "        # For now, it serves as a structural placeholder.\n",
    "        \n",
    "        # Create a temporary optimizer for the head for quick adaptation\n",
    "        head_optimizer = torch.optim.Adam(self.arc_output_head.parameters(), lr=1e-3)\n",
    "        loss_fn = FocalLoss(alpha=0.5, gamma=2.0, reduction='mean')\n",
    "        \n",
    "        # --- Pre-computation outside the loop ---\n",
    "        # Get the CTM features once, as they don't change for the task\n",
    "        with torch.no_grad():\n",
    "            eval_timestep = torch.zeros(self.input_bytes_eval.size(0), device=self.device).long()\n",
    "            eval_model_output_dict = self.model(\n",
    "                byte_sequence=self.input_bytes_eval,\n",
    "                mode='ctm_controlled_diffusion',\n",
    "                target_diffusion_output=None,\n",
    "                timestep=eval_timestep,\n",
    "                task_name=\"ARC_AGI_2_META_LEARN_FEATURES\"\n",
    "            )\n",
    "            ctm_core_output_data = eval_model_output_dict.get('ctm_core_data', {})\n",
    "            ctm_backbone_output = ctm_core_output_data.get('final_sync_out', ctm_core_output_data.get('ctm_latent_representation'))\n",
    "\n",
    "            if ctm_backbone_output is None:\n",
    "                print(\"  > Meta-learning WARNING: Could not extract features. Aborting adaptation.\")\n",
    "                return\n",
    "\n",
    "            if ctm_backbone_output.ndim > 2 and ctm_backbone_output.shape[1] > 0:\n",
    "                self.ctm_features_for_head = ctm_backbone_output.mean(dim=1).detach()\n",
    "            else:\n",
    "                self.ctm_features_for_head = ctm_backbone_output.detach()\n",
    "\n",
    "        # --- Quick fine-tuning loop ---\n",
    "        self.arc_output_head.train()\n",
    "        for epoch in range(3): # A few quick epochs\n",
    "            total_loss = 0\n",
    "            for pair in train_pairs:\n",
    "                head_optimizer.zero_grad()\n",
    "                \n",
    "                # Get target grid and its dimensions\n",
    "                output_grid_np = pair['output'].cpu().numpy()\n",
    "                h, w = pair['original_output_dims']\n",
    "                \n",
    "                # Prepare target tensor for loss calculation\n",
    "                target_grid_tensor = torch.from_numpy(output_grid_np).long().to(self.device)\n",
    "                \n",
    "                # Forward pass through the head ONLY\n",
    "                logits = self.arc_output_head(self.ctm_features_for_head)\n",
    "                logits = logits.view(1, NUM_ARC_SYMBOLS, MAX_GRID_SIZE[0], MAX_GRID_SIZE[1])\n",
    "                cropped_logits = logits[:, :, :h, :w]\n",
    "                \n",
    "                # Calculate loss\n",
    "                loss = loss_fn(cropped_logits, target_grid_tensor[:h, :w].unsqueeze(0))\n",
    "                \n",
    "                # Backward pass and optimization step\n",
    "                loss.backward()\n",
    "                head_optimizer.step()\n",
    "                total_loss += loss.item()\n",
    "            \n",
    "            if len(train_pairs) > 0:\n",
    "                print(f\"  > Meta-learning epoch {epoch+1}, Avg Loss: {total_loss / len(train_pairs):.4f}\")\n",
    "\n",
    "        self.arc_output_head.eval() # Return head to evaluation mode\n",
    "        \n",
    "        # After adaptation, re-compute the model's prediction\n",
    "        print(\"  > Re-evaluating prediction with adapted head...\")\n",
    "        self.model_prediction_full = self._get_model_prediction()\n",
    "        h_target, w_target = self.target_grid.shape\n",
    "        self.model_prediction_cropped = self.model_prediction_full[:h_target, :w_target]\n",
    "\n",
    "def metropolis_hastings_sampler(\n",
    "    initial_state: np.ndarray,\n",
    "    surrogate_func: CTMSurrogate,\n",
    "    output_space: ARCSelfEditSpace,\n",
    "    config: MCMCConfig,\n",
    "    error_focus: Optional[np.ndarray] = None\n",
    ") -> (np.ndarray, dict):\n",
    "    \"\"\"\n",
    "    Performs Metropolis-Hastings search with adaptive temperature, hybrid proposals,\n",
    "    and convergence monitoring.\n",
    "    \"\"\"\n",
    "    best_state = initial_state\n",
    "    best_energy = -surrogate_func.predict(initial_state)\n",
    "    current_state, current_energy = best_state, best_energy\n",
    "    \n",
    "    temperature = config.initial_temp\n",
    "    acceptance_history = []\n",
    "    energy_history = []\n",
    "    log_data = {'temp': [], 'acceptance_rate': [], 'energy': [], 'phase': 'exploration'}\n",
    "\n",
    "    is_refinement_phase = False\n",
    "\n",
    "    for step in range(config.chain_length + config.burn_in):\n",
    "        # --- Check for triggering local refinement ---\n",
    "        if config.enable_local_refinement and not is_refinement_phase and step > config.burn_in:\n",
    "            current_sim_to_target = np.sum(best_state == surrogate_func.target_grid) / surrogate_func.target_grid.size\n",
    "            if current_sim_to_target >= config.refinement_trigger_similarity:\n",
    "                print(f\"  > Triggering local refinement phase at step {step} (similarity: {current_sim_to_target:.2%}).\")\n",
    "                is_refinement_phase = True\n",
    "                log_data['phase'] = 'refinement'\n",
    "                # Optional: Reset temperature or adjust other params for refinement\n",
    "                temperature = config.initial_temp / 5.0 # Lower temp for refinement\n",
    "\n",
    "        # During refinement, proposals are generated with a much lower mutation rate.\n",
    "        proposal_state = random.choice(output_space.get_neighbors(\n",
    "            current_state, step=step, error_focus=error_focus,\n",
    "            strategy=config.proposal_strategy,\n",
    "            region_size=config.mutation_region_size,\n",
    "            is_refinement=is_refinement_phase\n",
    "        ))\n",
    "        proposal_energy = -surrogate_func.predict(proposal_state)\n",
    "        \n",
    "        energy_diff = proposal_energy - current_energy\n",
    "        \n",
    "        accepted = False\n",
    "        if energy_diff < 0 or (temperature > 0 and random.random() < math.exp(-energy_diff / temperature)):\n",
    "            current_state, current_energy = proposal_state, proposal_energy\n",
    "            accepted = True\n",
    "            if current_energy < best_energy:\n",
    "                best_state, best_energy = current_state, current_energy\n",
    "        \n",
    "        if step > config.burn_in:\n",
    "            acceptance_history.append(1 if accepted else 0)\n",
    "            energy_history.append(best_energy)\n",
    "\n",
    "            # Log metrics periodically\n",
    "            if step % 100 == 0:\n",
    "                acc_rate = np.mean(acceptance_history[-100:]) if acceptance_history else 0\n",
    "                log_data['temp'].append(temperature)\n",
    "                log_data['acceptance_rate'].append(acc_rate)\n",
    "                log_data['energy'].append(best_energy)\n",
    "\n",
    "            # Adaptive Temperature & Convergence Check\n",
    "            if len(acceptance_history) > 50:\n",
    "                acceptance_rate = np.mean(acceptance_history[-50:])\n",
    "                if acceptance_rate < config.target_acceptance_rate:\n",
    "                    temperature *= (1 + config.adaptive_adjustment_factor)\n",
    "                    # If acceptance is too low, our steps might be too big.\n",
    "                    config.mutation_region_size = max(2, config.mutation_region_size - 1)\n",
    "                else:\n",
    "                    temperature *= (1 - config.adaptive_adjustment_factor)\n",
    "                    # If acceptance is high, we can afford to explore more broadly.\n",
    "                    config.mutation_region_size = min(7, config.mutation_region_size + 1)\n",
    "\n",
    "                temperature = max(temperature, config.final_temp)\n",
    "                acceptance_history.pop(0)\n",
    "\n",
    "            if len(energy_history) > config.convergence_window:\n",
    "                windowed_energy = energy_history[-config.convergence_window:]\n",
    "                if np.std(windowed_energy) < config.convergence_threshold:\n",
    "                    print(f\"  > Convergence detected at step {step} (energy stddev < {config.convergence_threshold}). Terminating early.\")\n",
    "                    break\n",
    "                energy_history.pop(0)\n",
    "\n",
    "    print(f\"  > MCMC finished. Best energy: {-best_energy:.4f}\")\n",
    "    return best_state, log_data\n",
    "\n",
    "def perform_online_update(model, arc_output_head, optimizer, scheduler, input_bytes, failed_grid_np: np.ndarray, corrected_grid_np: np.ndarray, original_dims: tuple, device, failed_features: Optional[torch.Tensor] = None):\n",
    "    \"\"\"\n",
    "    Performs a single, targeted fine-tuning step on the model using Focal Loss\n",
    "    and a learning rate scheduler for more stable and focused updates.\n",
    "    Enhanced with comments on alternative loss functions and stability penalties.\n",
    "    \"\"\"\n",
    "    h, w = original_dims\n",
    "    target_for_loss = torch.from_numpy(corrected_grid_np[:h, :w]).long().to(device)\n",
    "    failed_for_loss = torch.from_numpy(failed_grid_np[:h, :w]).long().to(device)\n",
    "\n",
    "    correction_mask = (target_for_loss != failed_for_loss).float()\n",
    "\n",
    "    if torch.sum(correction_mask) == 0:\n",
    "        print(\"  > No pixel differences found between failed and corrected grid. Skipping online update.\")\n",
    "        return\n",
    "\n",
    "    model.train()\n",
    "    arc_output_head.train()\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    target_bytes_single = serialize_and_pad_grid(corrected_grid_np, max_len=MAX_SEQUENCE_LENGTH, pad_value=PADDING_BYTE_VALUE)\n",
    "    target_bytes_np = np.frombuffer(target_bytes_single, dtype=np.uint8).copy()\n",
    "    target_bytes_tensor = torch.from_numpy(target_bytes_np).to(torch.uint8).unsqueeze(0).to(device)\n",
    "\n",
    "    train_timestep = torch.zeros(1, device=device).long()\n",
    "    output_dict = model(\n",
    "        byte_sequence=input_bytes,\n",
    "        mode='ctm_controlled_diffusion',\n",
    "        target_diffusion_output=target_bytes_tensor,\n",
    "        timestep=train_timestep,\n",
    "        task_name=\"ARC_AGI_2_ONLINE_LEARN\"\n",
    "    )\n",
    "\n",
    "    ctm_core_output_data = output_dict.get('ctm_core_data')\n",
    "    ctm_backbone_output = None\n",
    "    if ctm_core_output_data and 'final_sync_out' in ctm_core_output_data:\n",
    "        ctm_backbone_output = ctm_core_output_data['final_sync_out']\n",
    "    elif ctm_core_output_data and 'ctm_latent_representation' in ctm_core_output_data:\n",
    "        ctm_backbone_output = ctm_core_output_data['ctm_latent_representation']\n",
    "    \n",
    "    if ctm_backbone_output is None:\n",
    "        print(\"Warning: CTM core output not found. Cannot perform online update.\")\n",
    "        model.eval()\n",
    "        arc_output_head.eval()\n",
    "        return\n",
    "\n",
    "    if ctm_backbone_output.ndim > 2 and ctm_backbone_output.shape[1] > 0:\n",
    "         ctm_features_for_head_new = ctm_backbone_output.mean(dim=1)\n",
    "    else:\n",
    "         ctm_features_for_head_new = ctm_backbone_output\n",
    "\n",
    "    predicted_logits = arc_output_head(ctm_features_for_head_new)\n",
    "    predicted_logits = predicted_logits.view(1, NUM_ARC_SYMBOLS, MAX_GRID_SIZE[0], MAX_GRID_SIZE[1])\n",
    "    cropped_logits = predicted_logits[:, :, :h, :w]\n",
    "    target_for_loss_unsqueezed = target_for_loss # Target should be [H, W] for FocalLoss\n",
    "\n",
    "    # --- Enhanced Self-Learning and Loss Stabilization ---\n",
    "    # The current implementation uses FocalLoss, which is a great start.\n",
    "    # Other options to explore:\n",
    "    # 1. Contrastive Loss: Instead of predicting pixels, predict which of two grids\n",
    "    #    (the correct one vs. a distractor) is the true target. This can improve\n",
    "    #    the feature representation's quality.\n",
    "    # 2. Add a penalty term to discourage drastic changes from the original prediction,\n",
    "    #    unless they lead to a significant improvement. This can be a regularization\n",
    "    #    term on the weights or a penalty on the latent space distance.\n",
    "    #    Example: loss += lambda * ||f(new) - f(old)||^2\n",
    "\n",
    "    # Use Focal Loss to focus on hard-to-correct pixels\n",
    "    loss_fn = FocalLoss(alpha=0.5, gamma=2.0, reduction='none')\n",
    "    loss_per_pixel = loss_fn(cropped_logits, target_for_loss_unsqueezed.unsqueeze(0)) # Unsqueeze target here\n",
    "\n",
    "    # Combine with the correction mask to only learn from changed pixels\n",
    "    # To implement multi-sample loss averaging, one could run MCMC multiple times\n",
    "    # to generate a batch of corrected grids, and average the loss over that batch.\n",
    "    masked_loss = loss_per_pixel * correction_mask.unsqueeze(0)\n",
    "    loss = masked_loss.sum() / correction_mask.sum().clamp(min=1e-8)\n",
    "    base_loss_val = loss.item()\n",
    "\n",
    "    # Add stabilization loss if features from the failed prediction are provided\n",
    "    if failed_features is not None:\n",
    "        # The penalty term discourages drastic changes from the original prediction's latent space.\n",
    "        stabilization_lambda = 0.25 # Hyperparameter to balance the two loss terms\n",
    "        stabilization_loss = torch.nn.functional.mse_loss(ctm_features_for_head_new, failed_features)\n",
    "        loss += stabilization_lambda * stabilization_loss\n",
    "        print(f\"  > Combined Loss: {loss.item():.4f} (Focal: {base_loss_val:.4f}, Stabilize: {stabilization_loss.item():.4f})\")\n",
    "    else:\n",
    "        print(f\"  > Focused Loss: {base_loss_val:.4f} on {int(correction_mask.sum())} pixels.\")\n",
    "\n",
    "    loss.backward()\n",
    "    torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "    optimizer.step()\n",
    "    scheduler.step() # Step the learning rate scheduler\n",
    "\n",
    "    model.eval()\n",
    "    arc_output_head.eval()\n",
    "    print(f\"  > Model updated. LR: {scheduler.get_last_lr()[0]:.6f}\")\n",
    "\n",
    "\n",
    "class IsolatedSelfLearningEnvironment:\n",
    "    \"\"\"\n",
    "    A simulated secure container to run the self-correction and online\n",
    "    learning process, preventing accidental harm to the main evaluation loop.\n",
    "    \"\"\"\n",
    "    def __init__(self, model, arc_output_head, optimizer, device):\n",
    "        self.model = model\n",
    "        self.arc_output_head = arc_output_head\n",
    "        self.optimizer = optimizer\n",
    "        self.device = device\n",
    "        self.scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=5, gamma=0.9)\n",
    "        # Add a traceback import if it's not already global\n",
    "        global traceback\n",
    "        import traceback\n",
    "\n",
    "\n",
    "    def run_correction_and_update(self, input_bytes, failed_grid, target_grid_full, original_dims, failed_features: Optional[torch.Tensor] = None, train_pairs: Optional[List[dict]] = None):\n",
    "        \"\"\"\n",
    "        Runs an ensemble of MCMC chains for correction and an online update.\n",
    "        This implements the ensemble, consensus, and iterative refinement strategies.\n",
    "        \"\"\"\n",
    "        print(\"\\n--- Entering Isolated Self-Learning Environment (Ensemble Mode) ---\")\n",
    "        try:\n",
    "            h_orig, w_orig = original_dims\n",
    "            final_target = target_grid_full[:h_orig, :w_orig]\n",
    "\n",
    "            # --- Adaptive Hyperparameter & Meta-Learning Enhancements ---\n",
    "            # Future work: Implement dynamic adjustment of MCMC params based on acceptance rates.\n",
    "            # Future work: Use meta-learning to adapt the surrogate network to new tasks.\n",
    "            \n",
    "            # 1. Initialize the CTM-based surrogate model\n",
    "            # The `feature_extractor` could be an auxiliary network for learned similarity.\n",
    "            ctm_surrogate = CTMSurrogate(model=self.model, arc_output_head=self.arc_output_head, input_bytes_eval=input_bytes, target_grid=final_target, device=self.device)\n",
    "\n",
    "            # --- Meta-Learning Step ---\n",
    "            if train_pairs:\n",
    "                ctm_surrogate.adapt_to_new_task(train_pairs, self.optimizer)\n",
    "\n",
    "            # 2. Define MCMC config with local refinement enabled\n",
    "            mcmc_config = MCMCConfig(\n",
    "                num_chains=5, # Run 5 chains for the ensemble\n",
    "                chain_length=2000, # Shorter chains for efficiency\n",
    "                initial_temp=35.0, final_temp=0.01, burn_in=200,\n",
    "                proposal_strategy=\"hybrid\", mutation_region_size=3,\n",
    "                convergence_window=150, convergence_threshold=1e-5,\n",
    "                enable_local_refinement=True, refinement_trigger_similarity=0.95\n",
    "            )\n",
    "\n",
    "            arc_space = ARCSelfEditSpace(\n",
    "                grid_dims=(h_orig, w_orig),\n",
    "                initial_mutation_rate=0.20, # Higher initial rate for diversity\n",
    "                final_mutation_rate=0.01,\n",
    "                decay_steps=mcmc_config.chain_length\n",
    "            )\n",
    "\n",
    "            # 3. Initial state for MCMC is the failed prediction\n",
    "            initial_mcmc_state = failed_grid[:h_orig, :w_orig]\n",
    "            error_focus = (initial_mcmc_state != final_target).astype(np.float32)\n",
    "\n",
    "            # 4. Run MCMC Ensemble\n",
    "            ensemble_results = []\n",
    "            print(f\"  > Starting MCMC ensemble with {mcmc_config.num_chains} chains...\")\n",
    "            for i in range(mcmc_config.num_chains):\n",
    "                print(f\"  > Running Chain {i+1}/{mcmc_config.num_chains}...\")\n",
    "                # Introduce slight diversity in initial state for each chain\n",
    "                diverse_initial_state = initial_mcmc_state.copy()\n",
    "                if i > 0: # Mutate the initial state slightly for other chains\n",
    "                    diverse_initial_state = arc_space.get_neighbors(diverse_initial_state, step=0, n_neighbors=1)[0]\n",
    "\n",
    "                corrected_grid_cropped, _ = metropolis_hastings_sampler(\n",
    "                    initial_state=diverse_initial_state,\n",
    "                    surrogate_func=ctm_surrogate,\n",
    "                    output_space=arc_space,\n",
    "                    config=mcmc_config,\n",
    "                    error_focus=error_focus\n",
    "                )\n",
    "                ensemble_results.append(corrected_grid_cropped)\n",
    "\n",
    "            # 5. Consensus Strategy: Majority vote per pixel\n",
    "            print(\"  > Combining ensemble results via majority vote...\")\n",
    "            final_corrected_grid_cropped = self.get_consensus_grid(ensemble_results, (h_orig, w_orig))\n",
    "\n",
    "            # 6. Prepare the full-sized corrected grid\n",
    "            corrected_grid_full = np.full(MAX_GRID_SIZE, PADDING_VALUE, dtype=int)\n",
    "            corrected_grid_full[:h_orig, :w_orig] = final_corrected_grid_cropped\n",
    "\n",
    "            # 7. Always perform an online update with the consensus grid.\n",
    "            similarity = np.sum(final_corrected_grid_cropped == final_target) / final_target.size\n",
    "            print(f\"  > Consensus solution has {similarity:.2%} similarity. Committing to online update.\")\n",
    "            perform_online_update(\n",
    "                model=self.model, arc_output_head=self.arc_output_head,\n",
    "                optimizer=self.optimizer, scheduler=self.scheduler,\n",
    "                input_bytes=input_bytes, failed_grid_np=failed_grid,\n",
    "                corrected_grid_np=corrected_grid_full, original_dims=original_dims,\n",
    "                device=self.device,\n",
    "                failed_features=failed_features\n",
    "            )\n",
    "            \n",
    "            print(\"--- Exiting Isolated Environment (Update ALWAYS Performed) ---\")\n",
    "            return corrected_grid_full\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"❌ Error within Isolated Self-Learning Environment: {e}\")\n",
    "            traceback.print_exc()\n",
    "            print(\"--- Exiting Isolated Environment (Error Occurred) ---\")\n",
    "            return None\n",
    "\n",
    "    def get_consensus_grid(self, grids: List[np.ndarray], dims: tuple) -> np.ndarray:\n",
    "        \"\"\"Determines the most likely grid from an ensemble using a pixel-wise majority vote.\"\"\"\n",
    "        h, w = dims\n",
    "        final_grid = np.zeros(dims, dtype=int)\n",
    "        for r in range(h):\n",
    "            for c in range(w):\n",
    "                # Count votes for each color at this pixel\n",
    "                pixel_votes = collections.Counter(grid[r, c] for grid in grids)\n",
    "                # Choose the color with the most votes\n",
    "                final_grid[r, c] = pixel_votes.most_common(1)[0][0]\n",
    "        return final_grid\n",
    "\n",
    "\n",
    "# Setup module paths based on user-provided successful import logic\n",
    "print(\"--- Setting up module paths ---\")\n",
    "# Get the absolute path to the project root\n",
    "project_root = '/workspaces/Arc-AGI-2'\n",
    "# Define the path to the 'contineous-thought-machines' directory\n",
    "module_path = os.path.join(project_root, 'contineous-thought-machines')\n",
    "\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)\n",
    "    print(f\"Added to sys.path: {module_path}\")\n",
    "\n",
    "try:\n",
    "    from safetensors.torch import load_file\n",
    "except ImportError:\n",
    "    print(\"Warning: safetensors not found. Loading .safetensors will fail.\")\n",
    "    def load_file(path, device=\"cpu\"):\n",
    "        raise ImportError(f\"safetensors is not installed, cannot load {path}\")\n",
    "\n",
    "import importlib.util\n",
    "\n",
    "# --- Statically Importing EnhancedCTMDiffusion model ---\n",
    "print(\"\\n--- Statically importing EnhancedCTMDiffusion model ---\")\n",
    "EnhancedCTMDiffusion = None\n",
    "try:\n",
    "    from models.ctm_Diffusion_NEWNEW import EnhancedCTMDiffusion\n",
    "    print(\" -> Successfully imported EnhancedCTMDiffusion from models package.\")\n",
    "except ImportError as e_direct:\n",
    "    print(f\"FATAL: Import from models package failed. Last error: {e_direct}\")\n",
    "    EnhancedCTMDiffusion = None # Ensure it's None on failure\n",
    "\n",
    "try:\n",
    "    from accelerate import Accelerator\n",
    "    ACCELERATE_AVAILABLE = True\n",
    "except ImportError:\n",
    "    print(\"Warning: Hugging Face Accelerate not found. Will run on a single device.\")\n",
    "    ACCELERATE_AVAILABLE = False\n",
    "    Accelerator = None\n",
    "\n",
    "# --- Constants and Configuration ---\n",
    "# These are gathered from your setup script to make this file runnable\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "MAX_GRID_SIZE = (30, 30)\n",
    "PADDING_VALUE = -1 # A common padding value for ARC\n",
    "ARC_INPUT_FLAT_DIM = MAX_GRID_SIZE[0] * MAX_GRID_SIZE[1]\n",
    "MAX_SEQUENCE_LENGTH = 8192\n",
    "PADDING_BYTE_VALUE = 0\n",
    "NUM_ARC_SYMBOLS = 10 # 0-9\n",
    "ARC_OUTPUT_HEAD_DIM = ARC_INPUT_FLAT_DIM * NUM_ARC_SYMBOLS\n",
    "LEARNING_RATE = 1e-4\n",
    "\n",
    "# --- Your Provided Setup Code ---\n",
    "\n",
    "# ## Data Handling ##\n",
    "def pad_grid(grid_list, max_dims, pad_value):\n",
    "    grid_np = np.array(grid_list, dtype=np.int32)\n",
    "    padded_grid = np.full(max_dims, pad_value, dtype=np.int32)\n",
    "    h, w = grid_np.shape\n",
    "    padded_grid[:h, :w] = grid_np\n",
    "    return padded_grid\n",
    "\n",
    "def serialize_and_pad_grid(grid, max_len=MAX_SEQUENCE_LENGTH, pad_value=PADDING_BYTE_VALUE):\n",
    "    flat_array = np.array(grid, dtype=np.uint8).flatten()\n",
    "    byte_sequence = flat_array.tobytes()\n",
    "    padding_len = max_len - len(byte_sequence)\n",
    "    if padding_len < 0:\n",
    "        return byte_sequence[:max_len]\n",
    "    return byte_sequence + bytes([pad_value] * padding_len)\n",
    "\n",
    "# Define EnhancedCTMConfig for ARC with EnhancedCTMDiffusion\n",
    "# Assuming EnhancedCTMConfig is a defined class and MAX_SEQUENCE_LENGTH is a defined variable\n",
    "# For example:\n",
    "# from your_model_library import EnhancedCTMConfig\n",
    "# MAX_SEQUENCE_LENGTH = 8192\n",
    "\n",
    "# From contineous-thought-machines/models/constants.py\n",
    "VALID_NEURON_SELECT_TYPES = [\n",
    "    'first-last', 'random', 'random-pairing',  # Legacy\n",
    "    # Biologically-inspired types\n",
    "    'bio_hebbian', 'bio_plasticity', 'bio_competitive', 'bio_homeostatic',\n",
    "    'bio_evolutionary', 'bio_stdp', 'bio_criticality', 'bio_multi_objective',\n",
    "    # Hybrid approaches\n",
    "    'adaptive_random', 'performance_guided', 'task_aware'\n",
    "]\n",
    "\n",
    "VALID_POSITIONAL_EMBEDDING_TYPES = [\n",
    "    'learnable-fourier', 'multi-learnable-fourier',\n",
    "    'custom-rotational', 'custom-rotational-1d'\n",
    "]\n",
    "\n",
    "# From contineous-thought-machines/models/ctm_Diffusion_NEWNEW.py\n",
    "from dataclasses import dataclass, field\n",
    "from typing import Dict, Optional, Tuple, Union, Any, List\n",
    "\n",
    "@dataclass\n",
    "class EnhancedCTMConfig: # Renamed from ContinualLearningConfig for consistency in the target file\n",
    "    \"\"\"Enhanced configuration for continual learning CTM-diffusion model,\n",
    "    incorporating binary processing, multi-task learning, and advanced CTM features.\"\"\"\n",
    "    \n",
    "    # Model architecture (General Transformer/Diffusion settings)\n",
    "    d_model: int = 512  # Main model dimensionality\n",
    "    n_heads: int = 8\n",
    "    n_layers: int = 24\n",
    "    max_sequence_length: int = 8192 # Max input sequence length in terms of bytes or patches\n",
    "    dropout: float = 0.1\n",
    "    \n",
    "    # --- Byte Processing Options ---\n",
    "    patch_embedding_dim: int = 256         # <<< NEW: Output embedding dimension per patch from patcher\n",
    "    patch_encoder_cnn_channels: int = 64   # <<< NEW: Intermediate channels for CNN patch encoder\n",
    "\n",
    "    # --- Dynamic Entropy Patching Options (Inspired by BLT paper) ---\n",
    "    use_dynamic_entropy_patcher: bool = True # Flag to enable dynamic entropy-based patching\n",
    "    entropy_patcher_threshold_type: str = \"global\"  # 'global' or 'relative_monotonic'\n",
    "    entropy_patcher_global_threshold: float = 0.75 # Entropy threshold for 'global' type\n",
    "    entropy_patcher_relative_threshold: float = 0.1 # Entropy diff threshold for 'relative_monotonic'\n",
    "    entropy_patcher_min_patch_size: int = 4      # Minimum number of bytes in a dynamic patch\n",
    "    entropy_patcher_max_patch_size: int = 128    # Maximum number of bytes in a dynamic patch (for CNN encoder)\n",
    "    \n",
    "    # --- Learnable Entropy Model Parameters (for _EntropyProxyModel) ---\n",
    "    entropy_model_byte_vocab_size: int = 256\n",
    "    entropy_model_embedding_dim: int = 64\n",
    "    entropy_model_hidden_dim: int = 128\n",
    "    entropy_model_num_layers: int = 1\n",
    "    entropy_model_dropout: float = 0.1\n",
    "    entropy_model_loss_weight: float = 0.1 # Weight for its auxiliary loss contribution\n",
    "    # Note: These parameters are used if use_dynamic_entropy_patcher is True,\n",
    "    # as LearnedBytePatcherEncoder now instantiates the learnable _EntropyProxyModel.\n",
    "    \n",
    "    # Fallback if not using learned_patch_encoder or dynamic_entropy_patcher\n",
    "    byte_embedding_dim: int = 256\n",
    "    multi_granularity: bool = False # Default to False if patcher is preferred\n",
    "    # multi_granularity_output_dim is complex to predefine, MGP should expose its output dim.\n",
    "    # For now, if multi_granularity is True AND use_learned_patch_encoder is False, this would be used.\n",
    "    multi_granularity_output_dim: int = 256 # Placeholder if MGP is used.\n",
    "    \n",
    "    hierarchical_processing: bool = True # General flag, could apply to patcher or MGP\n",
    "    \n",
    "    # CTM Core Parameters (Specific to the OriginalCTMCore module)\n",
    "    # These are prefixed with 'ctm_' to distinguish from general model params\n",
    "    ctm_iterations: int = 5  # Original 'iterations'\n",
    "    ctm_d_model: int = 512   # Original 'd_model' for CTM's internal latent space\n",
    "    ctm_input_dim: int = 256 # Dimensionality of inputs to CTM (e.g., from byte embeddings or other features)\n",
    "                             # This was 'd_input' in OriginalCTMCore if it took external features.\n",
    "                             # If CTM processes outputs of byte_embedding, this might be byte_embedding_dim.\n",
    "    ctm_heads: int = 8       # Attention heads within CTM\n",
    "    ctm_n_synch_out: int = 64\n",
    "    ctm_n_synch_action: int = 64\n",
    "    ctm_synapse_depth: int = 3\n",
    "    ctm_memory_length: int = 10\n",
    "    ctm_deep_nlms: bool = True\n",
    "    ctm_memory_hidden_dims: int = 2048\n",
    "    ctm_do_layernorm_nlm: bool = False\n",
    "    ctm_out_dims: int = 512  # Output dimension of CTM's own projector\n",
    "    ctm_prediction_reshaper: list = field(default_factory=lambda: [-1])\n",
    "    ctm_dropout: float = 0.1\n",
    "    ctm_dropout_nlm: Optional[float] = None\n",
    "    # Neuron selection strategy. Available options:\n",
    "    # Legacy: 'first-last', 'random', 'random-pairing'\n",
    "    # Biologically-inspired: 'bio_hebbian', 'bio_plasticity', 'bio_competitive',\n",
    "    #                        'bio_homeostatic', 'bio_evolutionary', 'bio_stdp',\n",
    "    #                        'bio_criticality', 'bio_multi_objective'\n",
    "    # Hybrid: 'adaptive_random', 'performance_guided', 'task_aware'\n",
    "    ctm_neuron_select_type: str = 'bio_multi_objective'\n",
    "    ctm_n_random_pairing_self: int = 0\n",
    "    \n",
    "    # Diffusion Parameters\n",
    "    diffusion_steps: int = 1000\n",
    "    noise_schedule: str = \"cosine\" # e.g., \"linear\", \"cosine\"\n",
    "    diffusion_beta_start: float = 0.0001\n",
    "    diffusion_beta_end: float = 0.02\n",
    "    diffusion_timesteps: int = 1000 # Number of timesteps for the diffusion process\n",
    "    ctm_diffusion_coupling_strength: float = 0.8 # How CTM influences diffusion\n",
    "    adaptive_scheduling: bool = True  # CTM-adaptive diffusion timestep scheduling\n",
    "    iterative_refinement: bool = True # Iterative CTM-diffusion refinement for sampling\n",
    "    \n",
    "\n",
    "    \n",
    "    # Training Efficiency\n",
    "    mixed_precision: bool = True\n",
    "    gradient_checkpointing: bool = True\n",
    "    sparse_attention: bool = True  # Now implemented with BinarySparseAttention\n",
    "    adaptive_depth: bool = False   # Defaulting to False, can be enabled if implemented\n",
    "    \n",
    "    # Sparse Attention Parameters\n",
    "    sparse_attention_ratio: float = 0.1  # Keep only 10% of attention connections\n",
    "    binary_pattern_size: int = 8  # Size of binary patterns to detect\n",
    "\n",
    "    # Attention Mechanism Type\n",
    "    attention_type: str = \"subquadratic\"  # Options: \"standard\", \"binary_sparse\", \"subquadratic\"\n",
    "    \n",
    "    # Subquadratic Attention Parameters (if attention_type is \"subquadratic\")\n",
    "    subquadratic_attn_epsilon: float = 1e-6\n",
    "    subquadratic_attn_poly_degree: int = 5\n",
    "    attention_qkv_bias: bool = True # General QKV bias for attention mechanisms like Subquadratic or standard MHA\n",
    "    # attn_drop and proj_drop for subquadratic_attn will be mapped from ctm_dropout\n",
    "\n",
    "    # Positional Embedding Parameters\n",
    "    positional_embedding_type: Optional[str] = 'multi-learnable-fourier' # e.g., 'custom-rotational-1d', 'learnable-fourier', multi-learnable-fourier' #Can set the value here. \n",
    "    positional_embedding_dim: Optional[int] = None  # Dimension of the positional embedding, defaults to ctm_input_dim if None\n",
    "    reshape_patch_sequence_to_grid: bool = True # If True, reshape patch sequence to a 2D grid for 2D PEs. Must set to true if using 2D Grid for Positional Embeddings.\n",
    "    patch_grid_width: Optional[int] = None       # Desired width of the patch grid if reshaping\n",
    "\n",
    "    # Pipeline Parallelism Parameters\n",
    "    enable_pipeline_parallelism: bool = True\n",
    "    pipeline_stages: int = 4  # CTM, MCMC, Diffusion prep, Diffusion exec\n",
    "    pipeline_overlap_ratio: float = 0.7  # Target overlap ratio\n",
    "    \n",
    "    # Adaptive Batch Sizing Parameters\n",
    "    enable_adaptive_batching: bool = True\n",
    "    initial_batch_size: int = 32\n",
    "    min_batch_size: int = 8\n",
    "    max_batch_size: int = 256\n",
    "    batch_adaptation_frequency: int = 100\n",
    "    memory_threshold_high: float = 0.85\n",
    "    memory_threshold_low: float = 0.6\n",
    "    \n",
    "    # Smart Data Sampling Parameters\n",
    "    enable_smart_sampling: bool = True\n",
    "    sample_importance_weight: float = 0.6\n",
    "    sample_diversity_weight: float = 0.4\n",
    "    initial_sample_ratio: float = 0.3\n",
    "    complexity_analysis_enabled: bool = True\n",
    "    \n",
    "    # Multi-input/output parameters\n",
    "    num_inputs: int = 1  # Number of input streams\n",
    "    num_outputs: int = 1  # Number of output heads\n",
    "    output_dims: List[int] = field(default_factory=lambda: [64])  # Dimensions for each output head\n",
    "    \n",
    "    # Self-supervised learning\n",
    "    ssl_dim: int = 128  # Dimension for self-supervised projection\n",
    "    ssl_weight: float = 0.1  # Weight for self-supervised loss\n",
    "    ssl_temperature: float = 0.07  # Temperature for contrastive loss\n",
    "    ssl_noise_std: float = 0.1  # Noise standard deviation for contrastive augmentation\n",
    "    \n",
    "    # Spatiotemporal Processing\n",
    "    use_spatial: bool = True  # Enable spatial processing for image/video data\n",
    "    \n",
    "    # WINA Attention\n",
    "    use_wina_attention: bool = True  # Enable WINA sparse attention\n",
    "    \n",
    "    # Multi-task Learning Parameters\n",
    "    max_tasks: int = 50  # Maximum number of tasks for continual learning\n",
    "    # Added to resolve TypeError for unexpected keyword arguments\n",
    "    vocab_size: Optional[int] = None\n",
    "    output_audio_bytes: bool = False\n",
    "    inferred_task_latent_dim: Optional[int] = None # Default to None, __post_init__ handles it\n",
    "    use_hipa_attention: bool = False # Default to False\n",
    "    hipa_num_heads: Optional[int] = None # Default to None\n",
    "    audio_output_dtype_str: Optional[str] = \"float32\" # Default as per __post_init__ logic\n",
    "    unet_input_feature_dim: Optional[int] = None # Default to None, __post_init__ calculates it\n",
    "\n",
    "    # --- JEPA Training Parameters (Integrated with LearnedBytePatcherEncoder) ---\n",
    "    use_jepa_training: bool = False\n",
    "    # jepa_embed_dim will be derived from patch_embedding_dim if dynamic_entropy_patcher is used\n",
    "    jepa_predictor_hidden_dim: int = 512 # Hidden dimension of JEPA predictor MLP\n",
    "    jepa_mask_ratio_min: float = 0.15 # Min proportion of patch sequence to mask for target\n",
    "    jepa_mask_ratio_max: float = 0.75 # Max proportion of patch sequence to mask for target\n",
    "    jepa_context_scale_min: float = 0.3 # Min proportion of patches for context\n",
    "    jepa_context_scale_max: float = 0.7 # Max proportion of patches for context\n",
    "    jepa_momentum_beta: float = 0.996 # Momentum for target encoder update\n",
    "    jepa_loss_weight: float = 0.1 # Weight for the JEPA loss component\n",
    "    jepa_num_target_blocks: int = 1 # Number of target blocks to predict\n",
    "\n",
    "     # --- Global Plasticity Loss Parameters ---\n",
    "    use_global_plasticity_loss: bool = True\n",
    "    global_plasticity_loss_weight: float = 0.05\n",
    "    local_neuron_selector_loss_weight: float = 0.1\n",
    "    target_hebbian_pattern: float = 0.0 # Target for the aggregated Hebbian signal\n",
    "\n",
    "    # --- Knowledge Store Parameters ---\n",
    "\n",
    "    def __post_init__(self):\n",
    "        # Validate output dimensions\n",
    "        if len(self.output_dims) != self.num_outputs:\n",
    "            raise ValueError(f\"output_dims length ({len(self.output_dims)}) must match num_outputs ({self.num_outputs})\")\n",
    "\n",
    "        # Merged content from the second __post_init__\n",
    "        if hasattr(self, 'ctm_prediction_reshaper') and self.ctm_prediction_reshaper == [-1] and self.vocab_size is not None:\n",
    "            pass\n",
    "        if hasattr(self, 'ctm_dropout_nlm') and self.ctm_dropout_nlm is None and hasattr(self, 'ctm_dropout'):\n",
    "            self.ctm_dropout_nlm = self.ctm_dropout\n",
    "        if hasattr(self, 'mcmc_output_space_dim') and self.mcmc_output_space_dim is None and hasattr(self, 'ctm_out_dims'):\n",
    "            self.mcmc_output_space_dim = self.ctm_out_dims\n",
    "        \n",
    "        if hasattr(self, 'ctm_neuron_select_type') and \\\n",
    "           VALID_NEURON_SELECT_TYPES is not None and self.ctm_neuron_select_type not in VALID_NEURON_SELECT_TYPES:\n",
    "            print(f\"Warning: ctm_neuron_select_type '{self.ctm_neuron_select_type}' is not in VALID_NEURON_SELECT_TYPES ({VALID_NEURON_SELECT_TYPES}).\")\n",
    "\n",
    "        if hasattr(self, 'positional_embedding_type') and self.positional_embedding_type is not None:\n",
    "            if VALID_POSITIONAL_EMBEDDING_TYPES is None: # Fallback if import failed\n",
    "                print(f\"Warning: VALID_POSITIONAL_EMBEDDING_TYPES not available for validation.\")\n",
    "            elif self.positional_embedding_type not in VALID_POSITIONAL_EMBEDDING_TYPES:\n",
    "                print(f\"Warning: positional_embedding_type '{self.positional_embedding_type}' is not in VALID_POSITIONAL_EMBEDDING_TYPES ({VALID_POSITIONAL_EMBEDDING_TYPES}).\")\n",
    "            if self.positional_embedding_dim is not None and self.positional_embedding_dim <= 0:\n",
    "                raise ValueError(\"positional_embedding_dim must be positive if set.\")\n",
    "            \n",
    "            if self.reshape_patch_sequence_to_grid:\n",
    "                if self.patch_grid_width is None or self.patch_grid_width <= 0:\n",
    "                    raise ValueError(\"patch_grid_width must be a positive integer if reshape_patch_sequence_to_grid is True.\")\n",
    "                if self.positional_embedding_type not in ['learnable-fourier', 'multi-learnable-fourier', 'custom-rotational']:\n",
    "                    print(f\"Warning: reshape_patch_sequence_to_grid is True, but positional_embedding_type ('{self.positional_embedding_type}') is not a typical 2D PE. Ensure compatibility.\")\n",
    "\n",
    "        # Validations for new patch encoder\n",
    "        if self.use_dynamic_entropy_patcher:\n",
    "            if self.patch_embedding_dim <= 0:\n",
    "                raise ValueError(\"patch_embedding_dim must be positive if use_dynamic_entropy_patcher is True.\")\n",
    "            if self.entropy_patcher_min_patch_size <= 0:\n",
    "                raise ValueError(\"entropy_patcher_min_patch_size must be positive.\")\n",
    "            if self.entropy_patcher_max_patch_size < self.entropy_patcher_min_patch_size:\n",
    "                raise ValueError(\"entropy_patcher_max_patch_size must be >= entropy_patcher_min_patch_size.\")\n",
    "            if self.entropy_patcher_threshold_type not in [\"global\", \"relative_monotonic\"]:\n",
    "                raise ValueError(\"entropy_patcher_threshold_type must be 'global' or 'relative_monotonic'.\")\n",
    "        elif self.multi_granularity and self.multi_granularity_output_dim <= 0:\n",
    "            print(\"Warning: multi_granularity_output_dim might not be correctly set for validation if not using a patcher and MGP is active.\")\n",
    "        \n",
    "        if not hasattr(self, 'inferred_task_latent_dim') or self.inferred_task_latent_dim is None:\n",
    "            print(\"Warning: inferred_task_latent_dim not found or is None in config, defaulting to 64.\")\n",
    "            self.inferred_task_latent_dim = 512\n",
    "        elif self.inferred_task_latent_dim <= 0: # This check is now safe\n",
    "            raise ValueError(\"inferred_task_latent_dim must be positive.\")\n",
    " \n",
    "        if hasattr(self, 'use_hipa_attention') and self.use_hipa_attention and \\\n",
    "            (not hasattr(self, 'hipa_num_heads') or self.hipa_num_heads <= 0):\n",
    "             raise ValueError(\"hipa_num_heads must be positive if use_hipa_attention is True.\")\n",
    " \n",
    "        if hasattr(self, 'audio_output_dtype_str'):\n",
    "            if self.audio_output_dtype_str == \"float32\":\n",
    "                self.audio_output_item_size = 4\n",
    "            elif self.audio_output_dtype_str == \"int16\":\n",
    "                self.audio_output_item_size = 2\n",
    "            else:\n",
    "                if hasattr(self, 'output_audio_bytes') and self.output_audio_bytes:\n",
    "                    raise ValueError(f\"Unsupported audio_output_dtype_str: {self.audio_output_dtype_str} when output_audio_bytes is True.\")\n",
    "                else:\n",
    "                    self.audio_output_item_size = 4\n",
    "        elif hasattr(self, 'output_audio_bytes') and self.output_audio_bytes:\n",
    "            if not hasattr(self, 'audio_output_dtype_str') or self.audio_output_dtype_str is None:\n",
    "                raise ValueError(\"audio_output_dtype_str must be defined in config if output_audio_bytes is True.\")\n",
    "        else:\n",
    "            self.audio_output_item_size = 4\n",
    "\n",
    "        # Calculate unet_input_feature_dim if not set\n",
    "        if self.unet_input_feature_dim is None:\n",
    "            if self.max_sequence_length <= 0 or self.audio_output_item_size <= 0:\n",
    "                raise ValueError(\"max_sequence_length and audio_output_item_size must be positive to calculate unet_input_feature_dim.\")\n",
    "            self.unet_input_feature_dim = self.max_sequence_length // self.audio_output_item_size\n",
    "            if self.unet_input_feature_dim <= 0:\n",
    "                raise ValueError(f\"Calculated unet_input_feature_dim ({self.unet_input_feature_dim}) must be positive. Check max_sequence_length and audio_output_item_size.\")\n",
    "        elif self.unet_input_feature_dim <= 0:\n",
    "            raise ValueError(\"unet_input_feature_dim, if set, must be positive.\")\n",
    "\n",
    "        if self.use_jepa_training:\n",
    "            if not (0 < self.jepa_mask_ratio_min < 1 and 0 < self.jepa_mask_ratio_max < 1 and self.jepa_mask_ratio_min <= self.jepa_mask_ratio_max):\n",
    "                raise ValueError(\"JEPA mask ratios must be between 0 and 1, with min <= max.\")\n",
    "            if not (0 < self.jepa_context_scale_min < 1 and 0 < self.jepa_context_scale_max < 1 and self.jepa_context_scale_min <= self.jepa_context_scale_max):\n",
    "                raise ValueError(\"JEPA context scales must be between 0 and 1, with min <= max.\")\n",
    "            if not (0 <= self.jepa_momentum_beta < 1):\n",
    "                raise ValueError(\"jepa_momentum_beta must be between 0 and 1.\")\n",
    "            if self.jepa_num_target_blocks <= 0:\n",
    "                raise ValueError(\"jepa_num_target_blocks must be positive.\")\n",
    "            if not self.use_dynamic_entropy_patcher:\n",
    "                print(\"Warning: JEPA training is enabled but use_dynamic_entropy_patcher is False. JEPA relies on the patch embeddings from LearnedBytePatcherEncoder.\")\n",
    "\n",
    "# Define EnhancedCTMConfig for ARC with EnhancedCTMDiffusion\n",
    "config_arc_diffusion = EnhancedCTMConfig(\n",
    "    d_model=512,\n",
    "    #inferred_task_latent_dim=64, # This line remains commented out\n",
    "    n_heads=8,\n",
    "    n_layers=24, \n",
    "    max_sequence_length=MAX_SEQUENCE_LENGTH,\n",
    "    dropout=0.1,\n",
    "    use_dynamic_entropy_patcher=True,\n",
    "    patch_embedding_dim=256,\n",
    "    patch_grid_width=16,\n",
    "    patch_encoder_cnn_channels=64,\n",
    "    entropy_patcher_threshold_type=\"global\",\n",
    "    entropy_patcher_global_threshold=0.75,\n",
    "    entropy_patcher_relative_threshold=0.1,\n",
    "    entropy_patcher_min_patch_size=4,\n",
    "    entropy_patcher_max_patch_size=128,\n",
    "    # Parameters for the learnable entropy model within LearnedBytePatcherEncoder\n",
    "    entropy_model_byte_vocab_size=256,\n",
    "    entropy_model_embedding_dim=64,\n",
    "    entropy_model_hidden_dim=128,\n",
    "    entropy_model_num_layers=1,\n",
    "    entropy_model_dropout=0.1,\n",
    "    entropy_model_loss_weight=0.1,\n",
    "    \n",
    "    ctm_input_dim=256,\n",
    "    ctm_d_model=512,\n",
    "    ctm_iterations=5,\n",
    "    ctm_heads=8,\n",
    "    ctm_out_dims=512,\n",
    "    ctm_neuron_select_type='bio_multi_objective',\n",
    "    \n",
    "    # Attention Mechanism Type\n",
    "    attention_type=\"subquadratic\",  # Options: \"standard\", \"binary_sparse\", \"subquadratic\"\n",
    "    \n",
    "    # Subquadratic Attention Parameters\n",
    "    subquadratic_attn_epsilon=1e-6,\n",
    "    subquadratic_attn_poly_degree=5,\n",
    "    attention_qkv_bias=True, # Corrected capitalization\n",
    "    \n",
    "    # Positional Embedding Parameters\n",
    "    positional_embedding_type='multi-learnable-fourier',\n",
    "    positional_embedding_dim=None,\n",
    "    reshape_patch_sequence_to_grid=True,\n",
    "    #patch_grid_width=None, #Already defined in the byte patch section of this config. \n",
    "\n",
    "    # Pipeline Parallelism Parameters\n",
    "    enable_pipeline_parallelism=True,\n",
    "    pipeline_stages=4,\n",
    "    pipeline_overlap_ratio=0.7,\n",
    "    \n",
    "    # Adaptive Batch Sizing Parameters\n",
    "    enable_adaptive_batching=True,\n",
    "    initial_batch_size=32,\n",
    "    min_batch_size=8,\n",
    "    max_batch_size=256,\n",
    "    batch_adaptation_frequency=100,\n",
    "    memory_threshold_high=0.85,\n",
    "    memory_threshold_low=0.6,\n",
    "    \n",
    "    # Smart Data Sampling Parametersa\n",
    "    enable_smart_sampling=True,\n",
    "    sample_importance_weight=0.6,\n",
    "    sample_diversity_weight=0.4,\n",
    "    initial_sample_ratio=0.3,\n",
    "    complexity_analysis_enabled=True,\n",
    "    \n",
    "    # Multi-input/output parameters\n",
    "    num_inputs=1,\n",
    "    num_outputs=1,\n",
    "    output_dims=[64],  # Directly pass the list value\n",
    "    \n",
    "    # Self-supervised learning\n",
    "    ssl_dim=128,\n",
    "    ssl_weight=0.1,\n",
    "    ssl_temperature=0.07,\n",
    "    ssl_noise_std=0.1,\n",
    "    \n",
    "    # Spatiotemporal Processing\n",
    "    use_spatial=True,\n",
    "    \n",
    "    # WINA Attention\n",
    "    use_wina_attention=True,\n",
    "    \n",
    "    # Multi-task Learning Parameters\n",
    "    max_tasks=50,\n",
    "    diffusion_steps=1000,\n",
    "    ctm_diffusion_coupling_strength=0.8,\n",
    "    vocab_size=None,\n",
    "    #enable_enhanced_mcmc=False, #ONLY USE THE ARC_AGI NOTEBOOK VERSION AND NOT THE ONE IMPORTED FROM THE DIFFUSION_NEWNEW file (This needs to be false). This flie cannot use this variable.\n",
    "    #mcmc_config=MCMC_CONFIG_ARC, #I don't think this is needed. \n",
    "    output_audio_bytes=False\n",
    ")\n",
    "\n",
    "print(\"✓ EnhancedCTMConfig for ARC (config_arc_diffusion) created.\")\n",
    "\n",
    "if 'enhanced_ctm_mcmc' not in globals():\n",
    "    print(\"Warning: 'enhanced_ctm_mcmc' not found in globals. Defaulting to None. Ensure the cell defining it (approx. lines 1820-1866) was run successfully.\")\n",
    "    enhanced_ctm_mcmc = None\n",
    "    \n",
    "if 'EnhancedCTMDiffusion' in globals() and EnhancedCTMDiffusion is not None:\n",
    "    ctm_model_arc = EnhancedCTMDiffusion(config=config_arc_diffusion).to(device)\n",
    "    print(\"✓ EnhancedCTMDiffusion model for ARC (ctm_model_arc) initialized.\")\n",
    "\n",
    "    # The external ARC output head will take features from the CTM core part of EnhancedCTMDiffusion\n",
    "    arc_output_head_input_dim = config_arc_diffusion.ctm_out_dims\n",
    "    arc_output_head = nn.Linear(arc_output_head_input_dim, ARC_OUTPUT_HEAD_DIM).to(device)\n",
    "    print(f\"✓ ARC Output Head initialized (input_dim: {arc_output_head_input_dim}, output_dim: {ARC_OUTPUT_HEAD_DIM}).\")\n",
    "\n",
    "    # Handle external MCMC integration if enabled\n",
    "    if 'enhanced_ctm_mcmc' in globals() and ENABLE_CTM_MCMC_INTEGRATION_FOR_ARC and enhanced_ctm_mcmc:\n",
    "        # Ensure the external MCMC module's input_dim matches the new CTM's output\n",
    "        if enhanced_ctm_mcmc.thought_network[0].in_features != config_arc_diffusion.ctm_out_dims:\n",
    "            print(f\"Re-initializing external enhanced_ctm_mcmc for new input_dim {config_arc_diffusion.ctm_out_dims}\")\n",
    "            # This part of the code assumes 'EnhancedCTMFenchelYoungIntegration' and other related variables are defined.\n",
    "            # If not, this will raise an error, which is expected behavior if setup is wrong.\n",
    "            enhanced_ctm_mcmc = EnhancedCTMFenchelYoungIntegration(\n",
    "                input_dim=config_arc_diffusion.ctm_out_dims,\n",
    "                output_space=arc_grid_output_space,\n",
    "                mcmc_config=MCMC_CONFIG_ARC,\n",
    "                use_large_neighborhood_search=True,\n",
    "                lns_frequency=5,\n",
    "                lns_neighborhood_size=10\n",
    "            )\n",
    "        ctm_mcmc_integration_arc = enhanced_ctm_mcmc.to(device) if enhanced_ctm_mcmc else None\n",
    "        print(f\"✓ External MCMC Integration for ARC is {'enabled' if ctm_mcmc_integration_arc else 'FAILED to enable'}.\")\n",
    "    else:\n",
    "        ctm_mcmc_integration_arc = None\n",
    "\n",
    "    arc_trainable_params = list(ctm_model_arc.parameters())\n",
    "    if arc_output_head:\n",
    "        arc_trainable_params.extend(list(arc_output_head.parameters()))\n",
    "    if ctm_mcmc_integration_arc:\n",
    "        arc_trainable_params.extend(list(ctm_mcmc_integration_arc.parameters()))\n",
    "\n",
    "    optimizer_arc = optim.AdamW([p for p in arc_trainable_params if p.requires_grad], lr=LEARNING_RATE, weight_decay=1e-4)\n",
    "\n",
    "    if ACCELERATE_AVAILABLE:\n",
    "        print(\" -> Preparing components with Hugging Face Accelerate...\")\n",
    "        accelerator_arc = Accelerator()\n",
    "        components_to_prepare = [ctm_model_arc, optimizer_arc]\n",
    "        if arc_output_head:\n",
    "            components_to_prepare.insert(1, arc_output_head)\n",
    "        if ctm_mcmc_integration_arc:\n",
    "            components_to_prepare.insert(2, ctm_mcmc_integration_arc)\n",
    "        \n",
    "        prepared_components = accelerator_arc.prepare(*components_to_prepare)\n",
    "        \n",
    "        # Unpack the prepared components carefully\n",
    "        ctm_model_arc = prepared_components[0]\n",
    "        next_idx = 1\n",
    "        if arc_output_head:\n",
    "            arc_output_head = prepared_components[next_idx]\n",
    "            next_idx += 1\n",
    "        if ctm_mcmc_integration_arc:\n",
    "            ctm_mcmc_integration_arc = prepared_components[next_idx]\n",
    "            next_idx += 1\n",
    "        optimizer_arc = prepared_components[next_idx]\n",
    "\n",
    "        print(\"✓ ARC models and optimizer prepared with Accelerate.\")\n",
    "else:\n",
    "    print(\"⚠️ Hugging Face Accelerate not available. Running on a single device.\")\n",
    "\n",
    "def find_directory(start_path, dir_name):\n",
    "    \"\"\"Recursively finds a directory by name.\"\"\"\n",
    "    for root, dirs, _ in os.walk(start_path):\n",
    "        if dir_name in dirs:\n",
    "            found_path = os.path.join(root, dir_name)\n",
    "            print(f\"Found '{dir_name}' directory at: {found_path}\")\n",
    "            return found_path\n",
    "    return None\n",
    "\n",
    "def find_file_directory(start_path, filename):\n",
    "    \"\"\"Recursively finds a file and returns its directory.\"\"\"\n",
    "    for root, _, files in os.walk(start_path):\n",
    "        if filename in files:\n",
    "            found_dir = os.path.abspath(root)\n",
    "            print(f\"Found '{filename}' in directory: {found_dir}\")\n",
    "            return found_dir\n",
    "    print(f\"Warning: File '{filename}' not found starting from '{start_path}'.\")\n",
    "    \n",
    "print(\"\\n--- Searching for evaluation and checkpoint directories ---\")\n",
    "# Path to ARC evaluation tasks\n",
    "# Path to CTM checkpoints\n",
    "CHECKPOINT_DIR_ARC_SEARCHED = find_directory(\".\", \"ctm_arc_agi_2_enhanced_diffusion\")\n",
    "\n",
    "# --- Search for the specific evaluation file to determine the data directory ---\n",
    "\n",
    "dynamic_eval_dir =  eval_dir \n",
    "CHECKPOINT_DIR_ARC = CHECKPOINT_DIR_ARC_SEARCHED if CHECKPOINT_DIR_ARC_SEARCHED else os.path.join(\"checkpoints\", \"ctm_arc_agi_2_enhanced_diffusion\")\n",
    "\n",
    "if not CHECKPOINT_DIR_ARC_SEARCHED:\n",
    "    print(f\"-> Checkpoint directory not found dynamically, using fallback: '{CHECKPOINT_DIR_ARC}'\")\n",
    "\n",
    "NUM_EPOCHS_ARC = 20\n",
    "\n",
    "\n",
    "\n",
    "class NewCustomARCGridDataset(Dataset):\n",
    "    def __init__(self, data_path, max_grid_size=MAX_GRID_SIZE, padding_value=PADDING_VALUE):\n",
    "        self.data_path = data_path\n",
    "        self.task_files = []\n",
    "        if os.path.isfile(data_path):\n",
    "            if data_path.endswith(\".json\"):\n",
    "                self.task_files.append(data_path)\n",
    "        elif os.path.isdir(data_path):\n",
    "            for root, _, files in os.walk(data_path):\n",
    "                for file in files:\n",
    "                    if file.endswith(\".json\"):\n",
    "                        self.task_files.append(os.path.join(root, file))\n",
    "        else:\n",
    "            print(f\"Error: Provided data path does not exist or is not a file/directory: {data_path}\")\n",
    "            self.tasks = []\n",
    "            return\n",
    "\n",
    "        if not self.task_files:\n",
    "            print(f\"Warning: No .json files found at path: {data_path}\")\n",
    "            self.tasks = []\n",
    "            return\n",
    "        \n",
    "        self.max_grid_size = max_grid_size\n",
    "        self.padding_value = padding_value\n",
    "        self.tasks = [json.load(open(f)) for f in self.task_files]\n",
    "        print(f\"Loaded {len(self.tasks)} tasks from {data_path}.\")\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.tasks)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        task_data = self.tasks[idx]\n",
    "        processed_task = {'train': [], 'test': [], 'id': os.path.basename(self.task_files[idx])}\n",
    "        for pair_type in ['train', 'test']:\n",
    "            for item in task_data.get(pair_type, []):\n",
    "                input_grid = item['input']\n",
    "                output_grid = item['output']\n",
    "                original_input_dims = (len(input_grid), len(input_grid[0]) if input_grid else 0)\n",
    "                original_output_dims = (len(output_grid), len(output_grid[0]) if output_grid else 0)\n",
    "                padded_input = pad_grid(input_grid, self.max_grid_size, self.padding_value)\n",
    "                padded_output = pad_grid(output_grid, self.max_grid_size, self.padding_value)\n",
    "                processed_task[pair_type].append({\n",
    "                    'input': torch.from_numpy(padded_input).long(),\n",
    "                    'output': torch.from_numpy(padded_output).long(),\n",
    "                    'original_input_dims': original_input_dims,\n",
    "                    'original_output_dims': original_output_dims\n",
    "                })\n",
    "        return processed_task\n",
    "\n",
    "# --- Safetensors loading fix ---\n",
    "# The load_file_safely function has been removed.\n",
    "# Direct use of 'load_file' from safetensors.torch is now used in the main evaluation loop.\n",
    "\n",
    "# --- Dataloader Initialization for Evaluation ---\n",
    "print(\"\\n--- Initializing Evaluation Dataloader ---\")\n",
    "arc_eval_loader = None\n",
    "if 'ARC_EVAL_DIR' in globals() and os.path.exists(ARC_EVAL_DIR):\n",
    "    print(f\"  > Using NewCustomARCGridDataset for evaluation from path: {ARC_EVAL_DIR}\")\n",
    "    arc_eval_dataset = NewCustomARCGridDataset(\n",
    "        data_path=ARC_EVAL_DIR,\n",
    "        max_grid_size=MAX_GRID_SIZE,\n",
    "        padding_value=PADDING_VALUE\n",
    "    )\n",
    "    if len(arc_eval_dataset) > 0:\n",
    "        arc_eval_loader = DataLoader(arc_eval_dataset, batch_size=1, shuffle=False)\n",
    "        print(f\"✓ Evaluation DataLoader initialized with {len(arc_eval_dataset)} tasks.\")\n",
    "    else:\n",
    "        print(\"⚠️ Evaluation dataset is empty. Skipping evaluation.\")\n",
    "else:\n",
    "    print(f\"⚠️ Evaluation directory not found or not specified (ARC_EVAL_DIR='{globals().get('ARC_EVAL_DIR', 'Not Set')}'). Cannot create DataLoader.\")\n",
    "\n",
    "# --- Main Evaluation Logic ---\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(f\"🔬 STARTING ARC-AGI-2 Evaluation on device '{device}'\")\n",
    "print(\"=\"*60 + \"\\n\")\n",
    "\n",
    "if not all([ctm_model_arc, arc_output_head, arc_eval_loader]):\n",
    "     print(\"⚠️ Skipping evaluation due to missing components.\")\n",
    "else:\n",
    "    latest_epoch = NUM_EPOCHS_ARC\n",
    "    ctm_checkpoint_path_eval = os.path.join(CHECKPOINT_DIR_ARC, f\"ctm_model_arc_epoch_{latest_epoch}.safetensors\")\n",
    "    head_checkpoint_path_eval = os.path.join(CHECKPOINT_DIR_ARC, f\"arc_output_head_epoch_{latest_epoch}.safetensors\")\n",
    "\n",
    "    try:\n",
    "        # Load CTM Model\n",
    "        if os.path.exists(ctm_checkpoint_path_eval):\n",
    "            print(f\"  > Loading CTM checkpoint from {ctm_checkpoint_path_eval}...\")\n",
    "            # Load state_dict using the safetensors library directly, as per user feedback\n",
    "            state_dict_ctm = load_file(ctm_checkpoint_path_eval, device=\"cpu\")\n",
    "            ctm_model_arc.load_state_dict(state_dict_ctm, strict=False)\n",
    "            print(f\"✓ Loaded CTM checkpoint from epoch {latest_epoch}.\")\n",
    "        else:\n",
    "            print(f\"⚠️ CTM Checkpoint not found at {ctm_checkpoint_path_eval}.\")\n",
    "\n",
    "        # Load ARC Output Head Model\n",
    "        if os.path.exists(head_checkpoint_path_eval):\n",
    "            print(f\"  > Loading ARC Output Head checkpoint from {head_checkpoint_path_eval}...\")\n",
    "            state_dict_head = load_file(head_checkpoint_path_eval, device=\"cpu\")\n",
    "            arc_output_head.load_state_dict(state_dict_head, strict=False)\n",
    "            print(f\"✓ Loaded ARC Output Head checkpoint from epoch {latest_epoch}.\")\n",
    "        else:\n",
    "            print(f\"⚠️ ARC Output Head Checkpoint not found at {head_checkpoint_path_eval}.\")\n",
    "\n",
    "        ctm_model_arc.eval()\n",
    "        arc_output_head.eval()\n",
    "\n",
    "        total_tasks = 0\n",
    "        solved_tasks = 0\n",
    "\n",
    "        # Instantiate the isolated learning environment\n",
    "        # The optimizer is already prepared by Accelerate and does not need to be unwrapped with unwrap_model.\n",
    "        learning_container = IsolatedSelfLearningEnvironment(\n",
    "            model=ctm_model_arc,\n",
    "            arc_output_head=arc_output_head,\n",
    "            optimizer=optimizer_arc,\n",
    "            device=device\n",
    "        )\n",
    "\n",
    "        for task_idx, task_batch in enumerate(arc_eval_loader):\n",
    "            if not task_batch: continue\n",
    "\n",
    "            current_task_data = task_batch\n",
    "            total_tasks += 1\n",
    "            task_solved_overall = True\n",
    "\n",
    "            if 'test' not in current_task_data or not current_task_data['test']:\n",
    "                print(f\"Task {task_idx + 1} ({current_task_data.get('id', 'N/A')}): No test cases found. Skipping.\")\n",
    "                task_solved_overall = False\n",
    "                continue\n",
    "\n",
    "            for test_pair_idx, test_pair in enumerate(current_task_data['test']):\n",
    "                # Squeeze the batch dimension (size 1) from the data loader output\n",
    "                input_grid_np_eval = test_pair['input'].squeeze(0).cpu().numpy()\n",
    "                input_bytes_eval_single = serialize_and_pad_grid(input_grid_np_eval, max_len=MAX_SEQUENCE_LENGTH, pad_value=PADDING_BYTE_VALUE)\n",
    "                input_bytes_eval_np = np.frombuffer(input_bytes_eval_single, dtype=np.uint8).copy()\n",
    "                input_bytes_eval = torch.from_numpy(input_bytes_eval_np).to(torch.uint8).unsqueeze(0).to(device)\n",
    "\n",
    "                target_grid_np = test_pair['output'].squeeze(0).cpu().numpy()\n",
    "                # The dataloader collates the (h, w) tuple into a tuple/list of tensors.\n",
    "                # We need to extract the integer values from these tensors.\n",
    "                h_tensor, w_tensor = test_pair['original_output_dims']\n",
    "                original_dims = (h_tensor.item(), w_tensor.item())\n",
    "\n",
    "                test_input_solved = False\n",
    "\n",
    "                # --- First Attempt: Standard Prediction (with no_grad) ---\n",
    "                with torch.no_grad():\n",
    "                    current_batch_size_eval = input_bytes_eval.size(0)\n",
    "                    eval_timestep = torch.zeros(current_batch_size_eval, device=input_bytes_eval.device).long()\n",
    "                    eval_model_output_dict = ctm_model_arc(\n",
    "                        byte_sequence=input_bytes_eval,\n",
    "                        mode='ctm_controlled_diffusion',\n",
    "                        target_diffusion_output=None,\n",
    "                        timestep=eval_timestep,\n",
    "                        task_name=\"ARC_AGI_2_EVAL_DIFFUSION\"\n",
    "                    )\n",
    "                    \n",
    "                    preds_grid = np.zeros(MAX_GRID_SIZE, dtype=int)\n",
    "                    # Extract features consistent with training loop logic\n",
    "                    ctm_core_output_data = eval_model_output_dict.get('ctm_core_data')\n",
    "                    ctm_backbone_output = None\n",
    "                    if ctm_core_output_data and 'final_sync_out' in ctm_core_output_data:\n",
    "                        ctm_backbone_output = ctm_core_output_data['final_sync_out']\n",
    "                    elif ctm_core_output_data and 'ctm_latent_representation' in ctm_core_output_data:\n",
    "                        ctm_backbone_output = ctm_core_output_data['ctm_latent_representation']\n",
    "                    \n",
    "                    if ctm_backbone_output is not None:\n",
    "                        # Process features like in training\n",
    "                        if ctm_backbone_output.ndim > 2 and ctm_backbone_output.shape[1] > 0:\n",
    "                             ctm_features_for_head = ctm_backbone_output.mean(dim=1)\n",
    "                        else:\n",
    "                             ctm_features_for_head = ctm_backbone_output\n",
    "                        \n",
    "                        logits = arc_output_head(ctm_features_for_head)\n",
    "                        preds_flat = torch.argmax(logits.view(-1, NUM_ARC_SYMBOLS), dim=-1)\n",
    "                        preds_grid = preds_flat.view(MAX_GRID_SIZE).long().cpu().numpy()\n",
    "\n",
    "                # --- Evaluate Prediction ---\n",
    "                h, w = original_dims\n",
    "                final_pred = preds_grid[:h, :w]\n",
    "                final_target = target_grid_np[:h, :w]\n",
    "\n",
    "                if np.array_equal(final_pred, final_target):\n",
    "                    print(f\"  > Test pair {test_pair_idx+1} solved on attempt 1.\")\n",
    "                    test_input_solved = True\n",
    "                else:\n",
    "                    # --- Second Attempt: Isolated Self-Correction (with gradients enabled) ---\n",
    "                    features_from_failed_pred = ctm_features_for_head.detach() # Detach to prevent gradients from flowing back further\n",
    "                    corrected_grid = learning_container.run_correction_and_update(\n",
    "                        input_bytes=input_bytes_eval,\n",
    "                        failed_grid=preds_grid,\n",
    "                        target_grid_full=target_grid_np,\n",
    "                        original_dims=original_dims,\n",
    "                        failed_features=features_from_failed_pred,\n",
    "                        train_pairs=current_task_data.get('train')\n",
    "                    )\n",
    "\n",
    "                    if corrected_grid is not None:\n",
    "                        # Re-evaluate the grid returned by the learning container\n",
    "                        final_pred_corrected = corrected_grid[:h, :w]\n",
    "                        if np.array_equal(final_pred_corrected, final_target):\n",
    "                            print(f\"  > Test pair {test_pair_idx+1} solved on attempt 2 (after self-correction).\")\n",
    "                            test_input_solved = True\n",
    "                        else:\n",
    "                            print(f\"  > Self-correction did not produce the correct solution.\")\n",
    "                \n",
    "                if not test_input_solved:\n",
    "                    task_solved_overall = False\n",
    "                    # This break will exit the loop over test pairs for the current task\n",
    "                    break\n",
    "            \n",
    "            if task_solved_overall:\n",
    "                solved_tasks += 1\n",
    "                print(f\"  Task {task_idx + 1}/{len(arc_eval_loader)} ({current_task_data.get('id', 'N/A')}): SOLVED\")\n",
    "            else:\n",
    "                print(f\"  Task {task_idx + 1}/{len(arc_eval_loader)} ({current_task_data.get('id', 'N/A')}): FAILED\")\n",
    "        \n",
    "        if total_tasks > 0:\n",
    "            accuracy = (solved_tasks / total_tasks) * 100\n",
    "            summary = f\"ARC-AGI-2 Evaluation Summary:\\n  Total tasks evaluated: {total_tasks}\\n  Tasks solved: {solved_tasks}\\n  Accuracy: {accuracy:.2f}%\"\n",
    "            print(f\"\\n{summary}\")\n",
    "            with open('arc_agi_2_evaluation_summary.txt', 'w') as f:\n",
    "                f.write(summary)\n",
    "        else:\n",
    "            print(\"\\nARC-AGI-2 Evaluation: No tasks were evaluated.\")\n",
    "            \n",
    "    except FileNotFoundError as e:\n",
    "        print(f\"❌ Checkpoint file not found: {e}. Please ensure paths are correct.\")   \n",
    "    except Exception as e:\n",
    "        print(f\"❌ Error during ARC-AGI-2 evaluation: {e}\")\n",
    "        traceback.print_exc()\n",
    "        \n",
    "    print(\"\\n🔬 ARC-AGI-2 Evaluation Phase Completed.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
