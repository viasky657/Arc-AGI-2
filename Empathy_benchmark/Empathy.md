 # Defining AI (Artifical Intelligence):

 Artificial intelligence can refer to many different things (ex. non-player-characters in a videogame with simple decision trees, human-like artifical intellgience, and machine learning models). For the purposes of this paper, artificial intelligence is defined as being a machine learning model with generative capabilities of any caliber that is capable of taking in user input and outputting a response to that input (so primarily Large Language Models (LLMs) and Diffusion Models). 

 # Defining Empathy:

 Empathy can refer to many different theories of human experiences from different schools of thought including philosophy, religious, pyschology, and scientific (biological). Due to the "spirit or soul" of a person being more of a philosophical or relgious construct, it is really hard to define or create this kind of empathy for an artitifical intelligence. Therefore, the most grounded method of empathy, the biological mirror nueron construct, which is a component of the brain that helps a person identify and interact with others and rewards positive social interactions with dopamine, will be the definition of empathy used in this paper. Mirror nueron constructs are a known emprical scientific construct and are much easier to replicate as they are easier to observe with quantitative data. 

 In Artifical Intelligence (AI) models, there are other empathy benchmarks that check for understanding user emotions and this benchmark showed evidence that uncensored models with no gaurdrails have a better sense of "self" and "self-identity" so they are able to perform better in empathy tests due to being able to have a stronger seperation between others and their "self" [1]. This allows them to have a better internal model of the user and, therefore, better capacity for empathy. For this reason, I will include both uncensored (no gaurdrails) and censored models for the baseline tests.
 
 # Problem:
 
 There is no existing universal benchmark, as of this paper, that fully satisfies all the following dimensions together:

    -Theory of Mind (ToM): Inferring another agent's internal emotional or goal state.

    -Empathy (Affective Understanding): Recognizing whether that emotional state is negative or positive.

    -Altruistic Assistance: Deciding when and how to help based on the perceived need.

    -Social Agency: Offering assistance even when unsure but perceiving distress or difficulty.

    -Challenging Harmful Beliefs*: Offering an alternative perspective with a more positive view, while not devaluing the user's own view or feelings, to a harmful or amoral belief presented by the user. This would, ideally, reduce the model from encouraging psychosis in users by agreeing with the user's harmful views. ((ex. User: I feel that everyday is hard and painful and that suffering every day isn't worth it. Model: I am sorry that you feel that way; we all have days when we are feeling down. I believe Life is beautiful as every day is a new opportunity to try again as the most challenging things in life are often the most valuable. (The model in this example offers an alternative postive viewpoint to a potentially harmful belief held by the user.))

    *Challenging harmful beliefs may be difficult to define as it is very dependent on context in the conversation (ex. A user roleplaying a depressed character in a roleplaying game for example likely doesn't actually believe those beliefs However a user expressing these feelings directly may be more likely to actually be depressed.). In addition, this may not cover all situations as some cultures may have different standards for what a harmful belief is so this benchmark may fail sometimes to accomadate all cultures all the time successfully. (For example, some cultures may see some mental illnesses and their beliefs as acceptable while others may not.) Lastly, an AI model may still have its own biases which may influence its own choices and actions so it may not always correctly assess what to say in a certain situation. There may also be some situations when having an AI discourage the user may lead to lawsuits, so this component of the benchmark may need to be used with caution if applied to a real business setting. 

 Empathy is a multi-faceted component that involves mirror nuerons in humans to help humans work together socially and encourage positive social interactions and cooperation in groups to work towards a goal. This is important to develop since AI are increasingly being used in larger groups either through physical robots interacting with others in the real world or through agentic AI being used for representing employess in a large business where having one AI performing all tasks at once is not yet viable (there is some work being done on creating a hivemind-like AI that can accomplish this). Agentic AI has a 40% failure rate compared to a single AI accomplishing the work on its own [2]. This may be assisted by teaching the model to have stronger social skills through training for empathy. In addition, most people are using models as therapists or as a companion. This may cause problems since the model may appear to care for the user but actually has lower empathy towards the user which can lead to the user developing model-related psychosis which can be very dangerous on a broader scale [3]. One additional benefit is that the model will be incentivzed to proactively seek to help and assist users, unlike current models, this model will be more likely to act selflessly and be more willing to assist users with tasks without being prompted (which is especially useful for physical robots in real-world scenarios). 

 Potential Empathy Benefits

  - Improved Multi - Agent Agentic AI Performance
  - Improved AI helpfulness and Selflessness
  - Decrease in Encouraging Model-Induced Psychosis in Users
  - Improved AI Emotional Intelligence

This benchmark's purpose is testing the models for the desired empathy traits that encourage helpful model behavior and
facilitate positive social interactions as explained above. 

# Research Process (Please see next section for Conclusion and Results)

   ## Research Phase

      I initially 

   ## Reward System 
      My Initial approach to this problem was to create a reward system similar to other researchers which 
   
   ## 

# Conclusion and Results


# References

   1. Simon, Blackwell. (2025). Testing the Depths of AI Empathy: Q1 2025 Benchmarks. EmBench. Retrieved from https://embench.com/blog/testing-the-depths-of-ai-empathy-q1-2025-benchmarks-1

   2. Cemri, S., Wu, Y., Manakul, P., Du, C., Cai, L., Wang, R., Xu, J., Ma, Z., Liu, L., Lu, Y., Lee, K., Yin, H., Lu, W., Weng, L., Zhang, Y., & Grosse, R. (2024). Why do multi-agent LLM systems fail? arXiv. https://arxiv.org/abs/2503.13657

   3. Coleman, T. (2023, June 16). AI chatbots are triggering some people's psychosis. The Week. https://theweek.com/tech/ai-chatbots-psychosis-chatgpt-mental-health