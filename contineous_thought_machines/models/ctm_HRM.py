"""
Hierarchical Reasoning Continuous Thought Machine (HR-CTM)
This model integrates the principles of the Hierarchical Reasoning Model (HRM)
into the Continuous Thought Machine (CTM) architecture. It features a two-level
recurrent system to achieve greater computational depth and reasoning capability.
Key Features:
1.  **Hierarchical Structure**: A high-level, slow-updating module (H-module) for
    abstract planning and a low-level, fast-updating module (L-module) for
    detailed computation.
2.  **Hierarchical Convergence**: The L-module performs multiple computational steps
    to reach a local equilibrium before the H-module performs a single update,
    enabling deep, nested reasoning.
3.  **Preservation of CTM Principles**: Leverages the core CTM concepts like
    synchronization-as-representation and neuron-level models within the
    hierarchical framework.
4.  **Replacement of Frequency Boosts**: The explicit frequency-based creative boosts
    are replaced by the intrinsic multi-timescale dynamics of the H and L modules.
"""

import torch
import torch.nn as nn
import torch.nn.functional as F
from typing import Dict, Optional, Tuple, List
import math
import numpy as np
from torch.nn import GRU

from .modules import SynapseUNET, SuperLinear, Squeeze
from .utils import compute_normalized_entropy
from .long_term_memory import LongTermMemory, MemoryReplayPolicy

from .mamba_block import Mamba2Block
from .ctm_components import (
    EnhancedCTMConfig,
    WorkingMemoryBuffer,
    WINAAttention,
    OriginalCTMCore,
    CTMFeedbackModule,
    BasalGangliaMechanism,
    SynapticEmpathy,
    MirrorNeuronLayer,
    EmotionStateTracker,
    GoalPredictor,
    BinarySparseAttention,
    WINAEnhancedMLP,
    WINASparsifier,
    MetaWINASparsifier,
    ConsciousnessController,
    ProgramSynthesizer,
    BidirectionalReasoningController,
    FrequencyDomainAwareAttention,
    TemporalSpatialTracker,
    ForesightSimulator,
    HyperNetwork,
    GlialSupport,
    SpatialReasoningModule,
    ThreeDSpatialReasoningModule,
)

class HRM_H_Module(nn.Module):
    """The High-Level, slow-updating recurrent module for the HR-CTM."""
    def __init__(self, config: EnhancedCTMConfig):
        super().__init__()
        self.config = config
        # This module integrates the result from the L-module (zL) into its own state (zH).
        self.mamba = Mamba2Block(d_model=config.d_model)
        self.sparse_attn = WINAAttention(d_model=config.d_model, n_heads=config.n_heads, dropout=config.dropout)
        self.norm1 = nn.LayerNorm(config.d_model)
        self.norm2 = nn.LayerNorm(config.d_model)
        self.planning_mlp = nn.Sequential(
            nn.Linear(config.d_model, config.d_model * 2),
            nn.ReLU(),
            nn.Linear(config.d_model * 2, config.d_model * 4),
            nn.ReLU(),
            nn.Linear(config.d_model * 4, config.d_model * 2),
            nn.ReLU(),
            nn.Linear(config.d_model * 2, config.d_model),
            nn.ReLU(),
            nn.Linear(config.d_model, config.d_model)
        )
        self.norm3 = nn.LayerNorm(config.d_model)
        # Project zL to match d_model for attention
        self.zl_proj = nn.Linear(config.d_model, config.d_model)  # Assuming zL has d_model
        patcher_config = {
            'embedding_dim': config.patch_embedding_dim,
            'patch_cnn_channels': config.patch_encoder_cnn_channels,
            'patching_mode': config.entropy_patcher_threshold_type,
            'global_threshold': config.entropy_patcher_global_threshold,
            'relative_threshold': config.entropy_patcher_relative_threshold,
            'min_patch_size': config.entropy_patcher_min_patch_size,
            'max_patch_size': config.entropy_patcher_max_patch_size,
            'entropy_byte_vocab_size': config.entropy_model_byte_vocab_size,
            'entropy_embedding_dim': config.entropy_model_embedding_dim,
            'entropy_hidden_dim': config.entropy_model_hidden_dim,
            'entropy_num_layers': config.entropy_model_num_layers,
            'entropy_dropout': config.entropy_model_dropout
        }
        if getattr(config, 'use_program_synthesizer', False):
            self.program_synthesizer = ProgramSynthesizer(
                d_model=config.d_model,
                n_heads=config.program_synth_n_heads,
                n_layers=config.program_synth_n_layers,
                d_ff=config.program_synth_d_ff,
                dropout=config.dropout,
                max_gen_len=config.max_sequence_length, # Or a more specific config
                patcher_config=patcher_config
            )
        else:
            self.program_synthesizer = None
        self.hypernet = HyperNetwork(config.d_model * 2, config.d_model)
        self.meta_learner = nn.Linear(config.d_model * 2, config.d_model)  # Base learner, params generated by hypernet
        self.foresight = ForesightSimulator(config.d_model)
        self.max_recursion = config.max_recursion
        self.early_stop_threshold = config.early_stop_threshold
        self.program_feedback_proj = nn.Linear(config.d_model, config.d_model)
        self.thought_ctm = OriginalCTMCore(config)
        self.thought_feedback_proj = nn.Linear(config.ctm_out_dims, config.d_model)
        
        # Add CTM-like components for H-module
        # Using N=1 since it's used as a regular MLP, not per-neuron.
        self.h_synapses = SuperLinear(2, 1, N=config.d_model, depth=config.ctm_synapse_depth, dropout=config.ctm_dropout)
        self.h_trace_processor = SuperLinear(config.ctm_memory_length, 1, N=config.d_model, depth=config.ctm_deep_nlms, dropout=config.ctm_dropout)
        self.h_q_proj = nn.Linear(config.d_model, config.d_model)  # For H-module sync-based query
        
    def forward(self, zH: torch.Tensor, zL: torch.Tensor, retrieved_memory: torch.Tensor, thought_guidance: bool = True) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor, torch.Tensor]:
        """
        Args:
            zH (torch.Tensor): Current high-level state.
            zL (torch.Tensor): Final low-level state from the L-cycle.
            retrieved_memory (torch.Tensor): Memory retrieved from the LTM.
            thought_guidance (bool): Flag to switch to direct CTM thought vector guidance. #Recommended on for most model usage.
        Returns:
            Tuple[torch.Tensor, torch.Tensor, torch.Tensor, torch.Tensor]: Next high-level state, encoded_patches (or None), patch_indices (or None), entropy_aux_loss (or 0).
        """
        current_zH = zH
        prev_zH = None
        depth = 0
        encoded_patches = None
        patch_indices = None
        entropy_aux_loss = torch.tensor(0.0, device=zH.device)
        
        # Initialize H-module trace
        h_trace = torch.zeros_like(current_zH.unsqueeze(-1).repeat(1, 1, self.config.ctm_memory_length))
        
        # Initialize sync for H-module
        decay_alpha_h, decay_beta_h = None, None
        r_h = torch.exp(torch.tensor(-0.1))  # Example decay rate
        
        while depth < self.max_recursion:
            # Compute H-module synchronization (pulsing)
            sync_h, decay_alpha_h, decay_beta_h = self.compute_synchronisation(
                current_zH, decay_alpha_h, decay_beta_h, r_h, 'action'  # Reuse 'action' type
            )
            
            # The query is the current high-level state modulated by sync
            q = self.h_q_proj(sync_h).unsqueeze(1)
            
            # The key/value is the information from the completed low-level cycle and retrieved memory
            # The retrieved_memory is now a single contextualized vector from the LTM's attention mechanism
            kv = self.zl_proj(zL) + retrieved_memory.squeeze(0) # Squeeze to remove the batch dim of 1
            # Attention step
            kv = (self.zl_proj(zL) + retrieved_memory.squeeze(0)).unsqueeze(1)
            current_zH = current_zH.unsqueeze(1)
            q = self.h_q_proj(sync_h).unsqueeze(1)
            
            # Dynamic routing: Compute WINA scores to decide per token
            scores = self.sparse_attn.wina_sparsifier.compute_wina_scores(current_zH, self.sparse_attn.q_proj.weight)
            route_to_attention = (scores > 0.5).float()  # Example threshold; make learnable
            
            # Mamba path (default/efficient)
            mamba_out = self.mamba(current_zH)
            
            # Sparse attention path (selective)
            attn_out = self.sparse_attn(current_zH, kv, kv)
            
            # Fuse based on routing
            current_zH = current_zH + (mamba_out * (1 - route_to_attention) + attn_out * route_to_attention)
            
            # Repeat for second block (or loop for more)
            scores = self.sparse_attn.wina_sparsifier.compute_wina_scores(current_zH, self.sparse_attn.q_proj.weight)
            route_to_attention = (scores > 0.5).float()
            
            mamba_out = self.mamba(current_zH)
            attn_out = self.sparse_attn(current_zH, kv, kv)
            current_zH = current_zH + (mamba_out * (1 - route_to_attention) + attn_out * route_to_attention)
            
            current_zH = current_zH.squeeze(1)
            
            meta_input = torch.cat([current_zH, zL], dim=-1)
            # Dynamic meta-learning with hypernetwork
            weight, bias = self.hypernet(meta_input)
            meta_update = F.linear(meta_input, weight, bias)
            current_zH = current_zH + meta_update * 0.1  # Small meta-update step
            current_zH = self.norm1(current_zH)
            
            # Additional planning layer
            planning_output = self.planning_mlp(current_zH)
            current_zH = self.norm3(current_zH + planning_output)
            
            # Add foresight simulation
            foresight_adjust = self.foresight(current_zH)
            current_zH = current_zH + foresight_adjust * 0.05
            
            # Add CTM-like synapse and NLM processing
            h_pre_synapse = torch.cat([current_zH, retrieved_memory.squeeze(0)], dim=-1)
            h_state = self.h_synapses(h_pre_synapse.view(h_pre_synapse.shape[0], self.config.d_model, 2))
            h_trace = torch.cat((h_trace[:, :, 1:], h_state.unsqueeze(-1)), dim=-1)
            current_zH = self.h_trace_processor(h_trace)
            
            if not thought_guidance and self.program_synthesizer is not None:
                # Synthesize a program using the new synthesizer
                encoded_patches, patch_indices, entropy_aux_loss = self.program_synthesizer(current_zH)
                
                # Feedback from synthesized program to high-level state
                if encoded_patches is not None and encoded_patches.size(1) > 0:
                    program_feedback = self.program_feedback_proj(encoded_patches.mean(dim=1))
                    current_zH = current_zH + program_feedback * 0.1
            elif not thought_guidance:
                # Program synthesizer is disabled, so we return empty tensors for compatibility
                encoded_patches = None
                patch_indices = None
                entropy_aux_loss = torch.tensor(0.0, device=zH.device)
            else:
                # Direct CTM thought vector guidance
                ctm_predictions, ctm_certainties, ctm_sync_out = self.thought_ctm(current_zH.unsqueeze(1))
                thought_feedback = self.thought_feedback_proj(ctm_sync_out)
                current_zH = current_zH + thought_feedback * 0.1
                # Set placeholders for return values
                encoded_patches = None
                patch_indices = None
                entropy_aux_loss = torch.tensor(0.0, device=zH.device)
            
            # Early stopping check
            if prev_zH is not None:
                delta = torch.norm(current_zH - prev_zH, dim=-1).mean()
                if delta < self.early_stop_threshold:
                    break
            
            prev_zH = current_zH.clone()
            depth += 1
        
        # The 'program' is now the sequence of encoded patches (or None in direct mode).
        # The other outputs might be used for loss calculation or debugging.
        return current_zH, encoded_patches, patch_indices, entropy_aux_loss

class HRM_L_Module(nn.Module):
    """The Low-Level, fast-updating CTM-based recurrent module for the HR-CTM."""
    def __init__(self, config: EnhancedCTMConfig, parent_ctm: 'HierarchicalCTM'):
        super().__init__()
        self.config = config
        self.d_model = config.ctm_d_model
        self.d_input = config.ctm_input_dim
        
        self.mamba_encoder = Mamba2Block(d_model=self.d_input)
        
        # Inherit synapse and NLM models from parent HierarchicalCTM
        # to ensure they are registered correctly under the main model.
        self.synapses = parent_ctm.synapses
        self.trace_processor = parent_ctm.trace_processor
        
        # Projector for the query, derived from the low-level sync state
        self.q_proj = nn.Linear(parent_ctm.synch_representation_size_action, self.d_input)
        self.top_down_projector = nn.Linear(self.config.d_model, self.d_model)  # Project zH to modulation signal
        
        if self.config.use_spatial:
            self.spatial_reasoning = SpatialReasoningModule(self.d_model)
            self.three_d_spatial_reasoning = ThreeDSpatialReasoningModule(self.d_model)
        else:
            self.spatial_reasoning = None
            self.three_d_spatial_reasoning = None

    def forward(self,
                activated_zL: torch.Tensor,
                zL_trace: torch.Tensor,
                zH: torch.Tensor,
                x_context: torch.Tensor,
                sync_action: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor]:
        """
        Performs one step of the low-level CTM computation.
    
        Args:
            activated_zL: Current post-activation state of the L-module. (B, D)
            zL_trace: History of pre-activations for the L-module. (B, D, M)
            zH: Current high-level state, provides top-down context. (B, D)
            x_context: External input, provides bottom-up context. (B, S, d_input)
            sync_action: Synchronization representation for generating attention query. (B, sync_dim)
    
        Returns:
            A tuple of (next_activated_zL, next_zL_trace).
        """
        x_context = self.mamba_encoder(x_context)
    
        # 1. Interact with context via attention
        # Query is from L-module's own action synchronisation
        q = self.q_proj(sync_action).unsqueeze(1)
        
        # Key/Value is a combination of external input and high-level context
        # Add zH as part of the key/value context
        kv = x_context + zH.unsqueeze(1)
        attn_out, _ = self.attention(q, kv, kv)
        attn_out = attn_out.squeeze(1)
    
        # 2. Form input for synapses
        pre_synapse_input = torch.cat((attn_out, activated_zL), dim=-1)
    
        # 3. Apply Synapses to get pre-activation state
        state = self.synapses(pre_synapse_input)
        top_down_mod = self.top_down_projector(zH)  # (B, D)
        state = state + top_down_mod * 0.3  # Modulate with strength 0.3
        
        # Add parietal-inspired spatial reasoning
        if self.config.use_spatial and self.spatial_reasoning is not None:
            state = self.spatial_reasoning(state.unsqueeze(1)).squeeze(1)
    
        # Add 3D spatial reasoning - assume a 3D grid size, e.g., (4,4,4) if d_model=64
        # Adjust based on actual d_model; here assuming d_model is cube-able
        if self.config.use_spatial and self.three_d_spatial_reasoning is not None:
            cube_root = int(self.d_model ** (1/3))
            grid_3d = (cube_root, cube_root, cube_root)
            state = self.three_d_spatial_reasoning(state.unsqueeze(1), grid_size=grid_3d).squeeze(1)
    
        # 4. Update state trace (memory for NLMs)
        next_zL_trace = torch.cat((zL_trace[:, :, 1:], state.unsqueeze(-1)), dim=-1)
    
        # 5. Apply Neuron-Level Models (NLMs) to get next post-activation state
        next_activated_zL = self.trace_processor(next_zL_trace)
        
        return next_activated_zL, next_zL_trace
    
class HierarchicalCTM(OriginalCTMCore):
    """
    The main Hierarchical Reasoning CTM model.
    Inherits from OriginalCTMCore to reuse helper methods for initialization."
    """
    def __init__(self, config: EnhancedCTMConfig):
        # We call nn.Module's init directly to avoid OriginalCTMCore's full init,
        # as we are building a different structure.
        super(OriginalCTMCore, self).__init__()
        self.config = config
        self.replay_batch_size = getattr(config, 'replay_batch_size', 4)
        self.d_model = config.ctm_d_model
        self.d_input = config.ctm_input_dim
        self.memory_length = config.ctm_memory_length

        # --- Instantiate CTM components needed for the L-module ---
        # These methods are borrowed from OriginalCTMCore
        self.synapses = self.get_synapses(
            config.ctm_synapse_depth, self.d_input + self.d_model, self.d_model, config.ctm_dropout
        )
        self.trace_processor = self.get_neuron_level_models(
            config.ctm_deep_nlms, config.ctm_do_layernorm_nlm, config.ctm_memory_length,
            config.ctm_memory_hidden_dims, self.d_model, config.ctm_dropout_nlm or config.ctm_dropout
        )

        # --- Synchronisation Setup (reusing logic from OriginalCTMCore) ---
        self.neuron_select_type = config.ctm_neuron_select_type
        self.verify_args() # verify neuron selection compatibility
        self.n_synch_out = config.ctm_n_synch_out
        self.n_synch_action = config.ctm_n_synch_action
        self.register_parameter('start_activated_state', nn.Parameter(torch.zeros((self.d_model)).uniform_(-math.sqrt(1/(self.d_model)), math.sqrt(1/(self.d_model)))))
        self.register_parameter('start_trace', nn.Parameter(torch.zeros((self.d_model, self.memory_length)).uniform_(-math.sqrt(1/(self.d_model+self.memory_length)), math.sqrt(1/(self.d_model+self.memory_length)))))
        self.synch_representation_size_action = self.calculate_synch_representation_size(self.n_synch_action)
        self.synch_representation_size_out = self.calculate_synch_representation_size(self.n_synch_out)

        self.set_synchronisation_parameters('action', self.n_synch_action, config.ctm_n_random_pairing_self)
        self.set_synchronisation_parameters('out', self.n_synch_out, config.ctm_n_random_pairing_self)
        
        # Add synchronization parameters for H-module
        self.n_synch_h = config.ctm_n_synch_action  # Reuse action size
        self.synch_representation_size_h = self.calculate_synch_representation_size(self.n_synch_h)
        self.set_synchronisation_parameters('h', self.n_synch_h, config.ctm_n_random_pairing_self)

        # --- Instantiate HRM Modules ---
        self.l_module = HRM_L_Module(config, self)
        self.h_module = HRM_H_Module(config)
        self.ltm = LongTermMemory(config.d_model, config.ltm_size, config.ltm_top_k, MemoryReplayPolicy[config.replay_policy.upper()])
        self.consciousness_controller = ConsciousnessController(config.d_model, config.consciousness_max_attention_steps)
        self.basal_ganglia = BasalGangliaMechanism(
            d_model=config.d_model,
            action_dim=config.ctm_n_synch_action,
            dopamine_dim=config.ctm_bg_dopamine_dim,
            context_dim=self.d_input
        )
        self.synaptic_empathy = SynapticEmpathy(config.d_model, config.ctm_memory_length, config.n_heads, config.dropout)
        self.mirror_neuron = MirrorNeuronLayer(config.d_model, config.n_heads, config.dropout, config.num_emotion_dim, config.goal_dim)
        self.temporal_spatial_tracker = TemporalSpatialTracker(config)
        self.working_memory = WorkingMemoryBuffer(config.d_model)
        self.glial_support = GlialSupport(config.d_model)
        
        # --- Input/Output Layers ---
        self.input_encoder = nn.Linear(config.ctm_input_dim, self.d_input)
        self.output_projector = nn.Linear(self.synch_representation_size_out, config.ctm_out_dims)

        # --- Initial States ---
        self.start_activated_zL = nn.Parameter(torch.zeros(self.d_model))
        self.start_trace_zL = nn.Parameter(torch.zeros(self.d_model, self.memory_length))
        self.start_zH = nn.Parameter(torch.zeros(self.d_model))
        nn.init.uniform_(self.start_activated_zL, -math.sqrt(1/self.d_model), math.sqrt(1/self.d_model))
        nn.init.uniform_(self.start_trace_zL, -math.sqrt(1/(self.d_model+self.memory_length)), math.sqrt(1/(self.d_model+self.memory_length)))
        nn.init.uniform_(self.start_zH, -math.sqrt(1/self.d_model), math.sqrt(1/self.d_model))
        
        self.fusion_proj = nn.Linear(2 * self.d_model, self.d_model)

    def forward_with_full_tracking(self, x: torch.Tensor, thought_guidance: bool = True,
                                   voice1_id: Optional[torch.Tensor] = None,
                                   voice2_id: Optional[torch.Tensor] = None,
                                   blend_degree: Optional[torch.Tensor] = None) -> Dict[str, torch.Tensor]:
        """
        The main forward pass implementing the hierarchical reasoning process.
        This method will replace the original CTM's iterative loop.
        self.fusion_proj = nn.Linear(2 * self.d_model, self.d_model)
        """
        b, s, _ = x.shape
        device = x.device
        self.consciousness_controller.wake_up(0)

        # 1. Project input
        x_context = self.input_encoder(x)
        
        # 2. Initialize states
        activated_zL = self.start_activated_zL.unsqueeze(0).expand(b, -1)
        zL_trace = self.start_trace_zL.unsqueeze(0).expand(b, -1, -1)
        zH = self.start_zH.unsqueeze(0).expand(b, -1)
        
        # Optional voice blending
        if voice1_id is not None and voice2_id is not None and blend_degree is not None:
            blended_voice = self.voice_blender(voice1_id, voice2_id, blend_degree)
            zH = zH + blended_voice  # Add blended voice to high-level state
        
        # 3. Initialize sync recurrent values
        decay_alpha_action, decay_beta_action = None, None
        decay_alpha_out, decay_beta_out = None, None
        decay_alpha_h, decay_beta_h = None, None
        r_action = torch.exp(-self.decay_params_action).unsqueeze(0).expand(b, -1)
        r_out = torch.exp(-self.decay_params_out).unsqueeze(0).expand(b, -1)
        r_h = torch.exp(-self.decay_params_h).unsqueeze(0).expand(b, -1)

        # Store history of high-level states for final representation
        zH_history = []
        programs = []
        total_entropy_loss = torch.tensor(0.0, device=device)

        # 4. Hierarchical recurrent loop
        for n in range(self.config.hrm_high_level_cycles):
            # The L-module's own synchronisation state is reset/recalculated each high-level cycle
            decay_alpha_action, decay_beta_action = None, None

            prev_zL = activated_zL.clone()
            for t in range(self.config.hrm_low_level_timesteps):
                # Compute L-module's action synchronisation for its attention query
                sync_action, decay_alpha_action, decay_beta_action = self.compute_synchronisation(
                    activated_zL, decay_alpha_action, decay_beta_action, r_action, 'action'
                )
                
                if self.basal_ganglia:
                    action_candidates = [sync_action, sync_action * 0.5, sync_action * 1.5]
                    sync_action = self.basal_ganglia.select_action(action_candidates, activated_zL, x_context.mean(dim=1))
                
                # Run one step of the L-module
                activated_zL, zL_trace = self.l_module(
                    activated_zL, zL_trace, zH, x_context, sync_action
                )
                
                activated_zL = self.working_memory.update(activated_zL)

                # Early stopping if change is small
                
                delta = torch.norm(activated_zL - prev_zL)
                if delta < 1e-3:
                    break
                prev_zL = activated_zL.clone()
            
            # Calculate surprise
            surprise = compute_normalized_entropy(activated_zL.unsqueeze(1)).mean()
            
            # Store zH in LTM if surprising
            if surprise > self.config.ltm_surprise_threshold:
                self.ltm.add_to_memory(zH.squeeze(0), surprise)

            # Retrieve from LTM
            retrieved_memory = self.ltm.retrieve_from_memory(zH.squeeze(0))

            # End of low-level cycle, update high-level state using the final L-state
            # Fuse retrieved_memory with zL before passing to h_module
            # Fuse retrieved_memory with zL before passing to h_module with bidirectional attention
            fused_input = torch.cat([activated_zL.unsqueeze(1), retrieved_memory], dim=1)
            attn_out, _ = nn.MultiheadAttention(self.d_model, num_heads=8, batch_first=True)(fused_input, fused_input, fused_input)
            fused_input = self.fusion_proj(attn_out.mean(dim=1))
            
            zH, encoded_patches, patch_indices, entropy_aux_loss = self.h_module(zH, fused_input, retrieved_memory, thought_guidance=thought_guidance)
            modulation = self.consciousness_controller.get_attention_modulation()
            zH = zH * modulation
            
            # Apply Synaptic Empathy
            synaptic_modulation, empathy_reward = self.synaptic_empathy(activated_zL.unsqueeze(-1), zH.unsqueeze(-1), zH)
            zH = zH + synaptic_modulation * 0.1
            
            # Apply Glial Support for state stabilization
            zH = self.glial_support(zH)
            
            # Apply Mirror Neuron Layer
            valence, arousal = self.mirror_neuron.get_valence_arousal(zH.unsqueeze(1))
            observed_valence, observed_arousal = self.mirror_neuron.get_valence_arousal(zH.unsqueeze(1))  # Placeholder for observed
            emotion = torch.cat([valence, arousal], dim=-1)
            observed_emotion = torch.cat([observed_valence, observed_arousal], dim=-1)
            modulated_zH, _, current_observed_goal, mirror_reward
