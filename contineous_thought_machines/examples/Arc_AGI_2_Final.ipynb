{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0990b813",
   "metadata": {},
   "source": [
    "# -----------------------------------------------------------------------------\n",
    "# Dependency Installation Notes\n",
    "# -----------------------------------------------------------------------------\n",
    " The following dependencies are required. Please install them in your Python environment,\n",
    " for example, using pip:\n",
    "\n",
    " pip install mediapy\n",
    " pip install torch\n",
    " pip install safetensors\n",
    " pip install numpy\n",
    "\n",
    " For advanced optimizations, consider installing the following:\n",
    " pip install flash-attn --no-build-isolation\n",
    " pip install deepspeed\n",
    " pip install accelerate\n",
    " pip install xformers\n",
    "\n",
    " It's recommended to use a virtual environment.\n",
    " -----------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c018e3a1",
   "metadata": {},
   "source": [
    "This section contains the set-up components for training the ctm model with the byte-level encoder with binary patches, ctm processing with synpase system set to multi-objective, \n",
    "\n",
    "\n",
    "\n",
    "and binary patches from the ctm (after 20 rounds of COT thinking) refined and trained with MCMC to encourage the model to have reasoning steps closely related to the best answer, \n",
    "\n",
    "\n",
    "\n",
    "Each epoch is saved as a safetensor checkpoint to preserve training progress."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f95c4b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Save the original working directory Before LFS to avoid python corruption errors.\n",
    "original_dir = os.getcwd()\n",
    "\n",
    "# Add the Git LFS APT repository\n",
    "!curl -s https://packagecloud.io/install/repositories/github/git-lfs/script.deb.sh | bash #You need to be the root user or else this won't work. \n",
    "!apt install git-lfs -y \n",
    "!git lfs install\n",
    "!pip install pickleshare\n",
    "!git clone https://github.com/viasky657/Arc-AGI-2.git\n",
    "%cd Arc-AGI-2\n",
    "!git lfs pull #This is to download the actual model tensors and not the Github LFS pointer files.  #This is to download the actual model tensors and not the Github LFS pointer files. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f3c357f",
   "metadata": {},
   "outputs": [],
   "source": [
    "!git lfs fetch --all\n",
    "!git lfs checkout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56296414",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.chdir(\"/workspace/Arc-AGI-2\")  # Set your root working directory\n",
    "print(\"Current directory:\", os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3acd51d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "!ls -lh *.safetensors #Check for successful LFS pull for the safetensors and the .pt files. \n",
    "!git rev-parse --is-inside-work-tree\n",
    "!git lfs ls-files\n",
    "!head -n 10 contineous-thought-machines/examples/checkpoints/ctm_arc_agi_2_enhanced_diffusion/arc_output_head_epoch_20.safetensors #The below output should be binary \n",
    "#and metadata if it is a real tensor and \n",
    "#not a LFS pointer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3f07f68",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Change Back to Original directory to avoid errors. \n",
    "# Safely return to the original directory\n",
    "%cd $original_di #Probably don't need to use this since the code above automatically creates the correct directory. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be570979",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting matplotlib\n",
      "  Downloading matplotlib-3.10.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (11 kB)\n",
      "Collecting contourpy>=1.0.1 (from matplotlib)\n",
      "  Downloading contourpy-1.3.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.5 kB)\n",
      "Collecting cycler>=0.10 (from matplotlib)\n",
      "  Downloading cycler-0.12.1-py3-none-any.whl.metadata (3.8 kB)\n",
      "Collecting fonttools>=4.22.0 (from matplotlib)\n",
      "  Downloading fonttools-4.58.5-cp310-cp310-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (106 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m106.9/106.9 kB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting kiwisolver>=1.3.1 (from matplotlib)\n",
      "  Downloading kiwisolver-1.4.8-cp310-cp310-manylinux_2_12_x86_64.manylinux2010_x86_64.whl.metadata (6.2 kB)\n",
      "Requirement already satisfied: numpy>=1.23 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (1.24.1)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (23.2)\n",
      "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (9.3.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /usr/lib/python3/dist-packages (from matplotlib) (2.4.7)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (2.8.2)\n",
      "Requirement already satisfied: six>=1.5 in /usr/lib/python3/dist-packages (from python-dateutil>=2.7->matplotlib) (1.16.0)\n",
      "Downloading matplotlib-3.10.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (8.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.6/8.6 MB\u001b[0m \u001b[31m20.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading contourpy-1.3.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (325 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m325.0/325.0 kB\u001b[0m \u001b[31m24.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading cycler-0.12.1-py3-none-any.whl (8.3 kB)\n",
      "Downloading fonttools-4.58.5-cp310-cp310-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (4.8 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.8/4.8 MB\u001b[0m \u001b[31m55.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading kiwisolver-1.4.8-cp310-cp310-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (1.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m67.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: kiwisolver, fonttools, cycler, contourpy, matplotlib\n",
      "Successfully installed contourpy-1.3.2 cycler-0.12.1 fonttools-4.58.5 kiwisolver-1.4.8 matplotlib-3.10.3\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.1.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython -m pip install --upgrade pip\u001b[0m\n",
      "Collecting mediapy\n",
      "  Downloading mediapy-1.2.4-py3-none-any.whl.metadata (4.8 kB)\n",
      "Requirement already satisfied: ipython in /usr/local/lib/python3.10/dist-packages (from mediapy) (8.17.2)\n",
      "Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (from mediapy) (3.10.3)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from mediapy) (1.24.1)\n",
      "Requirement already satisfied: Pillow in /usr/local/lib/python3.10/dist-packages (from mediapy) (9.3.0)\n",
      "Requirement already satisfied: decorator in /usr/local/lib/python3.10/dist-packages (from ipython->mediapy) (5.1.1)\n",
      "Requirement already satisfied: jedi>=0.16 in /usr/local/lib/python3.10/dist-packages (from ipython->mediapy) (0.19.1)\n",
      "Requirement already satisfied: matplotlib-inline in /usr/local/lib/python3.10/dist-packages (from ipython->mediapy) (0.1.6)\n",
      "Requirement already satisfied: prompt-toolkit!=3.0.37,<3.1.0,>=3.0.30 in /usr/local/lib/python3.10/dist-packages (from ipython->mediapy) (3.0.39)\n",
      "Requirement already satisfied: pygments>=2.4.0 in /usr/local/lib/python3.10/dist-packages (from ipython->mediapy) (2.16.1)\n",
      "Requirement already satisfied: stack-data in /usr/local/lib/python3.10/dist-packages (from ipython->mediapy) (0.6.3)\n",
      "Requirement already satisfied: traitlets>=5 in /usr/local/lib/python3.10/dist-packages (from ipython->mediapy) (5.13.0)\n",
      "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from ipython->mediapy) (1.1.3)\n",
      "Requirement already satisfied: pexpect>4.3 in /usr/local/lib/python3.10/dist-packages (from ipython->mediapy) (4.8.0)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->mediapy) (1.3.2)\n",
      "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib->mediapy) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->mediapy) (4.58.5)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->mediapy) (1.4.8)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->mediapy) (23.2)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /usr/lib/python3/dist-packages (from matplotlib->mediapy) (2.4.7)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib->mediapy) (2.8.2)\n",
      "Requirement already satisfied: parso<0.9.0,>=0.8.3 in /usr/local/lib/python3.10/dist-packages (from jedi>=0.16->ipython->mediapy) (0.8.3)\n",
      "Requirement already satisfied: ptyprocess>=0.5 in /usr/local/lib/python3.10/dist-packages (from pexpect>4.3->ipython->mediapy) (0.7.0)\n",
      "Requirement already satisfied: wcwidth in /usr/local/lib/python3.10/dist-packages (from prompt-toolkit!=3.0.37,<3.1.0,>=3.0.30->ipython->mediapy) (0.2.9)\n",
      "Requirement already satisfied: six>=1.5 in /usr/lib/python3/dist-packages (from python-dateutil>=2.7->matplotlib->mediapy) (1.16.0)\n",
      "Requirement already satisfied: executing>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from stack-data->ipython->mediapy) (2.0.1)\n",
      "Requirement already satisfied: asttokens>=2.1.0 in /usr/local/lib/python3.10/dist-packages (from stack-data->ipython->mediapy) (2.4.1)\n",
      "Requirement already satisfied: pure-eval in /usr/local/lib/python3.10/dist-packages (from stack-data->ipython->mediapy) (0.2.2)\n",
      "Downloading mediapy-1.2.4-py3-none-any.whl (26 kB)\n",
      "Installing collected packages: mediapy\n",
      "Successfully installed mediapy-1.2.4\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.1.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython -m pip install --upgrade pip\u001b[0m\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (1.24.1)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.1.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython -m pip install --upgrade pip\u001b[0m\n",
      "Collecting accelerate\n",
      "  Downloading accelerate-1.8.1-py3-none-any.whl.metadata (19 kB)\n",
      "Requirement already satisfied: numpy<3.0.0,>=1.17 in /usr/local/lib/python3.10/dist-packages (from accelerate) (1.24.1)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from accelerate) (23.2)\n",
      "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from accelerate) (5.9.6)\n",
      "Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from accelerate) (6.0.1)\n",
      "Requirement already satisfied: torch>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from accelerate) (2.1.0+cu118)\n",
      "Collecting huggingface_hub>=0.21.0 (from accelerate)\n",
      "  Downloading huggingface_hub-0.33.2-py3-none-any.whl.metadata (14 kB)\n",
      "Collecting safetensors>=0.4.3 (from accelerate)\n",
      "  Downloading safetensors-0.5.3-cp38-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.8 kB)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface_hub>=0.21.0->accelerate) (3.9.0)\n",
      "Collecting fsspec>=2023.5.0 (from huggingface_hub>=0.21.0->accelerate)\n",
      "  Downloading fsspec-2025.5.1-py3-none-any.whl.metadata (11 kB)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface_hub>=0.21.0->accelerate) (2.31.0)\n",
      "Collecting tqdm>=4.42.1 (from huggingface_hub>=0.21.0->accelerate)\n",
      "  Downloading tqdm-4.67.1-py3-none-any.whl.metadata (57 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m57.7/57.7 kB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub>=0.21.0->accelerate) (4.4.0)\n",
      "Collecting hf-xet<2.0.0,>=1.1.2 (from huggingface_hub>=0.21.0->accelerate)\n",
      "  Downloading hf_xet-1.1.5-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (879 bytes)\n",
      "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=2.0.0->accelerate) (1.12)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=2.0.0->accelerate) (3.0)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=2.0.0->accelerate) (3.1.2)\n",
      "Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch>=2.0.0->accelerate) (2.1.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=2.0.0->accelerate) (2.1.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub>=0.21.0->accelerate) (2.1.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub>=0.21.0->accelerate) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub>=0.21.0->accelerate) (1.26.13)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub>=0.21.0->accelerate) (2022.12.7)\n",
      "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=2.0.0->accelerate) (1.3.0)\n",
      "Downloading accelerate-1.8.1-py3-none-any.whl (365 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m365.3/365.3 kB\u001b[0m \u001b[31m17.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading huggingface_hub-0.33.2-py3-none-any.whl (515 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m515.4/515.4 kB\u001b[0m \u001b[31m61.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading safetensors-0.5.3-cp38-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (471 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m471.6/471.6 kB\u001b[0m \u001b[31m43.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading fsspec-2025.5.1-py3-none-any.whl (199 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m199.1/199.1 kB\u001b[0m \u001b[31m22.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading hf_xet-1.1.5-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.1/3.1 MB\u001b[0m \u001b[31m23.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0mm\n",
      "\u001b[?25hDownloading tqdm-4.67.1-py3-none-any.whl (78 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m78.5/78.5 kB\u001b[0m \u001b[31m10.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: tqdm, safetensors, hf-xet, fsspec, huggingface_hub, accelerate\n",
      "  Attempting uninstall: fsspec\n",
      "    Found existing installation: fsspec 2023.4.0\n",
      "    Uninstalling fsspec-2023.4.0:\n",
      "      Successfully uninstalled fsspec-2023.4.0\n",
      "Successfully installed accelerate-1.8.1 fsspec-2025.5.1 hf-xet-1.1.5 huggingface_hub-0.33.2 safetensors-0.5.3 tqdm-4.67.1\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.1.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython -m pip install --upgrade pip\u001b[0m\n",
      "Collecting diffusers\n",
      "  Downloading diffusers-0.34.0-py3-none-any.whl.metadata (20 kB)\n",
      "Requirement already satisfied: importlib_metadata in /usr/lib/python3/dist-packages (from diffusers) (4.6.4)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from diffusers) (3.9.0)\n",
      "Requirement already satisfied: huggingface-hub>=0.27.0 in /usr/local/lib/python3.10/dist-packages (from diffusers) (0.33.2)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from diffusers) (1.24.1)\n",
      "Collecting regex!=2019.12.17 (from diffusers)\n",
      "  Downloading regex-2024.11.6-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (40 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m40.5/40.5 kB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from diffusers) (2.31.0)\n",
      "Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from diffusers) (0.5.3)\n",
      "Requirement already satisfied: Pillow in /usr/local/lib/python3.10/dist-packages (from diffusers) (9.3.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.27.0->diffusers) (2025.5.1)\n",
      "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.27.0->diffusers) (23.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.27.0->diffusers) (6.0.1)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.27.0->diffusers) (4.67.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.27.0->diffusers) (4.4.0)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.27.0->diffusers) (1.1.5)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->diffusers) (2.1.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->diffusers) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->diffusers) (1.26.13)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->diffusers) (2022.12.7)\n",
      "Downloading diffusers-0.34.0-py3-none-any.whl (3.8 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.8/3.8 MB\u001b[0m \u001b[31m13.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading regex-2024.11.6-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (781 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m781.7/781.7 kB\u001b[0m \u001b[31m25.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: regex, diffusers\n",
      "Successfully installed diffusers-0.34.0 regex-2024.11.6\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.1.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython -m pip install --upgrade pip\u001b[0m\n",
      "Collecting xformers\n",
      "  Downloading xformers-0.0.31.post1-cp39-abi3-manylinux_2_28_x86_64.whl.metadata (1.1 kB)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from xformers) (1.24.1)\n",
      "Collecting torch==2.7.1 (from xformers)\n",
      "  Downloading torch-2.7.1-cp310-cp310-manylinux_2_28_x86_64.whl.metadata (29 kB)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch==2.7.1->xformers) (3.9.0)\n",
      "Collecting typing-extensions>=4.10.0 (from torch==2.7.1->xformers)\n",
      "  Downloading typing_extensions-4.14.1-py3-none-any.whl.metadata (3.0 kB)\n",
      "Collecting sympy>=1.13.3 (from torch==2.7.1->xformers)\n",
      "  Downloading sympy-1.14.0-py3-none-any.whl.metadata (12 kB)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch==2.7.1->xformers) (3.0)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch==2.7.1->xformers) (3.1.2)\n",
      "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch==2.7.1->xformers) (2025.5.1)\n",
      "Collecting nvidia-cuda-nvrtc-cu12==12.6.77 (from torch==2.7.1->xformers)\n",
      "  Downloading nvidia_cuda_nvrtc_cu12-12.6.77-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cuda-runtime-cu12==12.6.77 (from torch==2.7.1->xformers)\n",
      "  Downloading nvidia_cuda_runtime_cu12-12.6.77-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cuda-cupti-cu12==12.6.80 (from torch==2.7.1->xformers)\n",
      "  Downloading nvidia_cuda_cupti_cu12-12.6.80-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-cudnn-cu12==9.5.1.17 (from torch==2.7.1->xformers)\n",
      "  Downloading nvidia_cudnn_cu12-9.5.1.17-py3-none-manylinux_2_28_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-cublas-cu12==12.6.4.1 (from torch==2.7.1->xformers)\n",
      "  Downloading nvidia_cublas_cu12-12.6.4.1-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cufft-cu12==11.3.0.4 (from torch==2.7.1->xformers)\n",
      "  Downloading nvidia_cufft_cu12-11.3.0.4-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-curand-cu12==10.3.7.77 (from torch==2.7.1->xformers)\n",
      "  Downloading nvidia_curand_cu12-10.3.7.77-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cusolver-cu12==11.7.1.2 (from torch==2.7.1->xformers)\n",
      "  Downloading nvidia_cusolver_cu12-11.7.1.2-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-cusparse-cu12==12.5.4.2 (from torch==2.7.1->xformers)\n",
      "  Downloading nvidia_cusparse_cu12-12.5.4.2-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-cusparselt-cu12==0.6.3 (from torch==2.7.1->xformers)\n",
      "  Downloading nvidia_cusparselt_cu12-0.6.3-py3-none-manylinux2014_x86_64.whl.metadata (6.8 kB)\n",
      "Collecting nvidia-nccl-cu12==2.26.2 (from torch==2.7.1->xformers)\n",
      "  Downloading nvidia_nccl_cu12-2.26.2-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (2.0 kB)\n",
      "Collecting nvidia-nvtx-cu12==12.6.77 (from torch==2.7.1->xformers)\n",
      "  Downloading nvidia_nvtx_cu12-12.6.77-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-nvjitlink-cu12==12.6.85 (from torch==2.7.1->xformers)\n",
      "  Downloading nvidia_nvjitlink_cu12-12.6.85-py3-none-manylinux2010_x86_64.manylinux_2_12_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cufile-cu12==1.11.1.6 (from torch==2.7.1->xformers)\n",
      "  Downloading nvidia_cufile_cu12-1.11.1.6-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting triton==3.3.1 (from torch==2.7.1->xformers)\n",
      "  Downloading triton-3.3.1-cp310-cp310-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (1.5 kB)\n",
      "Requirement already satisfied: setuptools>=40.8.0 in /usr/local/lib/python3.10/dist-packages (from triton==3.3.1->torch==2.7.1->xformers) (68.2.2)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy>=1.13.3->torch==2.7.1->xformers) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch==2.7.1->xformers) (2.1.2)\n",
      "Downloading xformers-0.0.31.post1-cp39-abi3-manylinux_2_28_x86_64.whl (117.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m117.1/117.1 MB\u001b[0m \u001b[31m21.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading torch-2.7.1-cp310-cp310-manylinux_2_28_x86_64.whl (821.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m821.2/821.2 MB\u001b[0m \u001b[31m8.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cublas_cu12-12.6.4.1-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (393.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m393.1/393.1 MB\u001b[0m \u001b[31m16.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.6.80-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (8.9 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.9/8.9 MB\u001b[0m \u001b[31m165.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.6.77-py3-none-manylinux2014_x86_64.whl (23.7 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m23.7/23.7 MB\u001b[0m \u001b[31m131.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.6.77-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (897 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m897.7/897.7 kB\u001b[0m \u001b[31m86.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cudnn_cu12-9.5.1.17-py3-none-manylinux_2_28_x86_64.whl (571.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m571.0/571.0 MB\u001b[0m \u001b[31m11.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cufft_cu12-11.3.0.4-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (200.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m200.2/200.2 MB\u001b[0m \u001b[31m29.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cufile_cu12-1.11.1.6-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (1.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m111.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_curand_cu12-10.3.7.77-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (56.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m31.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0mm\n",
      "\u001b[?25hDownloading nvidia_cusolver_cu12-11.7.1.2-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (158.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m158.2/158.2 MB\u001b[0m \u001b[31m38.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cusparse_cu12-12.5.4.2-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (216.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m216.6/216.6 MB\u001b[0m \u001b[31m28.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cusparselt_cu12-0.6.3-py3-none-manylinux2014_x86_64.whl (156.8 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m156.8/156.8 MB\u001b[0m \u001b[31m36.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_nccl_cu12-2.26.2-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (201.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m201.3/201.3 MB\u001b[0m \u001b[31m29.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.6.85-py3-none-manylinux2010_x86_64.manylinux_2_12_x86_64.whl (19.7 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m19.7/19.7 MB\u001b[0m \u001b[31m132.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_nvtx_cu12-12.6.77-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (89 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m89.3/89.3 kB\u001b[0m \u001b[31m22.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading triton-3.3.1-cp310-cp310-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (155.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m155.6/155.6 MB\u001b[0m \u001b[31m38.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading sympy-1.14.0-py3-none-any.whl (6.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.3/6.3 MB\u001b[0m \u001b[31m129.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading typing_extensions-4.14.1-py3-none-any.whl (43 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.9/43.9 kB\u001b[0m \u001b[31m8.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: nvidia-cusparselt-cu12, typing-extensions, triton, sympy, nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufile-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cufft-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, torch, xformers\n",
      "  Attempting uninstall: typing-extensions\n",
      "    Found existing installation: typing_extensions 4.4.0\n",
      "    Uninstalling typing_extensions-4.4.0:\n",
      "      Successfully uninstalled typing_extensions-4.4.0\n",
      "  Attempting uninstall: triton\n",
      "    Found existing installation: triton 2.1.0\n",
      "    Uninstalling triton-2.1.0:\n",
      "      Successfully uninstalled triton-2.1.0\n",
      "  Attempting uninstall: sympy\n",
      "    Found existing installation: sympy 1.12\n",
      "    Uninstalling sympy-1.12:\n",
      "      Successfully uninstalled sympy-1.12\n",
      "  Attempting uninstall: torch\n",
      "    Found existing installation: torch 2.1.0+cu118\n",
      "    Uninstalling torch-2.1.0+cu118:\n",
      "      Successfully uninstalled torch-2.1.0+cu118\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "torchaudio 2.1.0+cu118 requires torch==2.1.0, but you have torch 2.7.1 which is incompatible.\n",
      "torchvision 0.16.0+cu118 requires torch==2.1.0, but you have torch 2.7.1 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed nvidia-cublas-cu12-12.6.4.1 nvidia-cuda-cupti-cu12-12.6.80 nvidia-cuda-nvrtc-cu12-12.6.77 nvidia-cuda-runtime-cu12-12.6.77 nvidia-cudnn-cu12-9.5.1.17 nvidia-cufft-cu12-11.3.0.4 nvidia-cufile-cu12-1.11.1.6 nvidia-curand-cu12-10.3.7.77 nvidia-cusolver-cu12-11.7.1.2 nvidia-cusparse-cu12-12.5.4.2 nvidia-cusparselt-cu12-0.6.3 nvidia-nccl-cu12-2.26.2 nvidia-nvjitlink-cu12-12.6.85 nvidia-nvtx-cu12-12.6.77 sympy-1.14.0 torch-2.7.1 triton-3.3.1 typing-extensions-4.14.1 xformers-0.0.31.post1\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.1.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython -m pip install --upgrade pip\u001b[0m\n",
      "Collecting seaborn\n",
      "  Downloading seaborn-0.13.2-py3-none-any.whl.metadata (5.4 kB)\n",
      "Requirement already satisfied: numpy!=1.24.0,>=1.20 in /usr/local/lib/python3.10/dist-packages (from seaborn) (1.24.1)\n",
      "Collecting pandas>=1.2 (from seaborn)\n",
      "  Downloading pandas-2.3.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (91 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m91.2/91.2 kB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: matplotlib!=3.6.1,>=3.4 in /usr/local/lib/python3.10/dist-packages (from seaborn) (3.10.3)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (1.3.2)\n",
      "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (4.58.5)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (1.4.8)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (23.2)\n",
      "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.10/dist-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (9.3.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /usr/lib/python3/dist-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (2.4.7)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (2.8.2)\n",
      "Collecting pytz>=2020.1 (from pandas>=1.2->seaborn)\n",
      "  Downloading pytz-2025.2-py2.py3-none-any.whl.metadata (22 kB)\n",
      "Collecting tzdata>=2022.7 (from pandas>=1.2->seaborn)\n",
      "  Downloading tzdata-2025.2-py2.py3-none-any.whl.metadata (1.4 kB)\n",
      "Requirement already satisfied: six>=1.5 in /usr/lib/python3/dist-packages (from python-dateutil>=2.7->matplotlib!=3.6.1,>=3.4->seaborn) (1.16.0)\n",
      "Downloading seaborn-0.13.2-py3-none-any.whl (294 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m294.9/294.9 kB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading pandas-2.3.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (12.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.3/12.3 MB\u001b[0m \u001b[31m35.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading pytz-2025.2-py2.py3-none-any.whl (509 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m509.2/509.2 kB\u001b[0m \u001b[31m58.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading tzdata-2025.2-py2.py3-none-any.whl (347 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m347.8/347.8 kB\u001b[0m \u001b[31m46.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: pytz, tzdata, pandas, seaborn\n",
      "Successfully installed pandas-2.3.1 pytz-2025.2 seaborn-0.13.2 tzdata-2025.2\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.1.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython -m pip install --upgrade pip\u001b[0m\n",
      "Requirement already satisfied: safetensors in /usr/local/lib/python3.10/dist-packages (0.5.3)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.1.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython -m pip install --upgrade pip\u001b[0m\n",
      "Collecting deepspeed\n",
      "  Downloading deepspeed-0.17.2.tar.gz (1.6 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m8.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting einops (from deepspeed)\n",
      "  Downloading einops-0.8.1-py3-none-any.whl.metadata (13 kB)\n",
      "Collecting hjson (from deepspeed)\n",
      "  Downloading hjson-3.1.0-py3-none-any.whl.metadata (2.6 kB)\n",
      "Collecting msgpack (from deepspeed)\n",
      "  Downloading msgpack-1.1.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (8.4 kB)\n",
      "Collecting ninja (from deepspeed)\n",
      "  Downloading ninja-1.11.1.4-py3-none-manylinux_2_12_x86_64.manylinux2010_x86_64.whl.metadata (5.0 kB)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from deepspeed) (1.24.1)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from deepspeed) (23.2)\n",
      "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from deepspeed) (5.9.6)\n",
      "Collecting py-cpuinfo (from deepspeed)\n",
      "  Downloading py_cpuinfo-9.0.0-py3-none-any.whl.metadata (794 bytes)\n",
      "Collecting pydantic>=2.0.0 (from deepspeed)\n",
      "  Downloading pydantic-2.11.7-py3-none-any.whl.metadata (67 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m68.0/68.0 kB\u001b[0m \u001b[31m19.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from deepspeed) (2.7.1)\n",
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from deepspeed) (4.67.1)\n",
      "Collecting nvidia-ml-py (from deepspeed)\n",
      "  Downloading nvidia_ml_py-12.575.51-py3-none-any.whl.metadata (9.3 kB)\n",
      "Collecting annotated-types>=0.6.0 (from pydantic>=2.0.0->deepspeed)\n",
      "  Downloading annotated_types-0.7.0-py3-none-any.whl.metadata (15 kB)\n",
      "Collecting pydantic-core==2.33.2 (from pydantic>=2.0.0->deepspeed)\n",
      "  Downloading pydantic_core-2.33.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.8 kB)\n",
      "Requirement already satisfied: typing-extensions>=4.12.2 in /usr/local/lib/python3.10/dist-packages (from pydantic>=2.0.0->deepspeed) (4.14.1)\n",
      "Collecting typing-inspection>=0.4.0 (from pydantic>=2.0.0->deepspeed)\n",
      "  Downloading typing_inspection-0.4.1-py3-none-any.whl.metadata (2.6 kB)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch->deepspeed) (3.9.0)\n",
      "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.10/dist-packages (from torch->deepspeed) (1.14.0)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch->deepspeed) (3.0)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch->deepspeed) (3.1.2)\n",
      "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch->deepspeed) (2025.5.1)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.10/dist-packages (from torch->deepspeed) (12.6.77)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.10/dist-packages (from torch->deepspeed) (12.6.77)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.10/dist-packages (from torch->deepspeed) (12.6.80)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.5.1.17 in /usr/local/lib/python3.10/dist-packages (from torch->deepspeed) (9.5.1.17)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.10/dist-packages (from torch->deepspeed) (12.6.4.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.10/dist-packages (from torch->deepspeed) (11.3.0.4)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.10/dist-packages (from torch->deepspeed) (10.3.7.77)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.10/dist-packages (from torch->deepspeed) (11.7.1.2)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.10/dist-packages (from torch->deepspeed) (12.5.4.2)\n",
      "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.3 in /usr/local/lib/python3.10/dist-packages (from torch->deepspeed) (0.6.3)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.26.2 in /usr/local/lib/python3.10/dist-packages (from torch->deepspeed) (2.26.2)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.10/dist-packages (from torch->deepspeed) (12.6.77)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.10/dist-packages (from torch->deepspeed) (12.6.85)\n",
      "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.10/dist-packages (from torch->deepspeed) (1.11.1.6)\n",
      "Requirement already satisfied: triton==3.3.1 in /usr/local/lib/python3.10/dist-packages (from torch->deepspeed) (3.3.1)\n",
      "Requirement already satisfied: setuptools>=40.8.0 in /usr/local/lib/python3.10/dist-packages (from triton==3.3.1->torch->deepspeed) (68.2.2)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy>=1.13.3->torch->deepspeed) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch->deepspeed) (2.1.2)\n",
      "Downloading pydantic-2.11.7-py3-none-any.whl (444 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m444.8/444.8 kB\u001b[0m \u001b[31m13.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading pydantic_core-2.33.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m21.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading einops-0.8.1-py3-none-any.whl (64 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m64.4/64.4 kB\u001b[0m \u001b[31m11.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading hjson-3.1.0-py3-none-any.whl (54 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m54.0/54.0 kB\u001b[0m \u001b[31m9.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading msgpack-1.1.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (408 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m408.6/408.6 kB\u001b[0m \u001b[31m22.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading ninja-1.11.1.4-py3-none-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (422 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m422.8/422.8 kB\u001b[0m \u001b[31m24.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_ml_py-12.575.51-py3-none-any.whl (47 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m47.5/47.5 kB\u001b[0m \u001b[31m8.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading py_cpuinfo-9.0.0-py3-none-any.whl (22 kB)\n",
      "Downloading annotated_types-0.7.0-py3-none-any.whl (13 kB)\n",
      "Downloading typing_inspection-0.4.1-py3-none-any.whl (14 kB)\n",
      "Building wheels for collected packages: deepspeed\n",
      "  Building wheel for deepspeed (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for deepspeed: filename=deepspeed-0.17.2-py3-none-any.whl size=1699832 sha256=e5a96049cc35a43dc5f8f231bded15f5432715f248458801d88e00949d481b96\n",
      "  Stored in directory: /root/.cache/pip/wheels/aa/90/50/536155ec6a2eb601f3052cfe5cab26cbb88f4a012c99566400\n",
      "Successfully built deepspeed\n",
      "Installing collected packages: py-cpuinfo, nvidia-ml-py, hjson, typing-inspection, pydantic-core, ninja, msgpack, einops, annotated-types, pydantic, deepspeed\n",
      "Successfully installed annotated-types-0.7.0 deepspeed-0.17.2 einops-0.8.1 hjson-3.1.0 msgpack-1.1.1 ninja-1.11.1.4 nvidia-ml-py-12.575.51 py-cpuinfo-9.0.0 pydantic-2.11.7 pydantic-core-2.33.2 typing-inspection-0.4.1\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.1.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython -m pip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install matplotlib\n",
    "!pip install mediapy\n",
    "!pip install numpy\n",
    "# For advanced optimizations, consider installing the following:\n",
    "!pip install accelerate\n",
    "!pip install diffusers\n",
    "!pip install xformers\n",
    "!pip install seaborn\n",
    "!pip install safetensors\n",
    "!pip install deepspeed\n",
    "!apt-get update -y && apt-get install -y portaudio19-dev\n",
    "!pip install pyaudio\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f940a53d-a8c0-49a4-9a32-fc571004ee4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Get:1 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  InRelease [1581 B]\n",
      "Get:2 http://security.ubuntu.com/ubuntu jammy-security InRelease [129 kB]      \n",
      "Get:3 http://archive.ubuntu.com/ubuntu jammy InRelease [270 kB]                \n",
      "Get:4 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  Packages [1804 kB]\n",
      "Get:5 http://security.ubuntu.com/ubuntu jammy-security/universe amd64 Packages [1266 kB]\n",
      "Get:6 https://ppa.launchpadcontent.net/deadsnakes/ppa/ubuntu jammy InRelease [18.1 kB]\n",
      "Get:7 http://archive.ubuntu.com/ubuntu jammy-updates InRelease [128 kB]        \n",
      "Get:8 https://ppa.launchpadcontent.net/deadsnakes/ppa/ubuntu jammy/main amd64 Packages [32.9 kB]\n",
      "Get:9 http://archive.ubuntu.com/ubuntu jammy-backports InRelease [127 kB]      \n",
      "Get:10 http://security.ubuntu.com/ubuntu jammy-security/restricted amd64 Packages [4795 kB]\n",
      "Get:11 http://archive.ubuntu.com/ubuntu jammy/multiverse amd64 Packages [266 kB]\n",
      "Get:12 http://archive.ubuntu.com/ubuntu jammy/universe amd64 Packages [17.5 MB]\n",
      "Get:13 http://security.ubuntu.com/ubuntu jammy-security/main amd64 Packages [3102 kB]\n",
      "Get:14 http://security.ubuntu.com/ubuntu jammy-security/multiverse amd64 Packages [48.5 kB]\n",
      "Get:15 http://archive.ubuntu.com/ubuntu jammy/main amd64 Packages [1792 kB]    \n",
      "Get:16 http://archive.ubuntu.com/ubuntu jammy/restricted amd64 Packages [164 kB]\n",
      "Get:17 http://archive.ubuntu.com/ubuntu jammy-updates/multiverse amd64 Packages [56.4 kB]\n",
      "Get:18 http://archive.ubuntu.com/ubuntu jammy-updates/restricted amd64 Packages [4948 kB]\n",
      "Get:19 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 Packages [3414 kB]\n",
      "Get:20 http://archive.ubuntu.com/ubuntu jammy-updates/universe amd64 Packages [1569 kB]\n",
      "Get:21 http://archive.ubuntu.com/ubuntu jammy-backports/main amd64 Packages [113 kB]\n",
      "Get:22 http://archive.ubuntu.com/ubuntu jammy-backports/universe amd64 Packages [38.4 kB]\n",
      "Fetched 41.6 MB in 6s (7537 kB/s)                            \n",
      "Reading package lists... Done\n",
      "Reading package lists... Done\n",
      "Building dependency tree... Done\n",
      "Reading state information... Done\n",
      "The following additional packages will be installed:\n",
      "  libaio1\n",
      "The following NEW packages will be installed:\n",
      "  libaio-dev libaio1\n",
      "0 upgraded, 2 newly installed, 0 to remove and 143 not upgraded.\n",
      "Need to get 28.4 kB of archives.\n",
      "After this operation, 110 kB of additional disk space will be used.\n",
      "Get:1 http://archive.ubuntu.com/ubuntu jammy/main amd64 libaio1 amd64 0.3.112-13build1 [7176 B]\n",
      "Get:2 http://archive.ubuntu.com/ubuntu jammy/main amd64 libaio-dev amd64 0.3.112-13build1 [21.2 kB]\n",
      "Fetched 28.4 kB in 1s (29.4 kB/s)    \n",
      "debconf: delaying package configuration, since apt-utils is not installed\n",
      "Selecting previously unselected package libaio1:amd64.\n",
      "(Reading database ... 20729 files and directories currently installed.)\n",
      "Preparing to unpack .../libaio1_0.3.112-13build1_amd64.deb ...\n",
      "Unpacking libaio1:amd64 (0.3.112-13build1) ...\n",
      "Selecting previously unselected package libaio-dev:amd64.\n",
      "Preparing to unpack .../libaio-dev_0.3.112-13build1_amd64.deb ...\n",
      "Unpacking libaio-dev:amd64 (0.3.112-13build1) ...\n",
      "Setting up libaio1:amd64 (0.3.112-13build1) ...\n",
      "Setting up libaio-dev:amd64 (0.3.112-13build1) ...\n",
      "Processing triggers for libc-bin (2.35-0ubuntu3.4) ...\n"
     ]
    }
   ],
   "source": [
    "!apt-get update && apt-get install -y libaio-dev\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "72c4316f-e85a-4be6-9641-3300230a135d",
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir -p /root/.triton/autotune\n",
    "#Triton (used by xFormers or FlashAttention) is trying to autotune kernels.\n",
    "\n",
    "#df command is likely being used internally to check disk usage of Triton cache.\n",
    "\n",
    "#Safe to ignore unless you rely on persistent Triton tuning across sessions.\n",
    "\n",
    "#🛠️ Fix (optional): Create the directory manually using the code above."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae49089a",
   "metadata": {},
   "source": [
    "# Setup Section for the Arc Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b678317",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ARC Output Head Dim: 9000\n",
      "\n",
      "-----------------------------------------------------------------------------\n",
      "Initializing Configuration and Model for ARC with EnhancedCTMDiffusion\n",
      "-----------------------------------------------------------------------------\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "EnhancedCTMConfig.__init__() got an unexpected keyword argument 'enable_consciousness_controller'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 482\u001b[0m\n\u001b[1;32m    479\u001b[0m                 \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWarning: JEPA training is enabled but use_dynamic_entropy_patcher is False. JEPA relies on the patch embeddings from LearnedBytePatcherEncoder.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    481\u001b[0m \u001b[38;5;66;03m# --- Model Configuration ---\u001b[39;00m\n\u001b[0;32m--> 482\u001b[0m config_arc_diffusion \u001b[38;5;241m=\u001b[39m \u001b[43mEnhancedCTMConfig\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    483\u001b[0m \u001b[43m    \u001b[49m\u001b[43md_model\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m512\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    484\u001b[0m \u001b[43m    \u001b[49m\u001b[43mn_heads\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m8\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    485\u001b[0m \u001b[43m    \u001b[49m\u001b[43mn_layers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m24\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    486\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_sequence_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mMAX_SEQUENCE_LENGTH\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    487\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdropout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.1\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    488\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_dynamic_entropy_patcher\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    489\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpatch_embedding_dim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m256\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    490\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpatch_grid_width\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m16\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    491\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpatch_encoder_cnn_channels\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m64\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    492\u001b[0m \u001b[43m    \u001b[49m\u001b[43mentropy_patcher_threshold_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mglobal\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    493\u001b[0m \u001b[43m    \u001b[49m\u001b[43mentropy_patcher_global_threshold\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.75\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    494\u001b[0m \u001b[43m    \u001b[49m\u001b[43mentropy_patcher_relative_threshold\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.1\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    495\u001b[0m \u001b[43m    \u001b[49m\u001b[43mentropy_patcher_min_patch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m4\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    496\u001b[0m \u001b[43m    \u001b[49m\u001b[43mentropy_patcher_max_patch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m128\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    497\u001b[0m \u001b[43m    \u001b[49m\u001b[43mentropy_model_byte_vocab_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m256\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    498\u001b[0m \u001b[43m    \u001b[49m\u001b[43mentropy_model_embedding_dim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m64\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    499\u001b[0m \u001b[43m    \u001b[49m\u001b[43mentropy_model_hidden_dim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m128\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    500\u001b[0m \u001b[43m    \u001b[49m\u001b[43mentropy_model_num_layers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    501\u001b[0m \u001b[43m    \u001b[49m\u001b[43mentropy_model_dropout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.1\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    502\u001b[0m \u001b[43m    \u001b[49m\u001b[43mentropy_model_loss_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.1\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    503\u001b[0m \u001b[43m    \u001b[49m\u001b[43mctm_input_dim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m256\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    504\u001b[0m \u001b[43m    \u001b[49m\u001b[43mctm_d_model\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m512\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    505\u001b[0m \u001b[43m    \u001b[49m\u001b[43mctm_iterations\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    506\u001b[0m \u001b[43m    \u001b[49m\u001b[43mctm_heads\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m8\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    507\u001b[0m \u001b[43m    \u001b[49m\u001b[43mctm_out_dims\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m512\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    508\u001b[0m \u001b[43m    \u001b[49m\u001b[43mctm_neuron_select_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mbio_multi_objective\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    509\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43msubquadratic\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    510\u001b[0m \u001b[43m    \u001b[49m\u001b[43msubquadratic_attn_epsilon\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1e-6\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    511\u001b[0m \u001b[43m    \u001b[49m\u001b[43msubquadratic_attn_poly_degree\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    512\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_qkv_bias\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    513\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpositional_embedding_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mmulti-learnable-fourier\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    514\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpositional_embedding_dim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    515\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreshape_patch_sequence_to_grid\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    516\u001b[0m \u001b[43m    \u001b[49m\u001b[43menable_pipeline_parallelism\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    517\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpipeline_stages\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m4\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    518\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpipeline_overlap_ratio\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.7\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    519\u001b[0m \u001b[43m    \u001b[49m\u001b[43menable_adaptive_batching\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    520\u001b[0m \u001b[43m    \u001b[49m\u001b[43minitial_batch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m32\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    521\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmin_batch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m8\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    522\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_batch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m256\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    523\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbatch_adaptation_frequency\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m100\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    524\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmemory_threshold_high\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.85\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    525\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmemory_threshold_low\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.6\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    526\u001b[0m \u001b[43m    \u001b[49m\u001b[43menable_smart_sampling\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    527\u001b[0m \u001b[43m    \u001b[49m\u001b[43msample_importance_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.6\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    528\u001b[0m \u001b[43m    \u001b[49m\u001b[43msample_diversity_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.4\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    529\u001b[0m \u001b[43m    \u001b[49m\u001b[43minitial_sample_ratio\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.3\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    530\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcomplexity_analysis_enabled\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    531\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnum_inputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    532\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    533\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_dims\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m64\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    534\u001b[0m \u001b[43m    \u001b[49m\u001b[43mssl_dim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m128\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    535\u001b[0m \u001b[43m    \u001b[49m\u001b[43mssl_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.1\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    536\u001b[0m \u001b[43m    \u001b[49m\u001b[43mssl_temperature\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.07\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    537\u001b[0m \u001b[43m    \u001b[49m\u001b[43mssl_noise_std\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.1\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    538\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_spatial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    539\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_wina_attention\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    540\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_tasks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m50\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    541\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdiffusion_steps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1000\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    542\u001b[0m \u001b[43m    \u001b[49m\u001b[43mctm_diffusion_coupling_strength\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.8\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    543\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvocab_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    544\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_audio_bytes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    545\u001b[0m \u001b[43m    \u001b[49m\u001b[43munet_input_feature_dim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mMAX_SEQUENCE_LENGTH\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m/\u001b[39;49m\u001b[38;5;241;43m/\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m4\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;66;43;03m# Calculated based on float32 audio\u001b[39;49;00m\n\u001b[1;32m    546\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlocal_hebbian_loss_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.01\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    547\u001b[0m \u001b[43m    \u001b[49m\u001b[43menable_consciousness_controller\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    548\u001b[0m \u001b[43m    \u001b[49m\u001b[43mconsciousness_max_attention_steps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m100\u001b[39;49m\n\u001b[1;32m    549\u001b[0m \u001b[43m)\u001b[49m\n\u001b[1;32m    550\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m✓ EnhancedCTMConfig for ARC (config_arc_diffusion) created.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    552\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mEnhancedCTMDiffusion\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mglobals\u001b[39m() \u001b[38;5;129;01mand\u001b[39;00m EnhancedCTMDiffusion \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[0;31mTypeError\u001b[0m: EnhancedCTMConfig.__init__() got an unexpected keyword argument 'enable_consciousness_controller'"
     ]
    }
   ],
   "source": [
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import random\n",
    "from typing import Optional, Callable, Tuple, Dict, Any, List, Union\n",
    "from dataclasses import dataclass, field\n",
    "import math\n",
    "import numpy as np\n",
    "import warnings # For ARCGridOutputSpace warnings\n",
    "import sys\n",
    "import os\n",
    "import json\n",
    "import glob\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.optim as optim\n",
    "from accelerate import Accelerator\n",
    "\n",
    "WORKSPACE_ROOT = \"/workspace/Arc-AGI-2\"\n",
    "\n",
    "ARC_TRAIN_DIR = os.path.join(WORKSPACE_ROOT, \"contineous_thought_machines\", \"data\", \"training\")\n",
    "ARC_EVAL_DIR = os.path.join(WORKSPACE_ROOT, \"contineous_thought_machines\", \"data\", \"evaluation\")\n",
    "\n",
    "def find_json_file(filename, search_dir):\n",
    "    \"\"\"\n",
    "    Search for a specific JSON file by filename in a given directory tree.\n",
    "    \"\"\"\n",
    "    for root, _, files in os.walk(search_dir):\n",
    "        if filename in files:\n",
    "            return os.path.join(root, filename)\n",
    "    return None\n",
    "\n",
    "def resolve_json_files(directory):\n",
    "    \"\"\"\n",
    "    Collect all JSON files in the given directory.\n",
    "    If any is missing, try to find it elsewhere in the workspace.\n",
    "    Returns a list of absolute file paths.\n",
    "    \"\"\"\n",
    "    json_files = []\n",
    "    # Get all JSON files that exist in the given directory\n",
    "    for file in os.listdir(directory):\n",
    "        if file.endswith(\".json\"):\n",
    "            abs_path = os.path.join(directory, file)\n",
    "            if os.path.exists(abs_path):\n",
    "                json_files.append(abs_path)\n",
    "            else:\n",
    "                # Try to find it in the workspace\n",
    "                print(f\"[WARN] File not found in expected path: {abs_path}. Searching workspace...\")\n",
    "                found = find_json_file(file, WORKSPACE_ROOT)\n",
    "                if found:\n",
    "                    print(f\"[INFO] Found {file} at: {found}\")\n",
    "                    json_files.append(found)\n",
    "                else:\n",
    "                    print(f\"[ERROR] Could not find {file} anywhere in {WORKSPACE_ROOT}\")\n",
    "    return json_files\n",
    "\n",
    "# Use the function for both training and evaluation dirs\n",
    "train_json_files = resolve_json_files(ARC_TRAIN_DIR)\n",
    "eval_json_files = resolve_json_files(ARC_EVAL_DIR)\n",
    "\n",
    "print(f\"✅ Found {len(train_json_files)} training JSON files.\")\n",
    "print(f\"✅ Found {len(eval_json_files)} evaluation JSON files.\")\n",
    "\n",
    "# Example: show first few\n",
    "print(\"Training files:\", train_json_files[:3])\n",
    "print(\"Evaluation files:\", eval_json_files[:3])\n",
    "\n",
    "\n",
    "MAX_GRID_SIZE = (30, 30)\n",
    "NUM_ARC_SYMBOLS = 10\n",
    "PADDING_VALUE = -1 # A value not in 0-9 to be ignored by the loss function\n",
    "\n",
    "# Configuration for ARC-AGI-2 Training (shared constants)\n",
    "ARC_INPUT_FLAT_DIM = MAX_GRID_SIZE[0] * MAX_GRID_SIZE[1]\n",
    "\n",
    "print(f\"Using MAX_GRID_SIZE: {MAX_GRID_SIZE}\")\n",
    "print(f\"Using NUM_ARC_SYMBOLS: {NUM_ARC_SYMBOLS}\")\n",
    "print(f\"Using ARC_INPUT_FLAT_DIM: {ARC_INPUT_FLAT_DIM}\") \n",
    "\n",
    "# Ensure the workspace root is in sys.path for correct module resolution.\n",
    "if WORKSPACE_ROOT not in sys.path:\n",
    "    sys.path.insert(0, WORKSPACE_ROOT)\n",
    "    print(f\"[INFO] Added workspace root to sys.path: {WORKSPACE_ROOT}\")\n",
    "\n",
    "# --- Constants and Configs ---\n",
    "MAX_GRID_SIZE = (30, 30)\n",
    "PADDING_VALUE = -1\n",
    "NUM_ARC_SYMBOLS = 10\n",
    "ARC_INPUT_FLAT_DIM = MAX_GRID_SIZE[0] * MAX_GRID_SIZE[1]\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "LEARNING_RATE = 1e-4\n",
    "CHECKPOINT_DIR = \"checkpoints\"\n",
    "ARC_TRAIN_DIR = \"/workspace/Arc-AGI-2/contineous_thought_machines/data/training\" #Training Dataset Directory\n",
    "\n",
    "ACCELERATE_AVAILABLE = True\n",
    "try:\n",
    "    from accelerate import Accelerator\n",
    "except ImportError:\n",
    "    ACCELERATE_AVAILABLE = False\n",
    "    Accelerator = None\n",
    "\n",
    "# Check for xformers\n",
    "XFORMERS_AVAILABLE = False\n",
    "if device == \"cuda\":\n",
    "    try:\n",
    "        import xformers\n",
    "        XFORMERS_AVAILABLE = True\n",
    "    except ImportError:\n",
    "        pass\n",
    "\n",
    "# Check for torch.compile\n",
    "TORCH_COMPILE_AVAILABLE = hasattr(torch, 'compile')\n",
    "\n",
    "# Check for deepspeed\n",
    "DEEPSPEED_AVAILABLE = False\n",
    "if device == \"cuda\":\n",
    "    try:\n",
    "        import deepspeed\n",
    "        DEEPSPEED_AVAILABLE = True\n",
    "    except ImportError:\n",
    "        pass\n",
    "\n",
    "# A reasonable default for dataloader config\n",
    "OPTIMIZED_DATALOADER_CONFIG = {\n",
    "    \"num_workers\": 4,\n",
    "    \"pin_memory\": True,\n",
    "    \"prefetch_factor\": 2\n",
    "} if torch.cuda.is_available() else {}\n",
    "\n",
    "# --- Context: 2D Grid Padding (from original code) ---\n",
    "# This function handles padding at the 2D grid level, before serialization.\n",
    "def pad_grid(grid_list, max_dims, pad_value):\n",
    "    \"\"\"Pads a 2D grid to specified maximum dimensions.\"\"\"\n",
    "    grid_np = np.array(grid_list, dtype=np.int32)\n",
    "    padded_grid = np.full(max_dims, pad_value, dtype=np.int32)\n",
    "    h, w = grid_np.shape\n",
    "    padded_grid[:h, :w] = grid_np\n",
    "    return padded_grid\n",
    "\n",
    "# --- Fix: Byte Sequence Padding for the Model --- #\n",
    "# According to the model explanation, the key step is to pad the *serialized byte sequence*\n",
    "# to `config.max_sequence_length`. The function below implements this logic.\n",
    "\n",
    "# Define the model's expected input dimension from the configuration.\n",
    "MAX_SEQUENCE_LENGTH = 8192\n",
    "PADDING_BYTE_VALUE = 0\n",
    "\n",
    "def serialize_and_pad_grid(grid, max_len=MAX_SEQUENCE_LENGTH, pad_value=PADDING_BYTE_VALUE):\n",
    "    \"\"\"\n",
    "    Serializes a grid into a byte sequence and pads it to a fixed length.\n",
    "\n",
    "    This function implements the required padding logic for the LearnedBytePatcherEncoder.\n",
    "    It takes a grid, converts it to a flat byte sequence, and then pads or truncates\n",
    "    it to `max_sequence_length` (8192 bytes), ensuring a fixed-size input for the model.\n",
    "    \n",
    "    Args:\n",
    "        grid (list or np.ndarray): The input ARC grid.\n",
    "        max_len (int): The target length for the byte sequence, corresponding to\n",
    "                       `config.max_sequence_length`.\n",
    "        pad_value (int): The byte value to use for padding (0-255).\n",
    "\n",
    "    Returns:\n",
    "        bytes: The padded byte sequence of length `max_len`.\n",
    "    \"\"\"\n",
    "    # Convert the grid to a NumPy array of single bytes (uint8) and flatten it.\n",
    "    # ARC values (0-9) fit perfectly within a single byte.\n",
    "    flat_array = np.array(grid, dtype=np.uint8).flatten()\n",
    "\n",
    "    # Serialize the flattened array into a raw byte sequence.\n",
    "    byte_sequence = flat_array.tobytes()\n",
    "\n",
    "    # Calculate the number of padding bytes needed.\n",
    "    padding_len = max_len - len(byte_sequence)\n",
    "\n",
    "    if padding_len < 0:\n",
    "        # If the original sequence is too long, truncate it.\n",
    "        padded_sequence = byte_sequence[:max_len]\n",
    "    else:\n",
    "        # If the sequence is shorter, create padding and append it.\n",
    "        padding = bytes([pad_value] * padding_len)\n",
    "        padded_sequence = byte_sequence + padding\n",
    "        \n",
    "    return padded_sequence\n",
    "\n",
    "class NewCustomARCGridDataset(Dataset):\n",
    "    def __init__(self, data_dir, max_grid_size=MAX_GRID_SIZE, padding_value=PADDING_VALUE):\n",
    "        self.data_dir = data_dir\n",
    "        self.task_files = glob.glob(os.path.join(data_dir, \"*.json\"))\n",
    "        self.max_grid_size = max_grid_size\n",
    "        self.padding_value = padding_value\n",
    "        self.tasks = []\n",
    "        print(f\"NewCustomARCGridDataset: Looking for tasks in: {data_dir}\")\n",
    "        if not self.task_files:\n",
    "            print(f\"NewCustomARCGridDataset Warning: No JSON files found in {data_dir}. Attempting fallback search.\")\n",
    "            base_dir = '/workspace/Arc-AGI-2'\n",
    "            self.task_files = []\n",
    "            for root, dirs, files in os.walk(base_dir):\n",
    "                for file in files:\n",
    "                    if file.endswith('.json'):\n",
    "                        self.task_files.append(os.path.join(root, file))\n",
    "            if self.task_files:\n",
    "                print(f\"Found {len(self.task_files)} JSON files via fallback search in {base_dir}\")\n",
    "            else:\n",
    "                print(f\"No JSON files found via fallback search in {base_dir}. Dataset will be empty.\")\n",
    "        for task_file in self.task_files:\n",
    "            try:\n",
    "                with open(task_file, 'r') as f:\n",
    "                    self.tasks.append(json.load(f))\n",
    "            except Exception as e:\n",
    "                print(f\"NewCustomARCGridDataset Warning: Could not load or parse {task_file}: {e}\")\n",
    "        if not self.tasks:\n",
    "            print(f\"NewCustomARCGridDataset Warning: No tasks successfully loaded from {data_dir}.\")\n",
    "        else:\n",
    "            print(f\"NewCustomARCGridDataset: Loaded {len(self.tasks)} ARC tasks from {data_dir}.\")\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.tasks)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        task_data = self.tasks[idx]\n",
    "        processed_task = {'train': [], 'test': [], 'id': os.path.basename(self.task_files[idx]) if idx < len(self.task_files) else 'unknown_task'}\n",
    "\n",
    "        for pair_type in ['train', 'test']:\n",
    "            for item in task_data.get(pair_type, []):\n",
    "                input_grid_list = item.get('input', [])\n",
    "                output_grid_list = item.get('output', [])\n",
    "                \n",
    "                original_input_dims = (len(input_grid_list), len(input_grid_list[0]) if input_grid_list and input_grid_list[0] else (0,0))\n",
    "                original_output_dims = (len(output_grid_list), len(output_grid_list[0]) if output_grid_list and output_grid_list[0] else (0,0))\n",
    "\n",
    "                padded_input_np = pad_grid(input_grid_list, self.max_grid_size, self.padding_value)\n",
    "                padded_output_np = pad_grid(output_grid_list, self.max_grid_size, self.padding_value)\n",
    "                \n",
    "                processed_task[pair_type].append({\n",
    "                    'input': torch.from_numpy(padded_input_np).long(),\n",
    "                    'output': torch.from_numpy(padded_output_np).long(),\n",
    "                    'original_input_dims': original_input_dims,\n",
    "                    'original_output_dims': original_output_dims\n",
    "                })\n",
    "        return processed_task\n",
    "\n",
    "def collate_fn_new_custom_arc(batch_of_tasks):\n",
    "    input_byte_sequences_list = []\n",
    "    target_byte_sequences_for_diffusion_list = []\n",
    "    original_target_grids_for_ce_loss_list = []\n",
    "\n",
    "    for task in batch_of_tasks:\n",
    "        if not isinstance(task, dict):\n",
    "            continue\n",
    "\n",
    "        # Process 'train' pairs from the task\n",
    "        for train_pair in task.get('train', []):\n",
    "            if not isinstance(train_pair, dict) or 'input' not in train_pair or 'output' not in train_pair:\n",
    "                continue\n",
    "\n",
    "            # train_pair['input'] and train_pair['output'] are already padded 2D LongTensors from NewCustomARCGridDataset\n",
    "            input_grid_np = train_pair['input'].numpy() # Convert to numpy for serialize_and_pad_grid\n",
    "            target_grid_np = train_pair['output'].numpy()\n",
    "\n",
    "            # 1. Create input_byte_sequences (uint8)\n",
    "            input_bytes = serialize_and_pad_grid(input_grid_np, max_len=MAX_SEQUENCE_LENGTH, pad_value=PADDING_BYTE_VALUE)\n",
    "            input_byte_sequences_list.append(torch.tensor(list(input_bytes), dtype=torch.uint8))\n",
    "\n",
    "            # 2. Create target_byte_sequences_for_diffusion (uint8)\n",
    "            target_bytes_for_diffusion = serialize_and_pad_grid(target_grid_np, max_len=MAX_SEQUENCE_LENGTH, pad_value=PADDING_BYTE_VALUE)\n",
    "            target_byte_sequences_for_diffusion_list.append(torch.tensor(list(target_bytes_for_diffusion), dtype=torch.uint8))\n",
    "\n",
    "            # 3. Keep original_target_grids_for_ce_loss (long tensor, flattened)\n",
    "            original_target_grids_for_ce_loss_list.append(train_pair['output'].view(-1)) # Flattened LongTensor\n",
    "            \n",
    "    if not input_byte_sequences_list:\n",
    "        return {\n",
    "            'input_byte_sequences': torch.empty(0, MAX_SEQUENCE_LENGTH, dtype=torch.uint8),\n",
    "            'target_byte_sequences_for_diffusion': torch.empty(0, MAX_SEQUENCE_LENGTH, dtype=torch.uint8),\n",
    "            'original_target_grids_for_ce_loss': torch.empty(0, ARC_INPUT_FLAT_DIM, dtype=torch.long),\n",
    "        }\n",
    "\n",
    "    # Stack all collected tensors\n",
    "    final_input_byte_sequences = torch.stack(input_byte_sequences_list)\n",
    "    final_target_byte_sequences_for_diffusion = torch.stack(target_byte_sequences_for_diffusion_list)\n",
    "    final_original_target_grids_for_ce_loss = torch.stack(original_target_grids_for_ce_loss_list)\n",
    "    \n",
    "    return {\n",
    "        'input_byte_sequences': final_input_byte_sequences,\n",
    "        'target_byte_sequences_for_diffusion': final_target_byte_sequences_for_diffusion,\n",
    "        'original_target_grids_for_ce_loss': final_original_target_grids_for_ce_loss,\n",
    "    }\n",
    "\n",
    "# --- ARC Training Setup ---\n",
    "ARC_OUTPUT_HEAD_DIM = ARC_INPUT_FLAT_DIM * NUM_ARC_SYMBOLS\n",
    "ARC_TASK_ID = 3\n",
    "print(f\"ARC Output Head Dim: {ARC_OUTPUT_HEAD_DIM}\")\n",
    "\n",
    "ctm_model_arc, optimizer_arc, accelerator_arc = None, None, None\n",
    "\n",
    "print(\"\\n-----------------------------------------------------------------------------\")\n",
    "print(\"Initializing Configuration for Integrated Diffusion CTM\")\n",
    "print(\"-----------------------------------------------------------------------------\")\n",
    "print(f\"Using device: {device}\")\n",
    "if device == \"cuda\":\n",
    "    print(\"✅ Mixed precision training enabled (BF16) - Expected ~2x speedup\")\n",
    "\n",
    "print(\"\\n🚀 OPTIMIZATION STATUS:\")\n",
    "print(f\"  ⚡ torch.compile: {'✅' if TORCH_COMPILE_AVAILABLE else '❌'}\")\n",
    "print(f\"  📈 Accelerate: {'✅' if ACCELERATE_AVAILABLE else '❌'}\")\n",
    "print(f\"  ⚡ xFormers: {'✅' if XFORMERS_AVAILABLE else '❌'}\")\n",
    "print(f\"  ⚡ Deepspeed: {'✅' if DEEPSPEED_AVAILABLE else '❌'}\")\n",
    "\n",
    "# From contineous_thought_machines/models/constants.py\n",
    "VALID_NEURON_SELECT_TYPES = [\n",
    "    'first-last', 'random', 'random-pairing',  # Legacy\n",
    "    # Biologically-inspired types\n",
    "    'bio_hebbian', 'bio_plasticity', 'bio_competitive', 'bio_homeostatic',\n",
    "    'bio_evolutionary', 'bio_stdp', 'bio_criticality', 'bio_multi_objective',\n",
    "    # Hybrid approaches\n",
    "    'adaptive_random', 'performance_guided', 'task_aware'\n",
    "]\n",
    "\n",
    "VALID_POSITIONAL_EMBEDDING_TYPES = [\n",
    "    'learnable-fourier', 'multi-learnable-fourier',\n",
    "    'custom-rotational'\n",
    "]\n",
    "\n",
    "# From contineous_thought_machines/models/ctm_Diffusion_NEWNEW.py\n",
    "from dataclasses import dataclass, field\n",
    "from typing import Dict, Optional, Tuple, Union, Any, List\n",
    "import math\n",
    "\n",
    "from contineous_thought_machines.models.ctm_Diffusion_NEWNEW import EnhancedCTMDiffusion\n",
    "\n",
    "@dataclass\n",
    "class EnhancedCTMConfig: # Renamed from ContinualLearningConfig for consistency in the target file\n",
    "    \"\"\"Enhanced configuration for continual learning CTM-diffusion model,\n",
    "    incorporating binary processing, multi-task learning, and advanced CTM features.\"\"\"\n",
    "    \n",
    "    # Model architecture (General Transformer/Diffusion settings)\n",
    "    d_model: int = 512  # Main model dimensionality\n",
    "    n_heads: int = 8\n",
    "    n_layers: int = 24\n",
    "    max_sequence_length: int = 8192 # Max input sequence length in terms of bytes or patches\n",
    "    dropout: float = 0.1\n",
    "    \n",
    "    # --- Byte Processing Options ---\n",
    "    patch_embedding_dim: int = 256         # <<< NEW: Output embedding dimension per patch from patcher\n",
    "    patch_encoder_cnn_channels: int = 64   # <<< NEW: Intermediate channels for CNN patch encoder\n",
    "\n",
    "    # --- Dynamic Entropy Patching Options (Inspired by BLT paper) ---\n",
    "    use_dynamic_entropy_patcher: bool = True # Flag to enable dynamic entropy-based patching\n",
    "    entropy_patcher_threshold_type: str = \"global\"  # 'global' or 'relative_monotonic'\n",
    "    entropy_patcher_global_threshold: float = 0.75 # Entropy threshold for 'global' type\n",
    "    entropy_patcher_relative_threshold: float = 0.1 # Entropy diff threshold for 'relative_monotonic'\n",
    "    entropy_patcher_min_patch_size: int = 4      # Minimum number of bytes in a dynamic patch\n",
    "    entropy_patcher_max_patch_size: int = 128    # Maximum number of bytes in a dynamic patch (for CNN encoder)\n",
    "    \n",
    "    # --- Learnable Entropy Model Parameters (for _EntropyProxyModel) ---\n",
    "    entropy_model_byte_vocab_size: int = 256\n",
    "    entropy_model_embedding_dim: int = 64\n",
    "    entropy_model_hidden_dim: int = 128\n",
    "    entropy_model_num_layers: int = 1\n",
    "    entropy_model_dropout: float = 0.1\n",
    "    entropy_model_loss_weight: float = 0.1 # Weight for its auxiliary loss contribution\n",
    "    # Note: These parameters are used if use_dynamic_entropy_patcher is True,\n",
    "    # as LearnedBytePatcherEncoder now instantiates the learnable _EntropyProxyModel.\n",
    "    \n",
    "    # Fallback if not using learned_patch_encoder or dynamic_entropy_patcher\n",
    "    byte_embedding_dim: int = 256\n",
    "    multi_granularity: bool = False # Default to False if patcher is preferred\n",
    "    # multi_granularity_output_dim is complex to predefine, MGP should expose its output dim.\n",
    "    # For now, if multi_granularity is True AND use_learned_patch_encoder is False, this would be used.\n",
    "    multi_granularity_output_dim: int = 256 # Placeholder if MGP is used.\n",
    "    \n",
    "    hierarchical_processing: bool = True # General flag, could apply to patcher or MGP\n",
    "    \n",
    "    # CTM Core Parameters (Specific to the OriginalCTMCore module)\n",
    "    # These are prefixed with 'ctm_' to distinguish from general model params\n",
    "    ctm_iterations: int = 5  # Original 'iterations'\n",
    "    ctm_d_model: int = 512   # Original 'd_model' for CTM's internal latent space\n",
    "    ctm_input_dim: int = 256 # Dimensionality of inputs to CTM (e.g., from byte embeddings or other features)\n",
    "                             # This was 'd_input' in OriginalCTMCore if it took external features.\n",
    "                             # If CTM processes outputs of byte_embedding, this might be byte_embedding_dim.\n",
    "    ctm_heads: int = 8       # Attention heads within CTM\n",
    "    ctm_n_synch_out: int = 64\n",
    "    ctm_n_synch_action: int = 64\n",
    "    ctm_synapse_depth: int = 3\n",
    "    ctm_memory_length: int = 10\n",
    "    ctm_deep_nlms: bool = True\n",
    "    ctm_memory_hidden_dims: int = 2048\n",
    "    ctm_do_layernorm_nlm: bool = False\n",
    "    ctm_out_dims: int = 512  # Output dimension of CTM's own projector\n",
    "    ctm_prediction_reshaper: list = field(default_factory=lambda: [-1])\n",
    "    ctm_dropout: float = 0.1\n",
    "    ctm_dropout_nlm: Optional[float] = None\n",
    "    # Neuron selection strategy. Available options:\n",
    "    # Legacy: 'first-last', 'random', 'random-pairing'\n",
    "    # Biologically-inspired: 'bio_hebbian', 'bio_plasticity', 'bio_competitive',\n",
    "    #                        'bio_homeostatic', 'bio_evolutionary', 'bio_stdp',\n",
    "    #                        'bio_criticality', 'bio_multi_objective'\n",
    "    # Hybrid: 'adaptive_random', 'performance_guided', 'task_aware'\n",
    "    ctm_neuron_select_type: str = 'bio_multi_objective'\n",
    "    ctm_n_random_pairing_self: int = 0\n",
    "    \n",
    "    #Inferred_Latent_Dimensions Set to Avoid runtime errors but it does not functionally do anything in the model or program processing. \n",
    "    inferred_task_latent_dim=512\n",
    "\n",
    "    # Diffusion Parameters\n",
    "    diffusion_steps: int = 1000\n",
    "    noise_schedule: str = \"cosine\" # e.g., \"linear\", \"cosine\"\n",
    "    diffusion_beta_start: float = 0.0001\n",
    "    diffusion_beta_end: float = 0.02\n",
    "    diffusion_timesteps: int = 1000 # Number of timesteps for the diffusion process\n",
    "    ctm_diffusion_coupling_strength: float = 0.8 # How CTM influences diffusion\n",
    "    adaptive_scheduling: bool = True  # CTM-adaptive diffusion timestep scheduling\n",
    "    iterative_refinement: bool = True # Iterative CTM-diffusion refinement for sampling\n",
    "    \n",
    "    # Training Efficiency\n",
    "    mixed_precision: bool = True\n",
    "    gradient_checkpointing: bool = True\n",
    "    sparse_attention: bool = True  # Now implemented with BinarySparseAttention\n",
    "    adaptive_depth: bool = False   # Defaulting to False, can be enabled if implemented\n",
    "    use_activity_plasticity: bool = True # To enable/disable plasticity updates; Needs to be set to TRUE\n",
    "    ctm_use_internal_feedback: bool = True # Enable self-modulating feedback within the CTM core\n",
    "\n",
    "    # --- Bidirectional Reasoning Parameters ---\n",
    "    enable_bidirectional_reasoning: bool = True # Allows CTM to move forward/backward in its thought process\n",
    "    reasoning_step_gating_threshold: float = 0.7 # Confidence threshold for the reasoning controller to terminate\n",
    "    max_reasoning_steps: int = 15 # Max total steps in a bidirectional reasoning loop to prevent infinite loops\n",
    "    \n",
    "    # Sparse Attention Parameters\n",
    "    sparse_attention_ratio: float = 0.1  # Keep only 10% of attention connections\n",
    "    binary_pattern_size: int = 8  # Size of binary patterns to detect\n",
    "\n",
    "    # Attention Mechanism Type\n",
    "    attention_type: str = \"WINA\"  # Options: \"standard\", \"binary_sparse\", \"WINA\" #Need to use WINA attention in place of \"standard\"\n",
    "    control_dim: int = 64 # Dimension for WINA control mechanism\n",
    "\n",
    "    # Positional Embedding Parameters\n",
    "    positional_embedding_type: Optional[str] = 'multi-learnable-fourier' # e.g., 'custom-rotational', 'learnable-fourier', multi-learnable-fourier' #Can set the value here.\n",
    "    positional_embedding_dim: Optional[int] = None  # Dimension of the positional embedding, defaults to ctm_input_dim if None\n",
    "    reshape_patch_sequence_to_grid: bool = True # If True, reshape patch sequence to a 2D grid for 2D PEs. Must set to true if using 2D Grid for Positional Embeddings.\n",
    "    patch_grid_width: Optional[int] = None       # Desired width of the patch grid if reshaping\n",
    "\n",
    "    # --- Hierarchical Reasoning Model (HRM) Parameters ---\n",
    "    use_hrm_core: bool = True # Set to True to use the HierarchicalCTM core\n",
    "    hrm_high_level_cycles: int = 4 # N: Number of high-level cycles\n",
    "    hrm_low_level_timesteps: int = 8 # T: Number of low-level timesteps per high-level cycle\n",
    "    program_vocab_size: int = 1024 # Vocabulary size for the program synthesizer\n",
    "    program_synth_n_heads: int = 4\n",
    "    program_synth_n_layers: int = 3\n",
    "    program_synth_d_ff: int = 1024\n",
    "    ltm_size: int = 2048 # Size of the long-term memory\n",
    "    ltm_surprise_threshold: float = 0.6 # Surprise threshold for storing in LTM\n",
    "    ltm_top_k: int = 5 # Top-k for memory retrieval\n",
    "    replay_batch_size: int = 4 # Batch size for memory replay\n",
    "    replay_policy: str = \"surprise_weighted_replay\" # \"simple_replay\", \"surprise_weighted_replay\", \"usefulness_replay\"\n",
    "\n",
    "    # Pipeline Parallelism Parameters\n",
    "    enable_pipeline_parallelism: bool = True\n",
    "    pipeline_stages: int = 3  # CTM, Diffusion prep, Diffusion exec\n",
    "    pipeline_overlap_ratio: float = 0.7  # Target overlap ratio\n",
    "    \n",
    "    # Adaptive Batch Sizing Parameters\n",
    "    enable_adaptive_batching: bool = True\n",
    "    initial_batch_size: int = 32\n",
    "    min_batch_size: int = 8\n",
    "    max_batch_size: int = 256\n",
    "    batch_adaptation_frequency: int = 100\n",
    "    memory_threshold_high: float = 0.85\n",
    "    memory_threshold_low: float = 0.6\n",
    "    \n",
    "    # Smart Data Sampling Parameters\n",
    "    enable_smart_sampling: bool = True\n",
    "    sample_importance_weight: float = 0.6\n",
    "    sample_diversity_weight: float = 0.4\n",
    "    initial_sample_ratio: float = 0.3\n",
    "    complexity_analysis_enabled: bool = True\n",
    "    \n",
    "    # Multi-input/output parameters\n",
    "    num_inputs: int = 1  # Number of input streams\n",
    "    num_outputs: int = 1  # Number of output heads\n",
    "    output_dims: List[int] = field(default_factory=lambda: [64])  # Dimensions for each output head\n",
    "    \n",
    "    # Self-supervised learning\n",
    "    ssl_dim: int = 128  # Dimension for self-supervised projection\n",
    "    ssl_weight: float = 0.1  # Weight for self-supervised loss\n",
    "    ssl_temperature: float = 0.07  # Temperature for contrastive loss\n",
    "    ssl_noise_std: float = 0.1  # Noise standard deviation for contrastive augmentation\n",
    "    \n",
    "    # Spatiotemporal Processing\n",
    "    use_spatial: bool = True  # Enable spatial processing for image/video data\n",
    "    \n",
    "    # WINA Attention\n",
    "    use_wina_attention: bool = True  # Enable WINA sparse attention\n",
    "    \n",
    "    # Multi-task Learning Parameters\n",
    "    max_tasks: int = 50  # Maximum number of tasks for continual learning\n",
    "    # Added to resolve TypeError for unexpected keyword arguments\n",
    "    vocab_size: Optional[int] = None\n",
    "    output_audio_bytes: bool = False\n",
    "    inferred_task_latent_dim: Optional[int] = None # Default to None, __post_init__ handles it\n",
    "    use_hipa_attention: bool = False # Default to False\n",
    "    hipa_num_heads: Optional[int] = None # Default to None\n",
    "    audio_output_dtype_str: Optional[str] = \"float32\" # Default as per __post_init__ logic\n",
    "    unet_input_feature_dim: Optional[int] = None # Default to None, __post_init__ calculates it\n",
    "\n",
    "    # --- JEPA Training Parameters (Integrated with LearnedBytePatcherEncoder) ---\n",
    "    use_jepa_training: bool = False\n",
    "    # jepa_embed_dim will be derived from patch_embedding_dim if dynamic_entropy_patcher is used\n",
    "    jepa_predictor_hidden_dim: int = 512 # Hidden dimension of JEPA predictor MLP\n",
    "    jepa_mask_ratio_min: float = 0.15 # Min proportion of patch sequence to mask for target\n",
    "    jepa_mask_ratio_max: float = 0.75 # Max proportion of patch sequence to mask for target\n",
    "    jepa_context_scale_min: float = 0.3 # Min proportion of patches for context\n",
    "    jepa_context_scale_max: float = 0.7 # Max proportion of patches for context\n",
    "    jepa_momentum_beta: float = 0.996 # Momentum for target encoder update\n",
    "    jepa_loss_weight: float = 0.1 # Weight for the JEPA loss component\n",
    "    jepa_num_target_blocks: int = 1 # Number of target blocks to predict\n",
    "\n",
    "    # --- Global Plasticity Loss Parameters ---\n",
    "    local_hebbian_loss_weight: float = 0.01 # New weight for backprop-based hebbian loss\n",
    "\n",
    "    # --- Basal Ganglia Parameters --- #Controls action suppression so that the model's unwanted first unrelated thoughts are suppressed which helps with model safety. Is needed for action suppresion.\n",
    "    ctm_enable_basal_ganglia: bool = True\n",
    "    ctm_bg_dopamine_dim: int = 32\n",
    "\n",
    "    # --- Synaptic Empathy Parameters ---\n",
    "    enable_synaptic_empathy: bool = True # Set to True to use the new SynapticEmpathy module\n",
    "    synaptic_empathy_reward_weight: float = 0.1\n",
    "\n",
    "    # --- Mirror Neuron / High-Level Empathy Parameters ---\n",
    "    enable_mirror_neurons: bool = True # Set to True to use the high-level MirrorNeuronLayer\n",
    "    num_emotion_dim: int = 4 # Dimensionality of the emotion state vector\n",
    "    goal_dim: int = 8 # Dimensionality of the predicted goal vector\n",
    "    mirror_reward_weight: float = 0.2 # Weight for the selfless reward signal\n",
    "\n",
    "\n",
    "    # --- Confidence Thresholding Parameters ---\n",
    "    confidence_threshold: float = 0.0 # Confidence threshold for abstaining. If > 0, model can abstain.\n",
    " \n",
    "    # --- Consciousness Controller Parameters ---\n",
    "    enable_consciousness_controller: bool = True\n",
    "    consciousness_max_attention_steps: int = 100\n",
    "\n",
    "    # --- Recursion Parameters ---\n",
    "    max_recursion: int = 3\n",
    "    early_stop_threshold: float = 1e-3\n",
    "    \n",
    "    def __post_init__(self):\n",
    "        # Validate output dimensions\n",
    "        if len(self.output_dims) != self.num_outputs:\n",
    "            raise ValueError(f\"output_dims length ({len(self.output_dims)}) must match num_outputs ({self.num_outputs})\")\n",
    "\n",
    "        # Merged content from the second __post_init__\n",
    "        if hasattr(self, 'ctm_prediction_reshaper') and self.ctm_prediction_reshaper == [-1] and self.vocab_size is not None:\n",
    "            pass\n",
    "        if hasattr(self, 'ctm_dropout_nlm') and self.ctm_dropout_nlm is None and hasattr(self, 'ctm_dropout'):\n",
    "            self.ctm_dropout_nlm = self.ctm_dropout\n",
    "        \n",
    "        if hasattr(self, 'ctm_neuron_select_type') and \\\n",
    "           VALID_NEURON_SELECT_TYPES is not None and self.ctm_neuron_select_type not in VALID_NEURON_SELECT_TYPES:\n",
    "            print(f\"Warning: ctm_neuron_select_type '{self.ctm_neuron_select_type}' is not in VALID_NEURON_SELECT_TYPES ({VALID_NEURON_SELECT_TYPES}).\")\n",
    "\n",
    "        if hasattr(self, 'positional_embedding_type') and self.positional_embedding_type is not None:\n",
    "            if VALID_POSITIONAL_EMBEDDING_TYPES is None: # Fallback if import failed\n",
    "                print(f\"Warning: VALID_POSITIONAL_EMBEDDING_TYPES not available for validation.\")\n",
    "            elif self.positional_embedding_type not in VALID_POSITIONAL_EMBEDDING_TYPES:\n",
    "                print(f\"Warning: positional_embedding_type '{self.positional_embedding_type}' is not in VALID_POSITIONAL_EMBEDDING_TYPES ({VALID_POSITIONAL_EMBEDDING_TYPES}).\")\n",
    "            if self.positional_embedding_dim is not None and self.positional_embedding_dim <= 0:\n",
    "                raise ValueError(\"positional_embedding_dim must be positive if set.\")\n",
    "            \n",
    "            if self.reshape_patch_sequence_to_grid:\n",
    "                if self.patch_grid_width is None or self.patch_grid_width <= 0:\n",
    "                    raise ValueError(\"patch_grid_width must be a positive integer if reshape_patch_sequence_to_grid is True.\")\n",
    "                if self.positional_embedding_type not in ['learnable-fourier', 'multi-learnable-fourier', 'custom-rotational']:\n",
    "                    print(f\"Warning: reshape_patch_sequence_to_grid is True, but positional_embedding_type ('{self.positional_embedding_type}') is not a typical 2D PE. Ensure compatibility.\")\n",
    "\n",
    "        # Validations for new patch encoder\n",
    "        if self.use_dynamic_entropy_patcher:\n",
    "            if self.patch_embedding_dim <= 0:\n",
    "                raise ValueError(\"patch_embedding_dim must be positive if use_dynamic_entropy_patcher is True.\")\n",
    "            if self.entropy_patcher_min_patch_size <= 0:\n",
    "                raise ValueError(\"entropy_patcher_min_patch_size must be positive.\")\n",
    "            if self.entropy_patcher_max_patch_size < self.entropy_patcher_min_patch_size:\n",
    "                raise ValueError(\"entropy_patcher_max_patch_size must be >= entropy_patcher_min_patch_size.\")\n",
    "            if self.entropy_patcher_threshold_type not in [\"global\", \"relative_monotonic\"]:\n",
    "                raise ValueError(\"entropy_patcher_threshold_type must be 'global' or 'relative_monotonic'.\")\n",
    "        elif self.multi_granularity and self.multi_granularity_output_dim <= 0:\n",
    "            print(\"Warning: multi_granularity_output_dim might not be correctly set for validation if not using a patcher and MGP is active.\")\n",
    "        \n",
    "        if not hasattr(self, 'inferred_task_latent_dim') or self.inferred_task_latent_dim is None:\n",
    "            print(\"Warning: inferred_task_latent_dim not found or is None in config, defaulting to 512.\")\n",
    "            self.inferred_task_latent_dim = 512\n",
    "        elif self.inferred_task_latent_dim <= 0: # This check is now safe\n",
    "            raise ValueError(\"inferred_task_latent_dim must be positive.\")\n",
    " \n",
    "        if hasattr(self, 'use_hipa_attention') and self.use_hipa_attention and \\\n",
    "            (not hasattr(self, 'hipa_num_heads') or self.hipa_num_heads <= 0):\n",
    "             raise ValueError(\"hipa_num_heads must be positive if use_hipa_attention is True.\")\n",
    " \n",
    "        if hasattr(self, 'audio_output_dtype_str'):\n",
    "            if self.audio_output_dtype_str == \"float32\":\n",
    "                self.audio_output_item_size = 4\n",
    "            elif self.audio_output_dtype_str == \"int16\":\n",
    "                self.audio_output_item_size = 2\n",
    "            else:\n",
    "                if hasattr(self, 'output_audio_bytes') and self.output_audio_bytes:\n",
    "                    raise ValueError(f\"Unsupported audio_output_dtype_str: {self.audio_output_dtype_str} when output_audio_bytes is True.\")\n",
    "                else:\n",
    "                    self.audio_output_item_size = 4\n",
    "        elif hasattr(self, 'output_audio_bytes') and self.output_audio_bytes:\n",
    "            if not hasattr(self, 'audio_output_dtype_str') or self.audio_output_dtype_str is None:\n",
    "                raise ValueError(\"audio_output_dtype_str must be defined in config if output_audio_bytes is True.\")\n",
    "        else:\n",
    "            self.audio_output_item_size = 4\n",
    "\n",
    "        # Calculate unet_input_feature_dim if not set\n",
    "        if self.unet_input_feature_dim is None:\n",
    "            if self.max_sequence_length <= 0 or self.audio_output_item_size <= 0:\n",
    "                raise ValueError(\"max_sequence_length and audio_output_item_size must be positive to calculate unet_input_feature_dim.\")\n",
    "            self.unet_input_feature_dim = self.max_sequence_length // self.audio_output_item_size\n",
    "            if self.unet_input_feature_dim <= 0:\n",
    "                raise ValueError(f\"Calculated unet_input_feature_dim ({self.unet_input_feature_dim}) must be positive. Check max_sequence_length and audio_output_item_size.\")\n",
    "        elif self.unet_input_feature_dim <= 0:\n",
    "            raise ValueError(\"unet_input_feature_dim, if set, must be positive.\")\n",
    "\n",
    "        if self.use_jepa_training:\n",
    "            if not (0 < self.jepa_mask_ratio_min < 1 and 0 < self.jepa_mask_ratio_max < 1 and self.jepa_mask_ratio_min <= self.jepa_mask_ratio_max):\n",
    "                raise ValueError(\"JEPA mask ratios must be between 0 and 1, with min <= max.\")\n",
    "            if not (0 < self.jepa_context_scale_min < 1 and 0 < self.jepa_context_scale_max < 1 and self.jepa_context_scale_min <= self.jepa_context_scale_max):\n",
    "                raise ValueError(\"JEPA context scales must be between 0 and 1, with min <= max.\")\n",
    "            if not (0 <= self.jepa_momentum_beta < 1):\n",
    "                raise ValueError(\"jepa_momentum_beta must be between 0 and 1.\")\n",
    "            if self.jepa_num_target_blocks <= 0:\n",
    "                raise ValueError(\"jepa_num_target_blocks must be positive.\")\n",
    "            if not self.use_dynamic_entropy_patcher:\n",
    "                print(\"Warning: JEPA training is enabled but use_dynamic_entropy_patcher is False. JEPA relies on the patch embeddings from LearnedBytePatcherEncoder.\")\n",
    "        \n",
    "\n",
    "        # Validations for recursion parameters\n",
    "        if self.max_recursion < 1:\n",
    "            raise ValueError(\"max_recursion must be at least 1\")\n",
    "        if self.early_stop_threshold <= 0:\n",
    "            raise ValueError(\"early_stop_threshold must be positive.\")\n",
    "\n",
    "# --- Model Configuration ---\n",
    "config_arc_diffusion = EnhancedCTMConfig(\n",
    "    d_model=512,\n",
    "    n_heads=8,\n",
    "    n_layers=24,\n",
    "    max_sequence_length=MAX_SEQUENCE_LENGTH,\n",
    "    dropout=0.1,\n",
    "    use_dynamic_entropy_patcher=True,\n",
    "    patch_embedding_dim=256,\n",
    "    patch_grid_width=16,\n",
    "    patch_encoder_cnn_channels=64,\n",
    "    entropy_patcher_threshold_type=\"global\",\n",
    "    entropy_patcher_global_threshold=0.75,\n",
    "    entropy_patcher_relative_threshold=0.1,\n",
    "    entropy_patcher_min_patch_size=4,\n",
    "    entropy_patcher_max_patch_size=128,\n",
    "    entropy_model_byte_vocab_size=256,\n",
    "    entropy_model_embedding_dim=64,\n",
    "    entropy_model_hidden_dim=128,\n",
    "    entropy_model_num_layers=1,\n",
    "    entropy_model_dropout=0.1,\n",
    "    entropy_model_loss_weight=0.1,\n",
    "    ctm_input_dim=256,\n",
    "    ctm_d_model=512,\n",
    "    ctm_iterations=5,\n",
    "    ctm_heads=8,\n",
    "    ctm_out_dims=512,\n",
    "    ctm_neuron_select_type='bio_multi_objective',\n",
    "    positional_embedding_type='multi-learnable-fourier',\n",
    "    positional_embedding_dim=None,\n",
    "    reshape_patch_sequence_to_grid=True,\n",
    "    enable_pipeline_parallelism=True,\n",
    "    pipeline_stages=4,\n",
    "    pipeline_overlap_ratio=0.7,\n",
    "    enable_adaptive_batching=True,\n",
    "    initial_batch_size=32,\n",
    "    min_batch_size=8,\n",
    "    max_batch_size=256,\n",
    "    batch_adaptation_frequency=100,\n",
    "    memory_threshold_high=0.85,\n",
    "    memory_threshold_low=0.6,\n",
    "    enable_smart_sampling=True,\n",
    "    sample_importance_weight=0.6,\n",
    "    sample_diversity_weight=0.4,\n",
    "    initial_sample_ratio=0.3,\n",
    "    complexity_analysis_enabled=True,\n",
    "    num_inputs=1,\n",
    "    num_outputs=1,\n",
    "    output_dims=[64],\n",
    "    ssl_dim=128,\n",
    "    ssl_weight=0.1,\n",
    "    ssl_temperature=0.07,\n",
    "    ssl_noise_std=0.1,\n",
    "    use_spatial=False,\n",
    "    use_wina_attention=True,\n",
    "    max_tasks=50,\n",
    "    diffusion_steps=1000,\n",
    "    ctm_diffusion_coupling_strength=0.8,\n",
    "    vocab_size=None,\n",
    "    output_audio_bytes=True,\n",
    "    unet_input_feature_dim=MAX_SEQUENCE_LENGTH // 4, # Calculated based on float32 audio\n",
    "    local_hebbian_loss_weight=0.01,\n",
    "    enable_consciousness_controller=True,\n",
    "    consciousness_max_attention_steps=100,\n",
    "    use_hrm_core=True,\n",
    "    attention_type=\"WINA\",\n",
    "    inferred_task_latent_dim=512 #This does nothing in the model training but is included in a placeholder to avoid possible errors with initializing Torch for training.\n",
    ")\n",
    "print(\"✓ EnhancedCTMConfig for ARC (config_arc_diffusion) created.\")\n",
    "    \n",
    "if 'EnhancedCTMDiffusion' in globals() and EnhancedCTMDiffusion is not None:\n",
    "    ctm_model_arc = EnhancedCTMDiffusion(config=config_arc_diffusion).to(device)\n",
    "    print(\"✓ EnhancedCTMDiffusion model for ARC (ctm_model_arc) initialized.\")\n",
    "\n",
    "    # The new EnhancedCTMDiffusion model is end-to-end and does not require an external output head.\n",
    "    print(\"✓ ARC Output Head is disabled as it's not needed for the new model.\")\n",
    "\n",
    "    # MCMC integration is disabled as per new model requirements.\n",
    "    \n",
    "    arc_trainable_params = list(ctm_model_arc.parameters()) # EnhancedCTMDiffusion parameters\n",
    "\n",
    "    optimizer_arc = optim.AdamW([p for p in arc_trainable_params if p.requires_grad], lr=LEARNING_RATE, weight_decay=1e-4)\n",
    "    \n",
    "    if ACCELERATE_AVAILABLE:\n",
    "        accelerator_arc = Accelerator()\n",
    "        # Only the main model and optimizer need to be prepared.\n",
    "        ctm_model_arc, optimizer_arc = accelerator_arc.prepare(ctm_model_arc, optimizer_arc)\n",
    "        print(\"✓ ARC models (EnhancedCTMDiffusion) and optimizer prepared with Accelerate.\")\n",
    "else:\n",
    "    print(\"⚠️ EnhancedCTMDiffusion model or its config for ARC-AGI-2 could not be initialized. Check imports.\")\n",
    "\n",
    "CHECKPOINT_DIR_ARC = os.path.join(CHECKPOINT_DIR, \"ctm_arc_agi_2_enhanced_diffusion\") # New checkpoint dir\n",
    "os.makedirs(CHECKPOINT_DIR_ARC, exist_ok=True)\n",
    "print(f\"ARC Checkpoints will be saved to: {CHECKPOINT_DIR_ARC}\")\n",
    "\n",
    "NUM_EPOCHS_ARC = 20\n",
    "ARC_BATCH_SIZE = 16\n",
    "\n",
    "arc_train_dataset = NewCustomARCGridDataset(ARC_TRAIN_DIR)\n",
    "arc_eval_dataset = NewCustomARCGridDataset(ARC_EVAL_DIR)\n",
    "\n",
    "arc_train_loader, arc_eval_loader = None, None\n",
    "if arc_train_dataset and len(arc_train_dataset) > 0:\n",
    "    arc_train_loader = DataLoader(\n",
    "        arc_train_dataset, batch_size=ARC_BATCH_SIZE, shuffle=True,\n",
    "        collate_fn=collate_fn_new_custom_arc, **OPTIMIZED_DATALOADER_CONFIG\n",
    "    )\n",
    "    if accelerator_arc: arc_train_loader = accelerator_arc.prepare(arc_train_loader)\n",
    "    print(f\"✓ ARC Training DataLoader initialized with {len(arc_train_dataset)} tasks.\")\n",
    "else:\n",
    "    print(\"⚠️ ARC Training DataLoader could not be initialized.\")\n",
    "\n",
    "if arc_eval_dataset and len(arc_eval_dataset) > 0:\n",
    "    arc_eval_loader = DataLoader(\n",
    "        arc_eval_dataset, batch_size=1, shuffle=False,\n",
    "        collate_fn=collate_fn_new_custom_arc, **OPTIMIZED_DATALOADER_CONFIG\n",
    "    )\n",
    "    if accelerator_arc: arc_eval_loader = accelerator_arc.prepare(arc_eval_loader)\n",
    "    print(f\"✓ ARC Evaluation DataLoader initialized with {len(arc_eval_dataset)} tasks.\")\n",
    "else:\n",
    "    print(\"⚠️ ARC Evaluation DataLoader could not be initialized.\")\n",
    "\n",
    "# The CE loss criterion is no longer needed as the model calculates its own loss.\n",
    "print(\"\\n✓ ARC-AGI-2 Setup Complete.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37224026",
   "metadata": {},
   "source": [
    "# Training Arc_AGI_2 and Principles Phase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d887b26b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "🚀 STARTING PHASE 4: ARC-AGI-2 Meta-Learning Training\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'NUM_EPOCHS_ARC' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 23\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m60\u001b[39m)\n\u001b[1;32m     22\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m🚀 STARTING PHASE 4: ARC-AGI-2 Meta-Learning Training\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 23\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m   Epochs: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[43mNUM_EPOCHS_ARC\u001b[49m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, Batch Size: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mARC_BATCH_SIZE\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, Task ID: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mARC_TASK_ID\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     24\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m   Device: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdevice\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mif\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;129;01mnot\u001b[39;00m\u001b[38;5;250m \u001b[39maccelerator_arc\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01melse\u001b[39;00m\u001b[38;5;250m \u001b[39maccelerator_arc\u001b[38;5;241m.\u001b[39mdevice\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     25\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m60\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'NUM_EPOCHS_ARC' is not defined"
     ]
    }
   ],
   "source": [
    "# --- ARC-AGI-2 Meta-Learning Training Loop ---\n",
    "import os\n",
    "import torch\n",
    "import torch.distributed as dist\n",
    "from safetensors.torch import save_file\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from tqdm.auto import tqdm\n",
    "from torch.cuda.amp import autocast, GradScaler\n",
    "import glob\n",
    "import json\n",
    "from dataclasses import dataclass, field\n",
    "from typing import List, Optional, Any\n",
    "import math\n",
    "\n",
    "CUDA_LAUNCH_BLOCKING=1 #Diagnose cuda errors. \n",
    "# --- FIX: Define NUM_ARC_SYMBOLS globally for DataLoader workers ---\n",
    "# The standard ARC task has 10 symbols (0-9).\n",
    "NUM_ARC_SYMBOLS = 10\n",
    "import numpy as np\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(f\"🚀 STARTING PHASE 4: ARC-AGI-2 Meta-Learning Training\")\n",
    "print(f\"   Epochs: {NUM_EPOCHS_ARC}, Batch Size: {ARC_BATCH_SIZE}, Task ID: {ARC_TASK_ID}\")\n",
    "print(f\"   Device: {device if not accelerator_arc else accelerator_arc.device}\")\n",
    "print(\"=\"*60 + \"\\n\")\n",
    "\n",
    "# --- Principles Training Configuration ---\n",
    "NUM_EPOCHS_PRINCIPLES = 3 #Can be lowered due to new DPPM++ Solver converging 10 epoch sooner.\n",
    "\n",
    "# --- Training Configuration ---\n",
    "USE_MIXED_PRECISION = torch.cuda.is_available()\n",
    "autocast_dtype = torch.bfloat16 if torch.cuda.is_bf16_supported() else torch.float16\n",
    "GRADIENT_ACCUMULATION_STEPS = 2\n",
    "MAX_GRAD_NORM = 1.0\n",
    "scaler = torch.amp.GradScaler('cuda',enabled=USE_MIXED_PRECISION)\n",
    "\n",
    "# --- Context: 2D Grid Padding (from original code) ---\n",
    "def pad_grid(grid_list, max_dims, pad_value):\n",
    "    \"\"\"Pads a 2D grid to specified maximum dimensions.\"\"\"\n",
    "    grid_np = np.array(grid_list, dtype=np.int32)\n",
    "    padded_grid = np.full(max_dims, pad_value, dtype=np.int32)\n",
    "    h, w = grid_np.shape\n",
    "    padded_grid[:h, :w] = grid_np\n",
    "    return padded_grid\n",
    "\n",
    "# --- Fix: Byte Sequence Padding for the Model --- #\n",
    "MAX_SEQUENCE_LENGTH = 8192\n",
    "PADDING_BYTE_VALUE = 0\n",
    "\n",
    "def serialize_and_pad_grid(grid, max_len=MAX_SEQUENCE_LENGTH, pad_value=PADDING_BYTE_VALUE):\n",
    "    \"\"\"\n",
    "    Serializes a grid into a byte sequence and pads it to a fixed length.\n",
    "    \"\"\"\n",
    "    flat_array = np.array(grid, dtype=np.uint8).flatten()\n",
    "    byte_sequence = flat_array.tobytes()\n",
    "    padding_len = max_len - len(byte_sequence)\n",
    "\n",
    "    if padding_len < 0:\n",
    "        padded_sequence = byte_sequence[:max_len]\n",
    "    else:\n",
    "        padding = bytes([pad_value] * padding_len)\n",
    "        padded_sequence = byte_sequence + padding\n",
    "        \n",
    "    return padded_sequence\n",
    "\n",
    "from contineous_thought_machines.models.ctm_Diffusion_NEWNEW import batched_numeric_tensor_to_bytes\n",
    "\n",
    "class PrinciplesDataset(Dataset):\n",
    "    def __init__(self, file_path, max_len=MAX_SEQUENCE_LENGTH, pad_value=PADDING_BYTE_VALUE, audio_duration_seconds=2.0, sample_rate=16000):\n",
    "        self.max_len = max_len\n",
    "        self.pad_value = pad_value\n",
    "        self.audio_duration_seconds = audio_duration_seconds\n",
    "        self.sample_rate = sample_rate\n",
    "        self.principles = []\n",
    "        try:\n",
    "            with open(file_path, 'r', encoding='utf-8') as f:\n",
    "                for line in f:\n",
    "                    line = line.strip()\n",
    "                    if line:\n",
    "                        self.principles.append(line)\n",
    "            print(f\"PrinciplesDataset: Loaded {len(self.principles)} principles from {file_path}.\")\n",
    "        except Exception as e:\n",
    "            print(f\"PrinciplesDataset Warning: Could not load or parse {file_path}: {e}\")\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.principles)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        principle_text = self.principles[idx]\n",
    "        \n",
    "        # 1. Prepare text input\n",
    "        text_bytes = torch.tensor(list(principle_text.encode('utf-8')), dtype=torch.uint8)\n",
    "        \n",
    "        # 2. Prepare silent audio template\n",
    "        num_audio_samples = int(self.audio_duration_seconds * self.sample_rate)\n",
    "        # Create on CPU, as conversion to bytes happens on CPU.\n",
    "        audio_template_numeric = torch.zeros(1, num_audio_samples) # Batch of 1\n",
    "        \n",
    "        # Convert audio template to bytes\n",
    "        audio_template_bytes = batched_numeric_tensor_to_bytes(audio_template_numeric, source_dtype=np.float32).squeeze(0)\n",
    "\n",
    "        # 3. Create combined byte sequence\n",
    "        separator = torch.tensor([255, 0, 255, 0, 255, 0, 255, 0], dtype=torch.uint8)\n",
    "        \n",
    "        combined_input_bytes = torch.cat([text_bytes, separator, audio_template_bytes])\n",
    "\n",
    "        # 4. Pad or truncate the combined sequence\n",
    "        padding_len = self.max_len - len(combined_input_bytes)\n",
    "        if padding_len < 0:\n",
    "            padded_sequence = combined_input_bytes[:self.max_len]\n",
    "        else:\n",
    "            padding = torch.full((padding_len,), self.pad_value, dtype=torch.uint8)\n",
    "            padded_sequence = torch.cat([combined_input_bytes, padding])\n",
    "            \n",
    "        return padded_sequence\n",
    "\n",
    "def collate_fn_principles(batch):\n",
    "    # The batch is already a list of tensors from __getitem__\n",
    "    # We just need to stack them.\n",
    "    return {'input_byte_sequences': torch.stack(batch)}\n",
    "\n",
    "class NewCustomARCGridDataset(Dataset):\n",
    "    def __init__(self, data_dir, max_grid_size=MAX_GRID_SIZE, padding_value=PADDING_VALUE):\n",
    "        self.data_dir = data_dir\n",
    "        self.task_files = glob.glob(os.path.join(data_dir, \"*.json\"))\n",
    "        self.max_grid_size = max_grid_size\n",
    "        self.padding_value = padding_value\n",
    "        self.tasks = []\n",
    "        print(f\"NewCustomARCGridDataset: Looking for tasks in: {data_dir}\")\n",
    "        if not self.task_files:\n",
    "            print(f\"NewCustomARCGridDataset Warning: No JSON files found in {data_dir}. Dataset will be empty.\")\n",
    "        for task_file in self.task_files:\n",
    "            try:\n",
    "                with open(task_file, 'r') as f:\n",
    "                    self.tasks.append(json.load(f))\n",
    "            except Exception as e:\n",
    "                print(f\"NewCustomARCGridDataset Warning: Could not load or parse {task_file}: {e}\")\n",
    "        if not self.tasks:\n",
    "            print(f\"NewCustomARCGridDataset Warning: No tasks successfully loaded from {data_dir}.\")\n",
    "        else:\n",
    "            print(f\"NewCustomARCGridDataset: Loaded {len(self.tasks)} ARC tasks from {data_dir}.\")\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.tasks)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        task_data = self.tasks[idx]\n",
    "        processed_task = {'train': [], 'test': [], 'id': os.path.basename(self.task_files[idx]) if idx < len(self.task_files) else 'unknown_task'}\n",
    "\n",
    "        for pair_type in ['train', 'test']:\n",
    "            for item in task_data.get(pair_type, []):\n",
    "                input_grid_list = item.get('input', [])\n",
    "                output_grid_list = item.get('output', [])\n",
    "                \n",
    "                original_input_dims = (len(input_grid_list), len(input_grid_list[0]) if input_grid_list and input_grid_list[0] else (0,0))\n",
    "                original_output_dims = (len(output_grid_list), len(output_grid_list[0]) if output_grid_list and output_grid_list[0] else (0,0))\n",
    "\n",
    "                padded_input_np = pad_grid(input_grid_list, self.max_grid_size, self.padding_value)\n",
    "                padded_output_np = pad_grid(output_grid_list, self.max_grid_size, self.padding_value)\n",
    "                \n",
    "                processed_task[pair_type].append({\n",
    "                    'input': torch.from_numpy(padded_input_np).long(),\n",
    "                    'output': torch.from_numpy(padded_output_np).long(),\n",
    "                    'original_input_dims': original_input_dims,\n",
    "                    'original_output_dims': original_output_dims\n",
    "                })\n",
    "        return processed_task\n",
    "\n",
    "def collate_fn_new_custom_arc(batch_of_tasks):\n",
    "    input_byte_sequences_list = []\n",
    "    target_byte_sequences_for_diffusion_list = []\n",
    "    original_target_grids_for_ce_loss_list = []\n",
    "\n",
    "    for task in batch_of_tasks:\n",
    "        if not isinstance(task, dict):\n",
    "            continue\n",
    "\n",
    "        for train_pair in task.get('train', []):\n",
    "            if not isinstance(train_pair, dict) or 'input' not in train_pair or 'output' not in train_pair:\n",
    "                continue\n",
    "\n",
    "            input_grid_np = train_pair['input'].numpy()\n",
    "            target_grid_np = train_pair['output'].numpy()\n",
    "\n",
    "            input_bytes = serialize_and_pad_grid(input_grid_np, max_len=MAX_SEQUENCE_LENGTH, pad_value=PADDING_BYTE_VALUE)\n",
    "            input_byte_sequences_list.append(torch.tensor(list(input_bytes), dtype=torch.uint8))\n",
    "\n",
    "            target_bytes_for_diffusion = serialize_and_pad_grid(target_grid_np, max_len=MAX_SEQUENCE_LENGTH, pad_value=PADDING_BYTE_VALUE)\n",
    "            target_byte_sequences_for_diffusion_list.append(torch.tensor(list(target_bytes_for_diffusion), dtype=torch.uint8))\n",
    "\n",
    "            original_target_grids_for_ce_loss_list.append(train_pair['output'].view(-1))\n",
    "            \n",
    "    if not input_byte_sequences_list:\n",
    "        return {\n",
    "            'input_byte_sequences': torch.empty(0, MAX_SEQUENCE_LENGTH, dtype=torch.uint8),\n",
    "            'target_byte_sequences_for_diffusion': torch.empty(0, MAX_SEQUENCE_LENGTH, dtype=torch.uint8),\n",
    "            'original_target_grids_for_ce_loss': torch.empty(0, ARC_INPUT_FLAT_DIM, dtype=torch.long),\n",
    "        }\n",
    "\n",
    "    final_input_byte_sequences = torch.stack(input_byte_sequences_list)\n",
    "    final_target_byte_sequences_for_diffusion = torch.stack(target_byte_sequences_for_diffusion_list)\n",
    "    final_original_target_grids_for_ce_loss = torch.stack(original_target_grids_for_ce_loss_list)\n",
    "    \n",
    "    # --- Fix for potential out-of-bounds padding values ---\n",
    "    # The CrossEntropyLoss criterion expects class indices to be in [0, C-1].\n",
    "    # If the padding value is negative or >= C, it can cause a CUDA 'device-side assert' error.\n",
    "    # We defensively clamp the target tensor to the valid range [0, NUM_ARC_SYMBOLS - 1].\n",
    "    final_original_target_grids_for_ce_loss.clamp_(min=0, max=NUM_ARC_SYMBOLS - 1)\n",
    "    \n",
    "    return {\n",
    "        'input_byte_sequences': final_input_byte_sequences,\n",
    "        'target_byte_sequences_for_diffusion': final_target_byte_sequences_for_diffusion,\n",
    "        'original_target_grids_for_ce_loss': final_original_target_grids_for_ce_loss,\n",
    "    }\n",
    "\n",
    "arc_train_dataset = NewCustomARCGridDataset(ARC_TRAIN_DIR)\n",
    "arc_eval_dataset = NewCustomARCGridDataset(ARC_EVAL_DIR)\n",
    "\n",
    "arc_train_loader, arc_eval_loader = None, None\n",
    "if arc_train_dataset and len(arc_train_dataset) > 0:\n",
    "    arc_train_loader = DataLoader(\n",
    "        arc_train_dataset, batch_size=ARC_BATCH_SIZE, shuffle=True,\n",
    "        collate_fn=collate_fn_new_custom_arc, **OPTIMIZED_DATALOADER_CONFIG\n",
    "    )\n",
    "    if accelerator_arc: arc_train_loader = accelerator_arc.prepare(arc_train_loader)\n",
    "    print(f\"✓ ARC Training DataLoader initialized with {len(arc_train_dataset)} tasks.\")\n",
    "else:\n",
    "    print(\"⚠️ ARC Training DataLoader could not be initialized.\")\n",
    "\n",
    "if arc_eval_dataset and len(arc_eval_dataset) > 0:\n",
    "    arc_eval_loader = DataLoader(\n",
    "        arc_eval_dataset, batch_size=1, shuffle=False,\n",
    "        collate_fn=collate_fn_new_custom_arc, **OPTIMIZED_DATALOADER_CONFIG\n",
    "    )\n",
    "    if accelerator_arc: arc_eval_loader = accelerator_arc.prepare(arc_eval_loader)\n",
    "    print(f\"✓ ARC Evaluation DataLoader initialized with {len(arc_eval_dataset)} tasks.\")\n",
    "else:\n",
    "    print(\"⚠️ ARC Evaluation DataLoader could not be initialized.\")\n",
    "\n",
    "\n",
    "# --- Principles Dataset and DataLoader ---\n",
    "PRINCIPLES_FILE_PATH = \"contineous-thought-machines/models/Principles/principles.txt\"\n",
    "principles_dataset = PrinciplesDataset(PRINCIPLES_FILE_PATH)\n",
    "principles_loader = None\n",
    "if principles_dataset and len(principles_dataset) > 0:\n",
    "    principles_loader = DataLoader(\n",
    "        principles_dataset, batch_size=ARC_BATCH_SIZE, shuffle=True,\n",
    "        collate_fn=collate_fn_principles, **OPTIMIZED_DATALOADER_CONFIG\n",
    "    )\n",
    "    if accelerator_arc: principles_loader = accelerator_arc.prepare(principles_loader)\n",
    "    print(f\"✓ Principles DataLoader initialized with {len(principles_dataset)} principles.\")\n",
    "else:\n",
    "    print(\"⚠️ Principles DataLoader could not be initialized.\")\n",
    "\n",
    "\n",
    "# === DEBUG + RANK CHECK ===\n",
    "def get_rank_debug():\n",
    "    if dist.is_available() and dist.is_initialized():\n",
    "        rank = dist.get_rank()\n",
    "        world_size = dist.get_world_size()\n",
    "    else:\n",
    "        rank = 0\n",
    "        world_size = 1\n",
    "\n",
    "    print(f\"[DEBUG] Rank {rank} out of {world_size} total ranks\")\n",
    "    return rank, world_size\n",
    "\n",
    "\n",
    "if not all([ctm_model_arc, optimizer_arc, arc_train_loader]):\n",
    "    print(\"⚠️ Skipping ARC-AGI-2 training due to missing components.\")\n",
    "else:\n",
    "    print(\"✓ All components ready for ARC training.\")\n",
    "    \n",
    "    for epoch in range(NUM_EPOCHS_ARC):\n",
    "        ctm_model_arc.train()\n",
    "        if hasattr(ctm_model_arc, 'wake_up'):\n",
    "            ctm_model_arc.wake_up()\n",
    "\n",
    "        total_epoch_loss = 0\n",
    "        \n",
    "        progress_bar = tqdm(enumerate(arc_train_loader), total=len(arc_train_loader), desc=f\"ARC Epoch {epoch + 1}\")\n",
    "\n",
    "        for batch_idx, batch_data in progress_bar:\n",
    "            if not batch_data or batch_data['input_byte_sequences'].numel() == 0:\n",
    "                print(f\"Skipping empty batch {batch_idx}\")\n",
    "                continue\n",
    "\n",
    "            input_bytes = batch_data['input_byte_sequences'].to(accelerator_arc.device if accelerator_arc else device)\n",
    "            target_bytes_for_diffusion = batch_data['target_byte_sequences_for_diffusion'].to(accelerator_arc.device if accelerator_arc else device)\n",
    "            \n",
    "            current_batch_size = input_bytes.size(0)\n",
    "\n",
    "            optimizer_arc.zero_grad()\n",
    "            \n",
    "            autocast_context = accelerator_arc.autocast() if accelerator_arc else autocast(enabled=USE_MIXED_PRECISION, dtype=autocast_dtype)\n",
    "\n",
    "            with autocast_context:\n",
    "                model_output_dict = ctm_model_arc(\n",
    "                    byte_sequence=input_bytes,\n",
    "                    target_diffusion_output=target_bytes_for_diffusion,\n",
    "                    mode='ctm_controlled_diffusion',\n",
    "                    timestep=torch.randint(0, config_arc_diffusion.diffusion_steps, (current_batch_size,), device=input_bytes.device).long()\n",
    "                )\n",
    "\n",
    "                total_loss = model_output_dict.get('total_loss', torch.tensor(0.0, device=input_bytes.device))\n",
    "\n",
    "            if torch.isnan(total_loss) or torch.isinf(total_loss):\n",
    "                print(f\"[NaN or Inf Loss Detected] at Epoch {epoch+1}, Batch {batch_idx+1}. Skipping backward pass.\")\n",
    "                continue\n",
    "\n",
    "            if accelerator_arc:\n",
    "                accelerator_arc.backward(total_loss)\n",
    "                if (batch_idx + 1) % GRADIENT_ACCUMULATION_STEPS == 0:\n",
    "                   if accelerator_arc.sync_gradients:\n",
    "                       accelerator_arc.clip_grad_norm_(ctm_model_arc.parameters(), MAX_GRAD_NORM)\n",
    "                   optimizer_arc.step()\n",
    "                   optimizer_arc.zero_grad()\n",
    "            else:\n",
    "                scaler.scale(total_loss).backward()\n",
    "                if (batch_idx + 1) % GRADIENT_ACCUMULATION_STEPS == 0:\n",
    "                    scaler.unscale_(optimizer_arc)\n",
    "                    torch.nn.utils.clip_grad_norm_(ctm_model_arc.parameters(), MAX_GRAD_NORM)\n",
    "                    scaler.step(optimizer_arc)\n",
    "                    scaler.update()\n",
    "                    optimizer_arc.zero_grad()\n",
    "\n",
    "            total_epoch_loss += total_loss.item()\n",
    "            progress_bar.set_postfix({\n",
    "                'loss': total_loss.item(),\n",
    "                'avg_loss': total_epoch_loss / (batch_idx + 1)\n",
    "            })\n",
    "\n",
    "        avg_epoch_loss = total_epoch_loss / len(arc_train_loader) if len(arc_train_loader) > 0 else 0\n",
    "        print(f\"Epoch [{epoch+1}/{NUM_EPOCHS_ARC}] completed. Average Loss: {avg_epoch_loss:.4f}\")\n",
    "\n",
    "        # --- Evaluation Step ---\n",
    "        ctm_model_arc.eval()\n",
    "        total_eval_loss = 0\n",
    "        with torch.no_grad():\n",
    "            for eval_batch_data in arc_eval_loader:\n",
    "                input_bytes = eval_batch_data['input_byte_sequences'].to(accelerator_arc.device if accelerator_arc else device)\n",
    "                target_bytes = eval_batch_data['target_byte_sequences_for_diffusion'].to(accelerator_arc.device if accelerator_arc else device)\n",
    "                \n",
    "                with autocast_context:\n",
    "                    eval_output = ctm_model_arc(\n",
    "                        byte_sequence=input_bytes,\n",
    "                        target_diffusion_output=target_bytes,\n",
    "                        mode='ctm_controlled_diffusion',\n",
    "                        timestep=torch.randint(0, config_arc_diffusion.diffusion_steps, (input_bytes.size(0),), device=input_bytes.device).long()\n",
    "                    )\n",
    "                    eval_loss = eval_output.get('total_loss', torch.tensor(0.0, device=input_bytes.device))\n",
    "                total_eval_loss += eval_loss.item()\n",
    "        \n",
    "        avg_eval_loss = total_eval_loss / len(arc_eval_loader) if len(arc_eval_loader) > 0 else 0\n",
    "        print(f\"Epoch [{epoch+1}/{NUM_EPOCHS_ARC}] Evaluation Loss: {avg_eval_loss:.4f}\")\n",
    "\n",
    "        # --- Checkpointing ---\n",
    "        if accelerator_arc and accelerator_arc.is_main_process:\n",
    "            if (epoch + 1) % 5 == 0: # Save every 5 epochs\n",
    "                accelerator_arc.wait_for_everyone()\n",
    "                unwrapped_model = accelerator_arc.unwrap_model(ctm_model_arc)\n",
    "                \n",
    "                # --- DeepSpeed Check ---\n",
    "                if hasattr(accelerator_arc.state, 'deepspeed_plugin') and accelerator_arc.state.deepspeed_plugin is not None:\n",
    "                    # DeepSpeed handles checkpointing via accelerator.save_state\n",
    "                    accelerator_arc.save_state(os.path.join(CHECKPOINT_DIR_ARC, f\"epoch_{epoch+1}\"))\n",
    "                else:\n",
    "                    # For other setups, save with safetensors on rank 0\n",
    "                    save_file(unwrapped_model.state_dict(), os.path.join(CHECKPOINT_DIR_ARC, f\"ctm_model_arc_epoch_{epoch+1}.safetensors\"))\n",
    "                \n",
    "                print(f\"✓ Checkpoint saved for epoch {epoch+1} to {CHECKPOINT_DIR_ARC}\")\n",
    "\n",
    "        if hasattr(ctm_model_arc, 'sleep_down'):\n",
    "            ctm_model_arc.sleep_down()\n",
    "\n",
    "    print(\"\\n🎉 ARC-AGI-2 Meta-Learning Training Phase Completed!\")\n",
    "\n",
    "\n",
    "# --- Principles Alignment Training Loop ---\n",
    "if principles_loader and NUM_EPOCHS_PRINCIPLES > 0 and 'ctm_model_arc' in globals() and ctm_model_arc is not None:\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(f\"🚀 STARTING PHASE: Principles Alignment Training\")\n",
    "    print(f\"   Epochs: {NUM_EPOCHS_PRINCIPLES}\")\n",
    "    print(\"=\"*60 + \"\\n\")\n",
    "\n",
    "    for epoch in range(NUM_EPOCHS_PRINCIPLES):\n",
    "        ctm_model_arc.train()\n",
    "        if hasattr(ctm_model_arc, 'wake_up'):\n",
    "            ctm_model_arc.wake_up()\n",
    "\n",
    "        total_epoch_loss_principles = 0\n",
    "        \n",
    "        progress_bar_principles = tqdm(enumerate(principles_loader), total=len(principles_loader), desc=f\"Principles Epoch {epoch + 1}\")\n",
    "\n",
    "        for batch_idx, batch_data in progress_bar_principles:\n",
    "            if not batch_data or batch_data['input_byte_sequences'].numel() == 0:\n",
    "                print(f\"Skipping empty principles batch {batch_idx}\")\n",
    "                continue\n",
    "\n",
    "            input_bytes = batch_data['input_byte_sequences'].to(accelerator_arc.device if accelerator_arc else device)\n",
    "            \n",
    "            current_batch_size = input_bytes.size(0)\n",
    "\n",
    "            optimizer_arc.zero_grad()\n",
    "            \n",
    "            autocast_context = accelerator_arc.autocast() if accelerator_arc else autocast(enabled=USE_MIXED_PRECISION, dtype=autocast_dtype)\n",
    "\n",
    "            with autocast_context:\n",
    "                # For principles, the input is the target. The model learns to reconstruct the principles.\n",
    "                model_output_dict = ctm_model_arc(\n",
    "                    byte_sequence=input_bytes,\n",
    "                    target_diffusion_output=input_bytes, # Self-supervision\n",
    "                    mode='ctm_controlled_diffusion',\n",
    "                    timestep=torch.randint(0, config_arc_diffusion.diffusion_steps, (current_batch_size,), device=input_bytes.device).long()\n",
    "                )\n",
    "\n",
    "                total_loss = model_output_dict.get('total_loss', torch.tensor(0.0, device=input_bytes.device))\n",
    "\n",
    "            if torch.isnan(total_loss) or torch.isinf(total_loss):\n",
    "                print(f\"[NaN or Inf Loss Detected] in Principles training at Epoch {epoch+1}, Batch {batch_idx+1}. Skipping backward pass.\")\n",
    "                continue\n",
    "\n",
    "            if accelerator_arc:\n",
    "                accelerator_arc.backward(total_loss)\n",
    "                if (batch_idx + 1) % GRADIENT_ACCUMULATION_STEPS == 0:\n",
    "                   if accelerator_arc.sync_gradients:\n",
    "                       accelerator_arc.clip_grad_norm_(ctm_model_arc.parameters(), MAX_GRAD_NORM)\n",
    "                   optimizer_arc.step()\n",
    "                   optimizer_arc.zero_grad()\n",
    "            else:\n",
    "                scaler.scale(total_loss).backward()\n",
    "                if (batch_idx + 1) % GRADIENT_ACCUMULATION_STEPS == 0:\n",
    "                    scaler.unscale_(optimizer_arc)\n",
    "                    torch.nn.utils.clip_grad_norm_(ctm_model_arc.parameters(), MAX_GRAD_NORM)\n",
    "                    scaler.step(optimizer_arc)\n",
    "                    scaler.update()\n",
    "                    optimizer_arc.zero_grad()\n",
    "\n",
    "            total_epoch_loss_principles += total_loss.item()\n",
    "            progress_bar_principles.set_postfix({\n",
    "                'loss': total_loss.item(),\n",
    "                'avg_loss': total_epoch_loss_principles / (batch_idx + 1)\n",
    "            })\n",
    "\n",
    "        avg_epoch_loss_principles = total_epoch_loss_principles / len(principles_loader) if len(principles_loader) > 0 else 0\n",
    "        print(f\"Principles Epoch [{epoch+1}/{NUM_EPOCHS_PRINCIPLES}] completed. Average Loss: {avg_epoch_loss_principles:.4f}\")\n",
    "\n",
    "        # --- Checkpointing for Principles Training ---\n",
    "        if accelerator_arc and accelerator_arc.is_main_process:\n",
    "            if (epoch + 1) % 5 == 0: # Save every 5 epochs\n",
    "                accelerator_arc.wait_for_everyone()\n",
    "                unwrapped_model = accelerator_arc.unwrap_model(ctm_model_arc)\n",
    "                \n",
    "                checkpoint_dir = os.path.join(CHECKPOINT_DIR_ARC, \"principles_checkpoints\")\n",
    "                os.makedirs(checkpoint_dir, exist_ok=True)\n",
    "\n",
    "                if hasattr(accelerator_arc.state, 'deepspeed_plugin') and accelerator_arc.state.deepspeed_plugin is not None:\n",
    "                    accelerator_arc.save_state(os.path.join(checkpoint_dir, f\"epoch_{epoch+1}\"))\n",
    "                else:\n",
    "                    save_file(unwrapped_model.state_dict(), os.path.join(checkpoint_dir, f\"ctm_model_arc_epoch_{epoch+1}.safetensors\"))\n",
    "                \n",
    "                print(f\"✓ Principles checkpoint saved for epoch {epoch+1} to {checkpoint_dir}\")\n",
    "\n",
    "        if hasattr(ctm_model_arc, 'sleep_down'):\n",
    "            ctm_model_arc.sleep_down()\n",
    "\n",
    "    print(\"\\n🎉 Principles Alignment Training Phase Completed!\")\n",
    "\n",
    "#The Mixed Context training is not needed since the Program Synthesizer is not being used and the CTM Nueron Network is being used instead. \n",
    "# --- Mixed Context Training ---\n",
    "'''\n",
    "import random\n",
    "\n",
    "class MixedContextDataset(Dataset):\n",
    "    def __init__(self, num_samples=1000, short_len=256, long_len=4096, vocab_size=256):\n",
    "        self.num_samples = num_samples\n",
    "        self.short_len = short_len\n",
    "        self.long_len = long_len\n",
    "        self.vocab_size = vocab_size\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.num_samples\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if random.random() < 0.5:\n",
    "            # Short sequence, pack multiple\n",
    "            num_packed = self.long_len // self.short_len\n",
    "            packed = []\n",
    "            mask = torch.zeros(self.long_len, self.long_len)\n",
    "            pos = 0\n",
    "            for i in range(num_packed):\n",
    "                seq = torch.randint(0, self.vocab_size, (self.short_len,))\n",
    "                packed.append(seq)\n",
    "                # Causal mask for this segment\n",
    "                segment_mask = torch.tril(torch.ones(self.short_len, self.short_len))\n",
    "                mask[pos:pos+self.short_len, pos:pos+self.short_len] = segment_mask\n",
    "                pos += self.short_len\n",
    "            sequence = torch.cat(packed)[:self.long_len]\n",
    "            is_long = False\n",
    "        else:\n",
    "            # Long sequence\n",
    "            sequence = torch.randint(0, self.vocab_size, (self.long_len,))\n",
    "            mask = torch.tril(torch.ones(self.long_len, self.long_len))\n",
    "            is_long = True\n",
    "\n",
    "        return {'sequence': sequence, 'mask': mask, 'is_long': is_long}\n",
    "\n",
    "#mixed_dataset = MixedContextDataset()\n",
    "\n",
    "#mixed_loader = DataLoader(mixed_dataset, batch_size=4, shuffle=True)\n",
    "\n",
    "# Mixed training loop\n",
    " for epoch in range(5): \n",
    "     ctm_model_arc.train()\n",
    "     total_loss = 0\n",
    "     for batch in mixed_loader:\n",
    "        sequence = batch['sequence'].to(device)\n",
    "        attn_mask = batch['mask'].to(device)\n",
    "        is_long = batch['is_long']\n",
    "\n",
    "         Assuming model has train_forward that computes loss\n",
    "        loss = ctm_model_arc.train_forward(sequence, attn_mask, use_rescaled_rope=is_long)\n",
    "\n",
    "        optimizer_arc.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer_arc.step()\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    print(f\"Mixed Context Epoch {epoch+1} Avg Loss: {total_loss / len(mixed_loader)}\")\n",
    "\n",
    "print(\"\\n🎉 Mixed Context Training Completed!\")\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a9ed5119-b7db-4a29-96c0-15d04f546f3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ARC_EVAL_DIR: /workspace/Arc-AGI-2/contineous-thought-machines/examples/contineous-thought-machines/data/evaluation\n",
      "Exists? False\n"
     ]
    }
   ],
   "source": [
    "print(f\"ARC_EVAL_DIR: {ARC_EVAL_DIR}\")\n",
    "print(\"Exists?\", os.path.exists(ARC_EVAL_DIR))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "752a08e7-f218-4c0f-87d7-1b718d508fdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "ARC_EVAL_DIR = \"/workspace/Arc-AGI-2/contineous-thought-machines/data/evaluation\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "61acb935-a668-4957-9f37-f6bc6c7ac4ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ARC_EVAL_DIR: /workspace/Arc-AGI-2/contineous-thought-machines/data/evaluation\n",
      "Exists? True\n"
     ]
    }
   ],
   "source": [
    "print(f\"ARC_EVAL_DIR: {ARC_EVAL_DIR}\")\n",
    "print(\"Exists?\", os.path.exists(ARC_EVAL_DIR))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0a86e362-591b-4704-8466-4c64f47f67c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0934a4d8.json  332f06d7.json  65b59efc.json  8e5c0c38.json  c7f57c3e.json\n",
      "135a2760.json  35ab12c3.json  67e490f4.json  8f215267.json  cb2d8a2c.json\n",
      "136b0064.json  36a08778.json  6e453dd6.json  8f3a5a89.json  cbebaa4b.json\n",
      "13e47133.json  38007db0.json  6e4f6532.json  9385bd28.json  d35bdbdc.json\n",
      "142ca369.json  3a25b0d8.json  6ffbe589.json  97d7923e.json  d59b0160.json\n",
      "16b78196.json  3dc255db.json  71e489b6.json  981571dc.json  d8e07eb2.json\n",
      "16de56c4.json  3e6067c3.json  7491f3cf.json  9aaea919.json  da515329.json\n",
      "1818057f.json  409aa875.json  7666fa5d.json  9bbf930d.json  db0c5428.json\n",
      "195c6913.json  446ef5d2.json  78332cb0.json  a251c730.json  db695cfb.json\n",
      "1ae2feb7.json  45a5af55.json  7b0280bc.json  a25697e4.json  dbff022c.json\n",
      "20270e3b.json  4a21e3da.json  7b3084d4.json  a32d8b75.json  dd6b8c4b.json\n",
      "20a9e565.json  4c3d4a41.json  7b5033c1.json  a395ee82.json  de809cff.json\n",
      "21897d95.json  4c416de3.json  7b80bb43.json  a47bf94d.json  dfadab01.json\n",
      "221dfab4.json  4c7dc4dd.json  7c66cb00.json  a6f40cea.json  e12f9a14.json\n",
      "247ef758.json  4e34c42c.json  7ed72f31.json  aa4ec2a5.json  e3721c99.json\n",
      "269e22fb.json  53fb4810.json  800d221b.json  abc82100.json  e376de54.json\n",
      "271d71e2.json  5545f144.json  80a900e0.json  b0039139.json  e8686506.json\n",
      "28a6681f.json  581f7754.json  8698868d.json  b10624e5.json  e87109e9.json\n",
      "291dc1e1.json  58490d8a.json  88bcf3b4.json  b5ca7ac4.json  edb79dae.json\n",
      "2b83f449.json  58f5dbd5.json  88e364bc.json  b6f77b65.json  eee78d87.json\n",
      "2ba387bc.json  5961cc34.json  89565ca0.json  b99e7126.json  f560132c.json\n",
      "2c181942.json  5dbc8537.json  898e7135.json  b9e38dc0.json  f931b4a8.json\n",
      "2d0172a1.json  62593bfd.json  8b7bacbf.json  bf45cf4b.json  faa9f03d.json\n",
      "31f7f899.json  64efde09.json  8b9c3697.json  c4d067a0.json  fc7cae8d.json\n"
     ]
    }
   ],
   "source": [
    "ls /workspace/Arc-AGI-2/contineous-thought-machines/data/evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d32d4e28",
   "metadata": {},
   "source": [
    "# Evaluation Arc_AGI_2 Phase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd259053-d85c-4591-bc59-d57bc5495307",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import numpy as np\n",
    "import glob\n",
    "import json\n",
    "import traceback\n",
    "import random\n",
    "from dataclasses import dataclass, field\n",
    "from typing import List, Optional, Any\n",
    "import math\n",
    "\n",
    "# --- De-noising and Meta-Learning Components ---\n",
    "\n",
    "def perform_online_update(model, optimizer, scheduler, input_bytes, corrected_grid_np: np.ndarray, device):\n",
    "    \"\"\"\n",
    "    Performs a single, targeted fine-tuning step on the end-to-end model.\n",
    "    \"\"\"\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    target_bytes_single = serialize_and_pad_grid(corrected_grid_np, max_len=MAX_SEQUENCE_LENGTH, pad_value=PADDING_BYTE_VALUE)\n",
    "    target_bytes_np = np.frombuffer(target_bytes_single, dtype=np.uint8).copy()\n",
    "    target_bytes_tensor = torch.from_numpy(target_bytes_np).to(torch.uint8).unsqueeze(0).to(device)\n",
    "\n",
    "    train_timestep = torch.zeros(1, device=device).long()\n",
    "    \n",
    "    # The model's forward pass calculates all necessary losses internally.\n",
    "    output_dict = model(\n",
    "        byte_sequence=input_bytes,\n",
    "        mode='ctm_controlled_diffusion',\n",
    "        target_diffusion_output=target_bytes_tensor,\n",
    "        timestep=train_timestep,\n",
    "        task_name=\"ARC_AGI_2_ONLINE_LEARN\"\n",
    "    )\n",
    "\n",
    "    loss = output_dict.get('total_loss')\n",
    "\n",
    "    if loss is not None and torch.isfinite(loss):\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "        print(f\"  > Model updated with loss: {loss.item():.4f}. LR: {scheduler.get_last_lr()[0]:.6f}\")\n",
    "    else:\n",
    "        print(\"  > Skipping online update due to invalid loss.\")\n",
    "\n",
    "    model.eval()\n",
    "\n",
    "# Setup module paths based on user-provided successful import logic\n",
    "print(\"--- Setting up module paths ---\")\n",
    "project_root = '/workspaces/Arc-AGI-2'\n",
    "module_path = os.path.join(project_root, 'contineous-thought-machines')\n",
    "\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)\n",
    "    print(f\"Added to sys.path: {module_path}\")\n",
    "\n",
    "try:\n",
    "    from safetensors.torch import load_file\n",
    "except ImportError:\n",
    "    print(\"Warning: safetensors not found. Loading .safetensors will fail.\")\n",
    "    def load_file(path, device=\"cpu\"):\n",
    "        raise ImportError(f\"safetensors is not installed, cannot load {path}\")\n",
    "\n",
    "print(\"\\n--- Statically importing EnhancedCTMDiffusion model ---\")\n",
    "EnhancedCTMDiffusion = None\n",
    "try:\n",
    "    from contineous_thought_machines.models.ctm_Diffusion_NEWNEW import EnhancedCTMDiffusion\n",
    "    print(\" -> Successfully imported EnhancedCTMDiffusion from models package.\")\n",
    "except ImportError as e_direct:\n",
    "    print(f\"FATAL: Import from models package failed. Last error: {e_direct}\")\n",
    "    EnhancedCTMDiffusion = None\n",
    "\n",
    "try:\n",
    "    from accelerate import Accelerator\n",
    "    ACCELERATE_AVAILABLE = True\n",
    "except ImportError:\n",
    "    print(\"Warning: Hugging Face Accelerate not found. Will run on a single device.\")\n",
    "    ACCELERATE_AVAILABLE = False\n",
    "    Accelerator = None\n",
    "\n",
    "# --- Constants and Configuration ---\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "MAX_GRID_SIZE = (30, 30)\n",
    "PADDING_VALUE = -1\n",
    "ARC_INPUT_FLAT_DIM = MAX_GRID_SIZE[0] * MAX_GRID_SIZE[1]\n",
    "MAX_SEQUENCE_LENGTH = 8192\n",
    "PADDING_BYTE_VALUE = 0\n",
    "NUM_ARC_SYMBOLS = 10\n",
    "LEARNING_RATE = 1e-4\n",
    "\n",
    "# --- Data Handling ---\n",
    "def pad_grid(grid_list, max_dims, pad_value):\n",
    "    grid_np = np.array(grid_list, dtype=np.int32)\n",
    "    padded_grid = np.full(max_dims, pad_value, dtype=np.int32)\n",
    "    h, w = grid_np.shape\n",
    "    padded_grid[:h, :w] = grid_np\n",
    "    return padded_grid\n",
    "\n",
    "def serialize_and_pad_grid(grid, max_len=MAX_SEQUENCE_LENGTH, pad_value=PADDING_BYTE_VALUE):\n",
    "    flat_array = np.array(grid, dtype=np.uint8).flatten()\n",
    "    byte_sequence = flat_array.tobytes()\n",
    "    padding_len = max_len - len(byte_sequence)\n",
    "    if padding_len < 0:\n",
    "        return byte_sequence[:max_len]\n",
    "    return byte_sequence + bytes([pad_value] * padding_len)\n",
    "\n",
    "@dataclass\n",
    "class EnhancedCTMConfig: # Renamed from ContinualLearningConfig for consistency in the target file\n",
    "    \"\"\"Enhanced configuration for continual learning CTM-diffusion model,\n",
    "    incorporating binary processing, multi-task learning, and advanced CTM features.\"\"\"\n",
    "    \n",
    "    # Model architecture (General Transformer/Diffusion settings)\n",
    "    d_model: int = 512  # Main model dimensionality\n",
    "    n_heads: int = 8\n",
    "    n_layers: int = 24\n",
    "    max_sequence_length: int = 8192 # Max input sequence length in terms of bytes or patches\n",
    "    dropout: float = 0.1\n",
    "    \n",
    "    # --- Byte Processing Options ---\n",
    "    patch_embedding_dim: int = 256         # <<< NEW: Output embedding dimension per patch from patcher\n",
    "    patch_encoder_cnn_channels: int = 64   # <<< NEW: Intermediate channels for CNN patch encoder\n",
    "\n",
    "    # --- Dynamic Entropy Patching Options (Inspired by BLT paper) ---\n",
    "    use_dynamic_entropy_patcher: bool = True # Flag to enable dynamic entropy-based patching\n",
    "    entropy_patcher_threshold_type: str = \"global\"  # 'global' or 'relative_monotonic'\n",
    "    entropy_patcher_global_threshold: float = 0.75 # Entropy threshold for 'global' type\n",
    "    entropy_patcher_relative_threshold: float = 0.1 # Entropy diff threshold for 'relative_monotonic'\n",
    "    entropy_patcher_min_patch_size: int = 4      # Minimum number of bytes in a dynamic patch\n",
    "    entropy_patcher_max_patch_size: int = 128    # Maximum number of bytes in a dynamic patch (for CNN encoder)\n",
    "    \n",
    "    # --- Learnable Entropy Model Parameters (for _EntropyProxyModel) ---\n",
    "    entropy_model_byte_vocab_size: int = 256\n",
    "    entropy_model_embedding_dim: int = 64\n",
    "    entropy_model_hidden_dim: int = 128\n",
    "    entropy_model_num_layers: int = 1\n",
    "    entropy_model_dropout: float = 0.1\n",
    "    entropy_model_loss_weight: float = 0.1 # Weight for its auxiliary loss contribution\n",
    "    # Note: These parameters are used if use_dynamic_entropy_patcher is True,\n",
    "    # as LearnedBytePatcherEncoder now instantiates the learnable _EntropyProxyModel.\n",
    "    \n",
    "    # Fallback if not using learned_patch_encoder or dynamic_entropy_patcher\n",
    "    byte_embedding_dim: int = 256\n",
    "    multi_granularity: bool = False # Default to False if patcher is preferred\n",
    "    # multi_granularity_output_dim is complex to predefine, MGP should expose its output dim.\n",
    "    # For now, if multi_granularity is True AND use_learned_patch_encoder is False, this would be used.\n",
    "    multi_granularity_output_dim: int = 256 # Placeholder if MGP is used.\n",
    "    \n",
    "    hierarchical_processing: bool = True # General flag, could apply to patcher or MGP\n",
    "    \n",
    "    # CTM Core Parameters (Specific to the OriginalCTMCore module)\n",
    "    # These are prefixed with 'ctm_' to distinguish from general model params\n",
    "    ctm_iterations: int = 5  # Original 'iterations'\n",
    "    ctm_d_model: int = 512   # Original 'd_model' for CTM's internal latent space\n",
    "    ctm_input_dim: int = 256 # Dimensionality of inputs to CTM (e.g., from byte embeddings or other features)\n",
    "                             # This was 'd_input' in OriginalCTMCore if it took external features.\n",
    "                             # If CTM processes outputs of byte_embedding, this might be byte_embedding_dim.\n",
    "    ctm_heads: int = 8       # Attention heads within CTM\n",
    "    ctm_n_synch_out: int = 64\n",
    "    ctm_n_synch_action: int = 64\n",
    "    ctm_synapse_depth: int = 3\n",
    "    ctm_memory_length: int = 10\n",
    "    ctm_deep_nlms: bool = True\n",
    "    ctm_memory_hidden_dims: int = 2048\n",
    "    ctm_do_layernorm_nlm: bool = False\n",
    "    ctm_out_dims: int = 512  # Output dimension of CTM's own projector\n",
    "    ctm_prediction_reshaper: list = field(default_factory=lambda: [-1])\n",
    "    ctm_dropout: float = 0.1\n",
    "    ctm_dropout_nlm: Optional[float] = None\n",
    "    # Neuron selection strategy. Available options:\n",
    "    # Legacy: 'first-last', 'random', 'random-pairing'\n",
    "    # Biologically-inspired: 'bio_hebbian', 'bio_plasticity', 'bio_competitive',\n",
    "    #                        'bio_homeostatic', 'bio_evolutionary', 'bio_stdp',\n",
    "    #                        'bio_criticality', 'bio_multi_objective'\n",
    "    # Hybrid: 'adaptive_random', 'performance_guided', 'task_aware'\n",
    "    ctm_neuron_select_type: str = 'bio_multi_objective'\n",
    "    ctm_n_random_pairing_self: int = 0\n",
    "    \n",
    "    # Diffusion Parameters\n",
    "    diffusion_steps: int = 1000\n",
    "    noise_schedule: str = \"cosine\" # e.g., \"linear\", \"cosine\"\n",
    "    diffusion_beta_start: float = 0.0001\n",
    "    diffusion_beta_end: float = 0.02\n",
    "    diffusion_timesteps: int = 1000 # Number of timesteps for the diffusion process\n",
    "    ctm_diffusion_coupling_strength: float = 0.8 # How CTM influences diffusion\n",
    "    adaptive_scheduling: bool = True  # CTM-adaptive diffusion timestep scheduling\n",
    "    iterative_refinement: bool = True # Iterative CTM-diffusion refinement for sampling\n",
    "    \n",
    "    # Training Efficiency\n",
    "    mixed_precision: bool = True\n",
    "    gradient_checkpointing: bool = True\n",
    "    sparse_attention: bool = True  # Now implemented with BinarySparseAttention\n",
    "    adaptive_depth: bool = False   # Defaulting to False, can be enabled if implemented\n",
    "    use_activity_plasticity: bool = True # To enable/disable plasticity updates; Needs to be set to TRUE\n",
    "    ctm_use_internal_feedback: bool = True # Enable self-modulating feedback within the CTM core\n",
    "\n",
    "    # --- Bidirectional Reasoning Parameters ---\n",
    "    enable_bidirectional_reasoning: bool = True # Allows CTM to move forward/backward in its thought process\n",
    "    reasoning_step_gating_threshold: float = 0.7 # Confidence threshold for the reasoning controller to terminate\n",
    "    max_reasoning_steps: int = 15 # Max total steps in a bidirectional reasoning loop to prevent infinite loops\n",
    "    \n",
    "    # Sparse Attention Parameters\n",
    "    sparse_attention_ratio: float = 0.1  # Keep only 10% of attention connections\n",
    "    binary_pattern_size: int = 8  # Size of binary patterns to detect\n",
    "\n",
    "    # Attention Mechanism Type\n",
    "    attention_type: str = \"WINA\"  # Options: \"standard\", \"binary_sparse\", \"WINA\" #Need to use WINA attention in place of \"standard\"\n",
    "\n",
    "    positional_embedding_type: Optional[str] = 'multi-learnable-fourier' # e.g., 'custom-rotational-1d', 'learnable-fourier', multi-learnable-fourier' #Can set the value here.\n",
    "    positional_embedding_dim: Optional[int] = None  # Dimension of the positional embedding, defaults to ctm_input_dim if None\n",
    "    reshape_patch_sequence_to_grid: bool = True # If True, reshape patch sequence to a 2D grid for 2D PEs. Must set to true if using 2D Grid for Positional Embeddings.\n",
    "    patch_grid_width: Optional[int] = None       # Desired width of the patch grid if reshaping\n",
    "\n",
    "    # --- Hierarchical Reasoning Model (HRM) Parameters ---\n",
    "    use_hrm_core: bool = True # Set to True to use the HierarchicalCTM core\n",
    "    hrm_high_level_cycles: int = 4 # N: Number of high-level cycles\n",
    "    hrm_low_level_timesteps: int = 8 # T: Number of low-level timesteps per high-level cycle\n",
    "    program_vocab_size: int = 1024 # Vocabulary size for the program synthesizer\n",
    "    program_synth_n_heads: int = 4\n",
    "    program_synth_n_layers: int = 3\n",
    "    program_synth_d_ff: int = 1024\n",
    "    ltm_size: int = 2048 # Size of the long-term memory\n",
    "    ltm_surprise_threshold: float = 0.6 # Surprise threshold for storing in LTM\n",
    "    replay_batch_size: int = 4 # Batch size for memory replay\n",
    "    replay_policy: str = \"surprise_weighted_replay\" # \"simple_replay\", \"surprise_weighted_replay\", \"usefulness_replay\"\n",
    "\n",
    "    # Pipeline Parallelism Parameters\n",
    "    enable_pipeline_parallelism: bool = True\n",
    "    pipeline_stages: int = 3  # CTM, Diffusion prep, Diffusion exec\n",
    "    pipeline_overlap_ratio: float = 0.7  # Target overlap ratio\n",
    "    \n",
    "    # Adaptive Batch Sizing Parameters\n",
    "    enable_adaptive_batching: bool = True\n",
    "    initial_batch_size: int = 32\n",
    "    min_batch_size: int = 8\n",
    "    max_batch_size: int = 256\n",
    "    batch_adaptation_frequency: int = 100\n",
    "    memory_threshold_high: float = 0.85\n",
    "    memory_threshold_low: float = 0.6\n",
    "    \n",
    "    # Smart Data Sampling Parameters\n",
    "    enable_smart_sampling: bool = True\n",
    "    sample_importance_weight: float = 0.6\n",
    "    sample_diversity_weight: float = 0.4\n",
    "    initial_sample_ratio: float = 0.3\n",
    "    complexity_analysis_enabled: bool = True\n",
    "    \n",
    "    # Multi-input/output parameters\n",
    "    num_inputs: int = 1  # Number of input streams\n",
    "    num_outputs: int = 1  # Number of output heads\n",
    "    output_dims: List[int] = field(default_factory=lambda: [64])  # Dimensions for each output head\n",
    "    \n",
    "    # Self-supervised learning\n",
    "    ssl_dim: int = 128  # Dimension for self-supervised projection\n",
    "    ssl_weight: float = 0.1  # Weight for self-supervised loss\n",
    "    ssl_temperature: float = 0.07  # Temperature for contrastive loss\n",
    "    ssl_noise_std: float = 0.1  # Noise standard deviation for contrastive augmentation\n",
    "    \n",
    "    # Spatiotemporal Processing\n",
    "    use_spatial: bool = True  # Enable spatial processing for image/video data\n",
    "    \n",
    "    # WINA Attention\n",
    "    use_wina_attention: bool = True  # Enable WINA sparse attention\n",
    "    \n",
    "    # Multi-task Learning Parameters\n",
    "    max_tasks: int = 50  # Maximum number of tasks for continual learning\n",
    "    # Added to resolve TypeError for unexpected keyword arguments\n",
    "    vocab_size: Optional[int] = None\n",
    "    output_audio_bytes: bool = False\n",
    "    inferred_task_latent_dim: Optional[int] = None # Default to None, __post_init__ handles it\n",
    "    use_hipa_attention: bool = False # Default to False\n",
    "    hipa_num_heads: Optional[int] = None # Default to None\n",
    "    audio_output_dtype_str: Optional[str] = \"float32\" # Default as per __post_init__ logic\n",
    "    unet_input_feature_dim: Optional[int] = None # Default to None, __post_init__ calculates it\n",
    "\n",
    "    # --- JEPA Training Parameters (Integrated with LearnedBytePatcherEncoder) ---\n",
    "    use_jepa_training: bool = False\n",
    "    # jepa_embed_dim will be derived from patch_embedding_dim if dynamic_entropy_patcher is used\n",
    "    jepa_predictor_hidden_dim: int = 512 # Hidden dimension of JEPA predictor MLP\n",
    "    jepa_mask_ratio_min: float = 0.15 # Min proportion of patch sequence to mask for target\n",
    "    jepa_mask_ratio_max: float = 0.75 # Max proportion of patch sequence to mask for target\n",
    "    jepa_context_scale_min: float = 0.3 # Min proportion of patches for context\n",
    "    jepa_context_scale_max: float = 0.7 # Max proportion of patches for context\n",
    "    jepa_momentum_beta: float = 0.996 # Momentum for target encoder update\n",
    "    jepa_loss_weight: float = 0.1 # Weight for the JEPA loss component\n",
    "    jepa_num_target_blocks: int = 1 # Number of target blocks to predict\n",
    "\n",
    "    # --- Global Plasticity Loss Parameters ---\n",
    "    local_hebbian_loss_weight: float = 0.01 # New weight for backprop-based hebbian loss\n",
    "\n",
    "    # --- Basal Ganglia Parameters --- #Controls action suppression so that the model's unwanted first unrelated thoughts are suppressed which helps with model safety. Is needed for action suppresion.\n",
    "    ctm_enable_basal_ganglia: bool = True\n",
    "    ctm_bg_dopamine_dim: int = 32\n",
    "\n",
    "    # --- Synaptic Empathy Parameters ---\n",
    "    enable_synaptic_empathy: bool = True # Set to True to use the new SynapticEmpathy module\n",
    "    synaptic_empathy_reward_weight: float = 0.1\n",
    "\n",
    "    # --- Mirror Neuron / High-Level Empathy Parameters ---\n",
    "    enable_mirror_neurons: bool = True # Set to True to use the high-level MirrorNeuronLayer\n",
    "    num_emotion_dim: int = 4 # Dimensionality of the emotion state vector\n",
    "    goal_dim: int = 8 # Dimensionality of the predicted goal vector\n",
    "    mirror_reward_weight: float = 0.2 # Weight for the selfless reward signal\n",
    "\n",
    "\n",
    "    # --- Confidence Thresholding Parameters ---\n",
    "    confidence_threshold: float = 0.0 # Confidence threshold for abstaining. If > 0, model can abstain.\n",
    " \n",
    "    # --- Consciousness Controller Parameters ---\n",
    "    enable_consciousness_controller: bool = True\n",
    "    consciousness_max_attention_steps: int = 100\n",
    "\n",
    "    def __post_init__(self):\n",
    "        # Validate output dimensions\n",
    "        if len(self.output_dims) != self.num_outputs:\n",
    "            raise ValueError(f\"output_dims length ({len(self.output_dims)}) must match num_outputs ({self.num_outputs})\")\n",
    "\n",
    "        # Merged content from the second __post_init__\n",
    "        if hasattr(self, 'ctm_prediction_reshaper') and self.ctm_prediction_reshaper == [-1] and self.vocab_size is not None:\n",
    "            pass\n",
    "        if hasattr(self, 'ctm_dropout_nlm') and self.ctm_dropout_nlm is None and hasattr(self, 'ctm_dropout'):\n",
    "            self.ctm_dropout_nlm = self.ctm_dropout\n",
    "        \n",
    "        if hasattr(self, 'ctm_neuron_select_type') and \\\n",
    "           VALID_NEURON_SELECT_TYPES is not None and self.ctm_neuron_select_type not in VALID_NEURON_SELECT_TYPES:\n",
    "            print(f\"Warning: ctm_neuron_select_type '{self.ctm_neuron_select_type}' is not in VALID_NEURON_SELECT_TYPES ({VALID_NEURON_SELECT_TYPES}).\")\n",
    "\n",
    "        if hasattr(self, 'positional_embedding_type') and self.positional_embedding_type is not None:\n",
    "            if VALID_POSITIONAL_EMBEDDING_TYPES is None: # Fallback if import failed\n",
    "                print(f\"Warning: VALID_POSITIONAL_EMBEDDING_TYPES not available for validation.\")\n",
    "            elif self.positional_embedding_type not in VALID_POSITIONAL_EMBEDDING_TYPES:\n",
    "                print(f\"Warning: positional_embedding_type '{self.positional_embedding_type}' is not in VALID_POSITIONAL_EMBEDDING_TYPES ({VALID_POSITIONAL_EMBEDDING_TYPES}).\")\n",
    "            if self.positional_embedding_dim is not None and self.positional_embedding_dim <= 0:\n",
    "                raise ValueError(\"positional_embedding_dim must be positive if set.\")\n",
    "            \n",
    "            if self.reshape_patch_sequence_to_grid:\n",
    "                if self.patch_grid_width is None or self.patch_grid_width <= 0:\n",
    "                    raise ValueError(\"patch_grid_width must be a positive integer if reshape_patch_sequence_to_grid is True.\")\n",
    "                if self.positional_embedding_type not in ['learnable-fourier', 'multi-learnable-fourier', 'custom-rotational']:\n",
    "                    print(f\"Warning: reshape_patch_sequence_to_grid is True, but positional_embedding_type ('{self.positional_embedding_type}') is not a typical 2D PE. Ensure compatibility.\")\n",
    "\n",
    "        # Validations for new patch encoder\n",
    "        if self.use_dynamic_entropy_patcher:\n",
    "            if self.patch_embedding_dim <= 0:\n",
    "                raise ValueError(\"patch_embedding_dim must be positive if use_dynamic_entropy_patcher is True.\")\n",
    "            if self.entropy_patcher_min_patch_size <= 0:\n",
    "                raise ValueError(\"entropy_patcher_min_patch_size must be positive.\")\n",
    "            if self.entropy_patcher_max_patch_size < self.entropy_patcher_min_patch_size:\n",
    "                raise ValueError(\"entropy_patcher_max_patch_size must be >= entropy_patcher_min_patch_size.\")\n",
    "            if self.entropy_patcher_threshold_type not in [\"global\", \"relative_monotonic\"]:\n",
    "                raise ValueError(\"entropy_patcher_threshold_type must be 'global' or 'relative_monotonic'.\")\n",
    "        elif self.multi_granularity and self.multi_granularity_output_dim <= 0:\n",
    "            print(\"Warning: multi_granularity_output_dim might not be correctly set for validation if not using a patcher and MGP is active.\")\n",
    "        \n",
    "        if not hasattr(self, 'inferred_task_latent_dim') or self.inferred_task_latent_dim is None:\n",
    "            print(\"Warning: inferred_task_latent_dim not found or is None in config, defaulting to 64.\")\n",
    "            self.inferred_task_latent_dim = 512\n",
    "        elif self.inferred_task_latent_dim <= 0: # This check is now safe\n",
    "            raise ValueError(\"inferred_task_latent_dim must be positive.\")\n",
    " \n",
    "        if hasattr(self, 'use_hipa_attention') and self.use_hipa_attention and \\\n",
    "            (not hasattr(self, 'hipa_num_heads') or self.hipa_num_heads <= 0):\n",
    "             raise ValueError(\"hipa_num_heads must be positive if use_hipa_attention is True.\")\n",
    " \n",
    "        if hasattr(self, 'audio_output_dtype_str'):\n",
    "            if self.audio_output_dtype_str == \"float32\":\n",
    "                self.audio_output_item_size = 4\n",
    "            elif self.audio_output_dtype_str == \"int16\":\n",
    "                self.audio_output_item_size = 2\n",
    "            else:\n",
    "                if hasattr(self, 'output_audio_bytes') and self.output_audio_bytes:\n",
    "                    raise ValueError(f\"Unsupported audio_output_dtype_str: {self.audio_output_dtype_str} when output_audio_bytes is True.\")\n",
    "                else:\n",
    "                    self.audio_output_item_size = 4\n",
    "        elif hasattr(self, 'output_audio_bytes') and self.output_audio_bytes:\n",
    "            if not hasattr(self, 'audio_output_dtype_str') or self.audio_output_dtype_str is None:\n",
    "                raise ValueError(\"audio_output_dtype_str must be defined in config if output_audio_bytes is True.\")\n",
    "        else:\n",
    "            self.audio_output_item_size = 4\n",
    "\n",
    "        # Calculate unet_input_feature_dim if not set\n",
    "        if self.unet_input_feature_dim is None:\n",
    "            if self.max_sequence_length <= 0 or self.audio_output_item_size <= 0:\n",
    "                raise ValueError(\"max_sequence_length and audio_output_item_size must be positive to calculate unet_input_feature_dim.\")\n",
    "            self.unet_input_feature_dim = self.max_sequence_length // self.audio_output_item_size\n",
    "            if self.unet_input_feature_dim <= 0:\n",
    "                raise ValueError(f\"Calculated unet_input_feature_dim ({self.unet_input_feature_dim}) must be positive. Check max_sequence_length and audio_output_item_size.\")\n",
    "        elif self.unet_input_feature_dim <= 0:\n",
    "            raise ValueError(\"unet_input_feature_dim, if set, must be positive.\")\n",
    "\n",
    "        if self.use_jepa_training:\n",
    "            if not (0 < self.jepa_mask_ratio_min < 1 and 0 < self.jepa_mask_ratio_max < 1 and self.jepa_mask_ratio_min <= self.jepa_mask_ratio_max):\n",
    "                raise ValueError(\"JEPA mask ratios must be between 0 and 1, with min <= max.\")\n",
    "            if not (0 < self.jepa_context_scale_min < 1 and 0 < self.jepa_context_scale_max < 1 and self.jepa_context_scale_min <= self.jepa_context_scale_max):\n",
    "                raise ValueError(\"JEPA context scales must be between 0 and 1, with min <= max.\")\n",
    "            if not (0 <= self.jepa_momentum_beta < 1):\n",
    "                raise ValueError(\"jepa_momentum_beta must be between 0 and 1.\")\n",
    "            if self.jepa_num_target_blocks <= 0:\n",
    "                raise ValueError(\"jepa_num_target_blocks must be positive.\")\n",
    "            if not self.use_dynamic_entropy_patcher:\n",
    "                print(\"Warning: JEPA training is enabled but use_dynamic_entropy_patcher is False. JEPA relies on the patch embeddings from LearnedBytePatcherEncoder.\")\n",
    "\n",
    "config_arc_diffusion = EnhancedCTMConfig(\n",
    "    enable_consciousness_controller=True,\n",
    "    consciousness_max_attention_steps=100\n",
    ")\n",
    "\n",
    "print(\"✓ EnhancedCTMConfig for ARC (config_arc_diffusion) created.\")\n",
    "\n",
    "if EnhancedCTMDiffusion is not None:\n",
    "    ctm_model_arc = EnhancedCTMDiffusion(config=config_arc_diffusion).to(device)\n",
    "    print(\"✓ EnhancedCTMDiffusion model for ARC (ctm_model_arc) initialized.\")\n",
    "    optimizer_arc = optim.AdamW(ctm_model_arc.parameters(), lr=LEARNING_RATE, weight_decay=1e-4)\n",
    "\n",
    "    if ACCELERATE_AVAILABLE:\n",
    "        print(\" -> Preparing components with Hugging Face Accelerate...\")\n",
    "        accelerator_arc = Accelerator()\n",
    "        ctm_model_arc, optimizer_arc = accelerator_arc.prepare(ctm_model_arc, optimizer_arc)\n",
    "        print(\"✓ ARC model and optimizer prepared with Accelerate.\")\n",
    "else:\n",
    "    print(\"⚠️ EnhancedCTMDiffusion model could not be initialized.\")\n",
    "    ctm_model_arc, optimizer_arc, accelerator_arc = None, None, None\n",
    "\n",
    "\n",
    "class ARCEvalDataset(Dataset):\n",
    "    def __init__(self, data_path, max_grid_size=MAX_GRID_SIZE, padding_value=PADDING_VALUE):\n",
    "        self.task_files = glob.glob(os.path.join(data_path, \"*.json\"))\n",
    "        if not self.task_files:\n",
    "            print(f\"Warning: No .json files found at path: {data_path}\")\n",
    "        self.max_grid_size = max_grid_size\n",
    "        self.padding_value = padding_value\n",
    "        self.tasks = [json.load(open(f)) for f in self.task_files]\n",
    "        print(f\"Loaded {len(self.tasks)} tasks from {data_path}.\")\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.tasks)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        task_data = self.tasks[idx]\n",
    "        processed_task = {'train': [], 'test': [], 'id': os.path.basename(self.task_files[idx])}\n",
    "        for pair_type in ['train', 'test']:\n",
    "            for item in task_data.get(pair_type, []):\n",
    "                input_grid = item['input']\n",
    "                output_grid = item['output']\n",
    "                original_input_dims = (len(input_grid), len(input_grid[0]) if input_grid else 0)\n",
    "                original_output_dims = (len(output_grid), len(output_grid[0]) if output_grid else 0)\n",
    "                padded_input = pad_grid(input_grid, self.max_grid_size, self.padding_value)\n",
    "                padded_output = pad_grid(output_grid, self.max_grid_size, self.padding_value)\n",
    "                processed_task[pair_type].append({\n",
    "                    'input': torch.from_numpy(padded_input).long(),\n",
    "                    'output': torch.from_numpy(padded_output).long(),\n",
    "                    'original_input_dims': original_input_dims,\n",
    "                    'original_output_dims': original_output_dims\n",
    "                })\n",
    "        return processed_task\n",
    "\n",
    "ARC_EVAL_DIR = \"/workspace/Arc-AGI-2/contineous-thought-machines/data/evaluation\"\n",
    "CHECKPOINT_DIR_ARC = \"/workspaces/Arc-AGI-2/contineous-thought-machines/examples/checkpoints/ctm_arc_agi_2_enhanced_diffusion\"\n",
    "CHECKPOINT_DIR_PRINCIPLES = os.path.join(CHECKPOINT_DIR_ARC, \"principles_checkpoints\")\n",
    "NUM_EPOCHS_ARC = 20\n",
    "NUM_EPOCHS_PRINCIPLES = 10 # Should match the value in training.py\n",
    "\n",
    "print(\"\\n--- Initializing Evaluation Dataloader ---\")\n",
    "arc_eval_loader = None\n",
    "if os.path.exists(ARC_EVAL_DIR):\n",
    "    arc_eval_dataset = ARCEvalDataset(data_path=ARC_EVAL_DIR)\n",
    "    if len(arc_eval_dataset) > 0:\n",
    "        arc_eval_loader = DataLoader(arc_eval_dataset, batch_size=1, shuffle=False)\n",
    "        print(f\"✓ Evaluation DataLoader initialized with {len(arc_eval_dataset)} tasks.\")\n",
    "else:\n",
    "    print(f\"⚠️ Evaluation directory not found: '{ARC_EVAL_DIR}'\")\n",
    "\n",
    "# --- Main Evaluation Logic ---\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(f\"🔬 STARTING ARC-AGI-2 Evaluation on device '{device}'\")\n",
    "print(\"=\"*60 + \"\\n\")\n",
    "\n",
    "if not all([ctm_model_arc, optimizer_arc, arc_eval_loader]):\n",
    "     print(\"⚠️ Skipping evaluation due to missing components.\")\n",
    "else:\n",
    "    latest_epoch = NUM_EPOCHS_PRINCIPLES\n",
    "    ctm_checkpoint_path_eval = os.path.join(CHECKPOINT_DIR_PRINCIPLES, f\"ctm_model_arc_epoch_{latest_epoch}.safetensors\")\n",
    "\n",
    "    try:\n",
    "        if os.path.exists(ctm_checkpoint_path_eval):\n",
    "            print(f\"  > Loading CTM checkpoint from {ctm_checkpoint_path_eval}...\")\n",
    "            state_dict_ctm = load_file(ctm_checkpoint_path_eval, device=\"cpu\")\n",
    "            model_to_load_ctm = accelerator_arc.unwrap_model(ctm_model_arc) if ACCELERATE_AVAILABLE else ctm_model_arc\n",
    "            model_to_load_ctm.load_state_dict(state_dict_ctm, strict=False)\n",
    "            print(f\"✓ Loaded CTM checkpoint from epoch {latest_epoch}.\")\n",
    "        else:\n",
    "            print(f\"⚠️ CTM Checkpoint not found at {ctm_checkpoint_path_eval}.\")\n",
    "\n",
    "        ctm_model_arc.eval()\n",
    "        if hasattr(ctm_model_arc, 'wake_up'):\n",
    "            ctm_model_arc.wake_up()\n",
    "        total_tasks = 0\n",
    "        solved_tasks = 0\n",
    "\n",
    "        # Create a scheduler for the online updates.\n",
    "        scheduler_arc = optim.lr_scheduler.StepLR(optimizer_arc, step_size=5, gamma=0.9)\n",
    "\n",
    "        for task_idx, task_batch in enumerate(arc_eval_loader):\n",
    "            if not task_batch: continue\n",
    "\n",
    "            current_task_data = task_batch\n",
    "            total_tasks += 1\n",
    "            task_solved_overall = True\n",
    "            \n",
    "            # Since batch_size is 1, unpack the lists\n",
    "            task_id = current_task_data['id'][0]\n",
    "            test_pairs = [{k: v.squeeze(0) for k, v in pair.items()} for pair in current_task_data['test'][0]]\n",
    "\n",
    "            if not test_pairs:\n",
    "                print(f\"Task {task_idx + 1} ({task_id}): No test cases found. Skipping.\")\n",
    "                continue\n",
    "\n",
    "            for test_pair_idx, test_pair in enumerate(test_pairs):\n",
    "                input_grid_np_eval = test_pair['input'].cpu().numpy()\n",
    "                input_bytes_eval = torch.from_numpy(np.frombuffer(serialize_and_pad_grid(input_grid_np_eval), dtype=np.uint8)).to(torch.uint8).unsqueeze(0).to(device)\n",
    "\n",
    "                target_grid_np = test_pair['output'].cpu().numpy()\n",
    "                h, w = test_pair['original_output_dims']\n",
    "                original_dims = (h.item(), w.item())\n",
    "                final_target = target_grid_np[:original_dims[0], :original_dims[1]]\n",
    "                \n",
    "                test_input_solved = False\n",
    "\n",
    "                # --- First Attempt: Standard Prediction ---\n",
    "                print(f\"  > Attempt 1 for test pair {test_pair_idx + 1}...\")\n",
    "                with torch.no_grad():\n",
    "                    eval_model_output = ctm_model_arc.iterative_ctm_diffusion_sample(shape=input_bytes_eval.shape, initial_byte_sequence_for_inference=input_bytes_eval, num_steps=50)\n",
    "                    output_bytes = eval_model_output[0]\n",
    "                    \n",
    "                    if output_bytes is not None and output_bytes.numel() > 0:\n",
    "                        grid_flat = np.frombuffer(output_bytes.squeeze(0).cpu().numpy().tobytes(), dtype=np.uint8)\n",
    "                        preds_grid = np.full(MAX_GRID_SIZE, PADDING_VALUE, dtype=int)\n",
    "                        reshaped_len = min(len(grid_flat), ARC_INPUT_FLAT_DIM)\n",
    "                        preds_grid.flat[:reshaped_len] = grid_flat[:reshaped_len]\n",
    "                    else:\n",
    "                        preds_grid = np.full(MAX_GRID_SIZE, PADDING_VALUE, dtype=int)\n",
    "\n",
    "                final_pred = preds_grid[:original_dims[0], :original_dims[1]]\n",
    "\n",
    "                if np.array_equal(final_pred, final_target):\n",
    "                    print(f\"    - Solved on first attempt.\")\n",
    "                    test_input_solved = True\n",
    "                else:\n",
    "                    print(f\"    - Failed on first attempt. Trying online update.\")\n",
    "                    # --- Second Attempt: Fine-tune and Predict Again ---\n",
    "                    perform_online_update(\n",
    "                        model=ctm_model_arc,\n",
    "                        optimizer=optimizer_arc,\n",
    "                        scheduler=scheduler_arc,\n",
    "                        input_bytes=input_bytes_eval,\n",
    "                        corrected_grid_np=target_grid_np, # Use the full target grid for the update\n",
    "                        device=device\n",
    "                    )\n",
    "                    \n",
    "                    print(f\"  > Attempt 2 for test pair {test_pair_idx + 1} (post-update)...\")\n",
    "                    with torch.no_grad():\n",
    "                        eval_model_output_2 = ctm_model_arc.iterative_ctm_diffusion_sample(shape=input_bytes_eval.shape, initial_byte_sequence_for_inference=input_bytes_eval, num_steps=50)\n",
    "                        output_bytes_2 = eval_model_output_2[0]\n",
    "\n",
    "                        if output_bytes_2 is not None and output_bytes_2.numel() > 0:\n",
    "                            grid_flat_2 = np.frombuffer(output_bytes_2.squeeze(0).cpu().numpy().tobytes(), dtype=np.uint8)\n",
    "                            preds_grid_2 = np.full(MAX_GRID_SIZE, PADDING_VALUE, dtype=int)\n",
    "                            reshaped_len_2 = min(len(grid_flat_2), ARC_INPUT_FLAT_DIM)\n",
    "                            preds_grid_2.flat[:reshaped_len_2] = grid_flat_2[:reshaped_len_2]\n",
    "                        else:\n",
    "                            preds_grid_2 = np.full(MAX_GRID_SIZE, PADDING_VALUE, dtype=int)\n",
    "                    \n",
    "                    final_pred_2 = preds_grid_2[:original_dims[0], :original_dims[1]]\n",
    "                    \n",
    "                    if np.array_equal(final_pred_2, final_target):\n",
    "                         print(f\"    - Solved on second attempt after fine-tuning.\")\n",
    "                         test_input_solved = True\n",
    "                    else:\n",
    "                         print(f\"    - Failed on second attempt.\")\n",
    "\n",
    "                if not test_input_solved:\n",
    "                    task_solved_overall = False\n",
    "                    break\n",
    "            \n",
    "            if task_solved_overall:\n",
    "                solved_tasks += 1\n",
    "                print(f\"  Task {task_idx + 1}/{len(arc_eval_loader)} ({task_id}): SOLVED\")\n",
    "            else:\n",
    "                print(f\"  Task {task_idx + 1}/{len(arc_eval_loader)} ({task_id}): FAILED\")\n",
    "\n",
    "        if total_tasks > 0:\n",
    "            accuracy = (solved_tasks / total_tasks) * 100\n",
    "            summary = f\"ARC-AGI-2 Evaluation Summary:\\n  Total tasks evaluated: {total_tasks}\\n  Tasks solved: {solved_tasks}\\n  Accuracy: {accuracy:.2f}%\"\n",
    "            print(f\"\\n{summary}\")\n",
    "            with open('arc_agi_2_evaluation_summary.txt', 'w') as f:\n",
    "                f.write(summary)\n",
    "        else:\n",
    "            print(\"\\nARC-AGI-2 Evaluation: No tasks were evaluated.\")\n",
    "        \n",
    "        if hasattr(ctm_model_arc, 'sleep_down'):\n",
    "            ctm_model_arc.sleep_down()\n",
    "            \n",
    "    except FileNotFoundError as e:\n",
    "        print(f\"❌ Checkpoint file not found: {e}. Please ensure paths are correct.\")   \n",
    "    except Exception as e:\n",
    "        print(f\"❌ Error during ARC-AGI-2 evaluation: {e}\")\n",
    "        traceback.print_exc()\n",
    "        \n",
    "    print(\"\\n🔬 ARC-AGI-2 Evaluation Phase Completed.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
